{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# Lesson 04 - Logistic Regression, Newton's Method, Softmax, Calibration\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Objectives\n- Implement logistic regression with Newton's method.\n- Extend to softmax regression for multiclass classification.\n- Visualize calibration with reliability diagrams.\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## From the notes\n\n**Logistic regression**\n- Hypothesis: $h_\\theta(x) = \\sigma(\\theta^T x)$, where $\\sigma(z) = 1/(1+e^{-z})$.\n- Cost (negative log-likelihood): $J(\\theta) = -\\sum_i y^{(i)} \\log h_\\theta(x^{(i)}) + (1-y^{(i)}) \\log(1-h_\\theta(x^{(i)}))$.\n\n**Softmax**\n- $p(y=k|x) = \\frac{\\exp(\\theta_k^T x)}{\\sum_j \\exp(\\theta_j^T x)}$.\n\n_TODO: Validate equations against the official CS229 main notes PDF._\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Intuition\nLogistic regression treats classification as a probabilistic model. Newton's method uses second-order information for faster convergence. Softmax generalizes logistic regression to K classes.\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Data\nWe use a 2D synthetic classification dataset for visualization and calibration plots.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "import numpy as np\nimport matplotlib.pyplot as plt\n\nnp.random.seed(42)\n\n# Binary classification data\nm = 120\nX_pos = np.random.multivariate_normal([2, 2], [[1, 0.3], [0.3, 1]], m//2)\nX_neg = np.random.multivariate_normal([-2, -2], [[1, -0.2], [-0.2, 1]], m//2)\nX = np.vstack([X_pos, X_neg])\ny = np.hstack([np.ones(m//2), np.zeros(m//2)])\nXb = np.c_[np.ones(m), X]\n\ndef sigmoid(z):\n    return 1 / (1 + np.exp(-z))\n\ndef newton_logistic(X, y, iters=10):\n    theta = np.zeros(X.shape[1])\n    for _ in range(iters):\n        p = sigmoid(X @ theta)\n        R = np.diag(p * (1 - p))\n        grad = X.T @ (p - y)\n        hess = X.T @ R @ X\n        theta -= np.linalg.pinv(hess) @ grad\n    return theta\n\ntheta = newton_logistic(Xb, y)\nprobs = sigmoid(Xb @ theta)\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Experiments\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "# Softmax on a 3-class toy dataset\nK = 3\nXc = np.vstack([\n    np.random.multivariate_normal([2, 0], np.eye(2), 60),\n    np.random.multivariate_normal([-2, 0], np.eye(2), 60),\n    np.random.multivariate_normal([0, 2], np.eye(2), 60),\n])\nyc = np.array([0]*60 + [1]*60 + [2]*60)\nXc_b = np.c_[np.ones(Xc.shape[0]), Xc]\n\ndef softmax(z):\n    z = z - z.max(axis=1, keepdims=True)\n    expz = np.exp(z)\n    return expz / expz.sum(axis=1, keepdims=True)\n\ndef softmax_gd(X, y, K, alpha=0.1, iters=200):\n    theta = np.zeros((K, X.shape[1]))\n    y_one = np.eye(K)[y]\n    for _ in range(iters):\n        probs = softmax(X @ theta.T)\n        grad = (probs - y_one).T @ X / len(y)\n        theta -= alpha * grad\n    return theta\n\ntheta_sm = softmax_gd(Xc_b, yc, K)\nprobs_sm = softmax(Xc_b @ theta_sm.T)\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Visualizations\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "# Decision boundary for logistic regression\nplt.figure(figsize=(6,4))\nplt.scatter(X_pos[:,0], X_pos[:,1], label=\"class 1\")\nplt.scatter(X_neg[:,0], X_neg[:,1], label=\"class 0\")\nx1 = np.linspace(-4, 4, 100)\nx2 = -(theta[0] + theta[1]*x1) / theta[2]\nplt.plot(x1, x2, color=\"black\", label=\"boundary\")\nplt.title(\"Logistic regression boundary\")\nplt.xlabel(\"x1\")\nplt.ylabel(\"x2\")\nplt.legend()\nplt.show()\n\n# Calibration plot\nbins = np.linspace(0, 1, 6)\nbin_ids = np.digitize(probs, bins) - 1\nacc = [y[bin_ids==i].mean() if np.any(bin_ids==i) else 0 for i in range(len(bins)-1)]\nconf = [probs[bin_ids==i].mean() if np.any(bin_ids==i) else 0 for i in range(len(bins)-1)]\nplt.figure(figsize=(6,4))\nplt.plot(conf, acc, marker=\"o\")\nplt.plot([0,1],[0,1], linestyle=\"--\", color=\"gray\")\nplt.title(\"Reliability diagram\")\nplt.xlabel(\"Predicted probability\")\nplt.ylabel(\"Empirical accuracy\")\nplt.show()\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Takeaways\n- Newton's method can converge quickly for logistic regression by using curvature information.\n- Softmax provides a normalized probability distribution over K classes.\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Explain it in an interview\n- Explain why logistic regression outputs probabilities rather than hard labels.\n- Describe how you would assess calibration in a classifier.\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Exercises\n- Implement logistic regression with gradient descent and compare to Newton's method.\n- Try temperature scaling to improve calibration.\n- Extend softmax to include L2 regularization.\n"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "pygments_lexer": "ipython3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}