{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# Lesson 18 - Collaborative Filtering via Matrix Factorization\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Objectives\n- Implement matrix factorization with SGD.\n- Reconstruct a sparse ratings matrix.\n- Visualize training loss over epochs.\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## From the notes\n\n**Collaborative filtering**\n- Factor ratings matrix $R \\approx U V^T$.\n- Optimize with squared error on observed entries.\n\n_TODO: Validate collaborative filtering equations in the CS229 main notes PDF._\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Intuition\nMatrix factorization learns latent user and item vectors such that their dot product approximates observed ratings.\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Data\nWe generate a small synthetic ratings matrix with missing entries.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "import numpy as np\nimport matplotlib.pyplot as plt\n\nnp.random.seed(42)\n\nnum_users, num_items, k = 20, 15, 3\ntrue_U = np.random.randn(num_users, k)\ntrue_V = np.random.randn(num_items, k)\nR_full = true_U @ true_V.T\nmask = np.random.rand(num_users, num_items) < 0.6\nR = np.where(mask, R_full, np.nan)\n\nU = 0.1 * np.random.randn(num_users, k)\nV = 0.1 * np.random.randn(num_items, k)\n\ndef sgd(R, U, V, lr=0.05, reg=0.01, epochs=200):\n    errors = []\n    rows, cols = np.where(~np.isnan(R))\n    for _ in range(epochs):\n        for i, j in zip(rows, cols):\n            err = R[i, j] - U[i] @ V[j]\n            U[i] += lr * (err * V[j] - reg * U[i])\n            V[j] += lr * (err * U[i] - reg * V[j])\n        pred = U @ V.T\n        errors.append(np.nanmean((pred - R) ** 2))\n    return errors\n\nlosses = sgd(R, U, V)\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Experiments\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "losses[-1]\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Visualizations\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "plt.figure(figsize=(6,4))\nplt.plot(losses)\nplt.title(\"Matrix factorization loss\")\nplt.xlabel(\"epoch\")\nplt.ylabel(\"MSE\")\nplt.show()\n\nplt.figure(figsize=(6,4))\nplt.imshow(U @ V.T, aspect=\"auto\", cmap=\"viridis\")\nplt.title(\"Reconstructed ratings\")\nplt.xlabel(\"item\")\nplt.ylabel(\"user\")\nplt.colorbar()\nplt.show()\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Takeaways\n- Matrix factorization learns latent factors that explain observed ratings.\n- SGD with regularization prevents overfitting on sparse data.\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Explain it in an interview\n- Explain how matrix factorization handles missing entries.\n- Describe the role of regularization in collaborative filtering.\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Exercises\n- Try different latent dimensions and compare loss.\n- Hold out a validation set to tune regularization.\n- Add user and item bias terms.\n"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "pygments_lexer": "ipython3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}