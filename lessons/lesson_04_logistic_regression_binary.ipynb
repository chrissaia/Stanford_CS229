{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Lesson 04: Logistic Regression (Binary)",
        "",
        "## Objectives",
        "- Implement logistic regression with gradient descent.",
        "- Visualize decision boundaries and probabilities.",
        "- Compare linear vs logistic outputs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## From the notes",
        "Hypothesis: \\(h_\theta(x) = g(\theta^T x)\\) with \\(g(z)=1/(1+e^{-z})\\).",
        "",
        "Log-likelihood for Bernoulli labels:",
        "\\[",
        "\\ell(\theta) = \\sum_{i=1}^m \\left[y^{(i)} \\log h_\theta(x^{(i)}) + (1-y^{(i)})\\log (1-h_\theta(x^{(i)}))\right].",
        "\\]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Intuition",
        "Logistic regression models the probability of class 1 using a sigmoid. The decision boundary is linear in feature space."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data",
        "We create two Gaussian clusters to separate with a linear boundary."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "import numpy as np",
        "import matplotlib.pyplot as plt",
        "",
        "np.random.seed(42)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Synthetic binary data",
        "m = 120",
        "mean0 = np.array([-1.0, -1.0])",
        "mean1 = np.array([1.5, 1.2])",
        "X0 = np.random.multivariate_normal(mean0, np.eye(2)*0.6, size=m//2)",
        "X1 = np.random.multivariate_normal(mean1, np.eye(2)*0.6, size=m//2)",
        "X_raw = np.vstack([X0, X1])",
        "y = np.array([0]*(m//2) + [1]*(m//2))",
        "X = np.c_[np.ones(m), X_raw]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Implementation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "def sigmoid(z):",
        "    return 1 / (1 + np.exp(-z))",
        "",
        "",
        "def logistic_cost(X, y, theta):",
        "    preds = sigmoid(X @ theta)",
        "    eps = 1e-9",
        "    return -np.mean(y * np.log(preds + eps) + (1 - y) * np.log(1 - preds + eps))",
        "",
        "",
        "def logistic_gd(X, y, alpha=0.1, num_iters=200):",
        "    theta = np.zeros(X.shape[1])",
        "    history = []",
        "    for _ in range(num_iters):",
        "        preds = sigmoid(X @ theta)",
        "        grad = X.T @ (preds - y) / len(y)",
        "        theta -= alpha * grad",
        "        history.append(logistic_cost(X, y, theta))",
        "    return theta, np.array(history)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Experiments"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "theta, history = logistic_gd(X, y)",
        "probs = sigmoid(X @ theta)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Visualizations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "plt.figure(figsize=(6,4))",
        "plt.scatter(X0[:,0], X0[:,1], label=\"class 0\", alpha=0.7)",
        "plt.scatter(X1[:,0], X1[:,1], label=\"class 1\", alpha=0.7)",
        "",
        "x1_vals = np.linspace(X_raw[:,0].min()-1, X_raw[:,0].max()+1, 100)",
        "# decision boundary: theta0 + theta1 x1 + theta2 x2 = 0",
        "x2_vals = -(theta[0] + theta[1]*x1_vals) / theta[2]",
        "plt.plot(x1_vals, x2_vals, color=\"black\", label=\"boundary\")",
        "plt.xlabel(\"x1\")",
        "plt.ylabel(\"x2\")",
        "plt.title(\"Logistic regression decision boundary\")",
        "plt.legend()",
        "plt.show()",
        "",
        "plt.figure(figsize=(6,4))",
        "plt.plot(history)",
        "plt.xlabel(\"iteration\")",
        "plt.ylabel(\"negative log-likelihood\")",
        "plt.title(\"Training loss\")",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Takeaways",
        "- Logistic regression models probabilities via the sigmoid link.",
        "- The objective is convex, so gradient descent converges reliably."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Explain it in an interview",
        "- Describe the sigmoid and log-likelihood.",
        "- Emphasize how the linear boundary arises from \\(\theta^T x = 0\\)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exercises",
        "1. Add L2 regularization and compare the boundary.",
        "2. Implement Newton's method and compare convergence.",
        "3. Try a non-linearly separable dataset and add polynomial features."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}