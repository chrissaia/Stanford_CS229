{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# Lesson 02 - Linear Regression (GD, SGD, Normal Equation)\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Objectives\n- Implement batch gradient descent and stochastic gradient descent.\n- Solve linear regression with the normal equation.\n- Compare convergence behavior and sensitivity to feature scaling.\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## From the notes\n\n**Notation**\n- $h_\\theta(x) = \\theta^T x$ with $x_0 = 1$.\n- Cost: $J(\\theta) = \\frac{1}{2m} \\sum_{i=1}^m (h_\\theta(x^{(i)}) - y^{(i)})^2$.\n- Gradient descent: $\\theta := \\theta - \\alpha \\nabla_\\theta J(\\theta)$.\n- Normal equation: $\\theta = (X^T X)^{-1} X^T y$.\n\n_TODO: Verify the exact notation matches the official CS229 notes PDF._\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Intuition\nLinear regression finds the line (or hyperplane) that minimizes squared error. Batch GD uses full gradients, SGD uses single-example updates, and the normal equation gives a closed-form solution.\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Data\nWe reuse a 1D synthetic regression dataset to make optimization behavior easy to visualize.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "import numpy as np\nimport matplotlib.pyplot as plt\n\nnp.random.seed(42)\n\n# Data\nm = 80\nx = np.linspace(0, 10, m)\ny = 3.0 * x + 1.5 + np.random.normal(scale=2.5, size=m)\nX = np.c_[np.ones(m), x]\n\ndef compute_cost(X, y, theta):\n    errors = X @ theta - y\n    return (errors @ errors) / (2 * len(y))\n\ndef batch_gd(X, y, alpha=0.01, iters=2000):\n    theta = np.zeros(X.shape[1])\n    history = []\n    for _ in range(iters):\n        grad = (X.T @ (X @ theta - y)) / len(y)\n        theta -= alpha * grad\n        history.append(compute_cost(X, y, theta))\n    return theta, history\n\ndef sgd(X, y, alpha=0.01, iters=30):\n    theta = np.zeros(X.shape[1])\n    history = []\n    for _ in range(iters):\n        for i in range(len(y)):\n            grad = (X[i] @ theta - y[i]) * X[i]\n            theta -= alpha * grad\n        history.append(compute_cost(X, y, theta))\n    return theta, history\n\ntheta_gd, hist_gd = batch_gd(X, y)\ntheta_sgd, hist_sgd = sgd(X, y)\ntheta_ne = np.linalg.pinv(X.T @ X) @ X.T @ y\n\ntheta_gd, theta_sgd, theta_ne\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Experiments\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "# Feature scaling experiment\nX_scaled = X.copy()\nX_scaled[:, 1] = (X_scaled[:, 1] - X_scaled[:, 1].mean()) / X_scaled[:, 1].std()\n\ntheta_gd_scaled, hist_gd_scaled = batch_gd(X_scaled, y, alpha=0.05)\nhist_gd[-1], hist_gd_scaled[-1]\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Visualizations\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "plt.figure(figsize=(6,4))\nplt.plot(hist_gd, label=\"Batch GD\")\nplt.plot(hist_sgd, label=\"SGD\")\nplt.title(\"Convergence of GD vs SGD\")\nplt.xlabel(\"Iteration\")\nplt.ylabel(\"J(\u03b8)\")\nplt.legend()\nplt.show()\n\npreds = X @ theta_gd\nplt.figure(figsize=(6,4))\nplt.scatter(x, y, alpha=0.6, label=\"data\")\nplt.plot(x, preds, color=\"black\", label=\"GD fit\")\nplt.title(\"Linear regression fit\")\nplt.xlabel(\"x\")\nplt.ylabel(\"y\")\nplt.legend()\nplt.show()\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Takeaways\n- Batch GD is stable but slower per step; SGD is noisy but often faster to good solutions.\n- The normal equation provides a closed-form solution when X^T X is invertible.\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Explain it in an interview\n- Explain the difference between GD, SGD, and the normal equation.\n- Describe why feature scaling accelerates gradient descent.\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Exercises\n- Implement mini-batch gradient descent and compare to GD/SGD.\n- Show what happens if X^T X is singular and use the pseudo-inverse.\n- Add L2 regularization and derive the closed-form ridge solution.\n"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "pygments_lexer": "ipython3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}