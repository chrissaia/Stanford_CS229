{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Lesson 02: Linear Regression with GD + SGD (Refined)",
        "",
        "## Objectives",
        "- Implement batch gradient descent (BGD) and stochastic gradient descent (SGD).",
        "- Compare convergence behavior and learning curves.",
        "- Visualize optimization dynamics and prediction error."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## From the notes: notation + objective",
        "We use \\(m\\) training examples \\(\\{(x^{(i)}, y^{(i)})\\}_{i=1}^m\\), with \\(x_0=1\\). The hypothesis is \\(h_\theta(x)=\theta^T x\\).",
        "",
        "Objective:",
        "\\[",
        "J(\theta) = \frac{1}{2m}\\sum_{i=1}^m (h_\theta(x^{(i)}) - y^{(i)})^2.",
        "\\]",
        "Gradient:",
        "\\[",
        "",
        "\\nabla_\theta J(\theta) = \frac{1}{m} X^T(X\theta - y).",
        "\\]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Intuition",
        "Gradient descent iteratively nudges \\(\theta\\) to reduce error. SGD uses a single example at a time, producing noisier but often faster updates."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data",
        "We generate a noisy linear dataset similar to the lecture example."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "import numpy as np",
        "import matplotlib.pyplot as plt",
        "",
        "np.random.seed(42)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Synthetic data",
        "m = 80",
        "X_raw = np.linspace(0, 12, m)",
        "y = 4.0 * X_raw - 3.0 + np.random.normal(0, 3.0, size=m)",
        "X = np.c_[np.ones(m), X_raw]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Implementation: batch gradient descent"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "def compute_cost(X, y, theta):",
        "    preds = X @ theta",
        "    return 0.5 / len(y) * np.sum((preds - y) ** 2)",
        "",
        "",
        "def batch_gd(X, y, alpha=0.05, num_iters=200):",
        "    theta = np.zeros(X.shape[1])",
        "    history = []",
        "    for _ in range(num_iters):",
        "        grad = (X.T @ (X @ theta - y)) / len(y)",
        "        theta -= alpha * grad",
        "        history.append(compute_cost(X, y, theta))",
        "    return theta, np.array(history)",
        "",
        "",
        "def sgd(X, y, alpha=0.01, num_iters=10):",
        "    theta = np.zeros(X.shape[1])",
        "    history = []",
        "    for _ in range(num_iters):",
        "        for i in range(len(y)):",
        "            xi = X[i:i+1]",
        "            yi = y[i]",
        "            grad = xi.T @ (xi @ theta - yi)",
        "            theta -= alpha * grad.flatten()",
        "            history.append(compute_cost(X, y, theta))",
        "    return theta, np.array(history)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Experiments",
        "We compare BGD vs SGD and inspect convergence."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "theta_bgd, hist_bgd = batch_gd(X, y)",
        "theta_sgd, hist_sgd = sgd(X, y)",
        "",
        "pred_bgd = X @ theta_bgd",
        "pred_sgd = X @ theta_sgd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Visualizations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "plt.figure(figsize=(6,4))",
        "plt.scatter(X_raw, y, alpha=0.6, label=\"data\")",
        "plt.plot(X_raw, pred_bgd, color=\"C1\", label=\"BGD\")",
        "plt.plot(X_raw, pred_sgd, color=\"C2\", label=\"SGD\")",
        "plt.xlabel(\"x\")",
        "plt.ylabel(\"y\")",
        "plt.title(\"Linear regression fits\")",
        "plt.legend()",
        "plt.show()",
        "",
        "plt.figure(figsize=(6,4))",
        "plt.plot(hist_bgd, label=\"BGD\")",
        "plt.plot(hist_sgd, label=\"SGD\", alpha=0.7)",
        "plt.xlabel(\"update step\")",
        "plt.ylabel(\"J(theta)\")",
        "plt.title(\"Convergence curves\")",
        "plt.legend()",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Takeaways",
        "- BGD provides smooth convergence, SGD introduces noise but can reach good solutions faster.",
        "- Learning rate tuning is crucial for stability."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Explain it in an interview",
        "- Start with the least-squares objective and its gradient.",
        "- Emphasize why SGD is used for large datasets."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exercises",
        "1. Add feature scaling and compare convergence.",
        "2. Implement mini-batch GD and compare to BGD/SGD.",
        "3. Visualize the cost surface for \\(\theta_0, \theta_1\\)."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}