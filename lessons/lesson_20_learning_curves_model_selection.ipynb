{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Lesson 20: Learning Curves + Model Selection",
        "",
        "## Objectives",
        "- Diagnose bias vs variance using learning curves.",
        "- Compare models of different complexity.",
        "- Connect cross-validation to model selection."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## From the notes",
        "Learning curves plot training vs validation error as a function of dataset size. Gap patterns indicate bias or variance issues."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Intuition",
        "High bias leads to high training/validation error; high variance leads to low training but high validation error."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data",
        "We use polynomial regression on a noisy sinusoid."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "import numpy as np",
        "import matplotlib.pyplot as plt",
        "",
        "np.random.seed(42)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Data",
        "m = 80",
        "X_raw = np.linspace(-3, 3, m)",
        "y = np.sin(X_raw) + np.random.normal(0, 0.2, size=m)",
        "",
        "# Train/val split",
        "idx = np.arange(m)",
        "np.random.shuffle(idx)",
        "train_idx, val_idx = idx[:50], idx[50:]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Implementation: polynomial regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "def poly_features(x, degree):",
        "    return np.vstack([x**d for d in range(degree+1)]).T",
        "",
        "",
        "def fit_poly(X, y, lam=0.0):",
        "    reg = lam * np.eye(X.shape[1])",
        "    reg[0,0] = 0",
        "    return np.linalg.pinv(X.T @ X + reg) @ X.T @ y",
        "",
        "",
        "def mse(X, y, theta):",
        "    preds = X @ theta",
        "    return np.mean((preds - y) ** 2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Experiments: learning curves"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "train_errors = []",
        "val_errors = []",
        "",
        "for m_train in range(10, len(train_idx)+1, 5):",
        "    idx_subset = train_idx[:m_train]",
        "    X_train = poly_features(X_raw[idx_subset], degree=5)",
        "    y_train = y[idx_subset]",
        "    theta = fit_poly(X_train, y_train)",
        "    train_errors.append(mse(X_train, y_train, theta))",
        "",
        "    X_val = poly_features(X_raw[val_idx], degree=5)",
        "    y_val = y[val_idx]",
        "    val_errors.append(mse(X_val, y_val, theta))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Visualizations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "plt.figure(figsize=(6,4))",
        "plt.plot(range(10, len(train_idx)+1, 5), train_errors, label=\"train\")",
        "plt.plot(range(10, len(train_idx)+1, 5), val_errors, label=\"val\")",
        "plt.xlabel(\"training set size\")",
        "plt.ylabel(\"MSE\")",
        "plt.title(\"Learning curves\")",
        "plt.legend()",
        "plt.show()",
        "",
        "# Compare polynomial degrees",
        "plt.figure(figsize=(6,4))",
        "for deg in [1, 3, 5]:",
        "    X_train = poly_features(X_raw[train_idx], degree=deg)",
        "    theta = fit_poly(X_train, y[train_idx])",
        "    X_plot = poly_features(X_raw, degree=deg)",
        "    plt.plot(X_raw, X_plot @ theta, label=f\"degree {deg}\")",
        "plt.scatter(X_raw, y, alpha=0.4, label=\"data\")",
        "plt.xlabel(\"x\")",
        "plt.ylabel(\"y\")",
        "plt.title(\"Model complexity comparison\")",
        "plt.legend()",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Takeaways",
        "- Learning curves diagnose bias/variance issues.",
        "- Model selection balances complexity and generalization."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Explain it in an interview",
        "- Interpret learning curves and how they guide next steps.",
        "- Describe cross-validation for selecting hyperparameters."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exercises",
        "1. Add regularization and compare learning curves.",
        "2. Try degree 8 and observe variance.",
        "3. Implement k-fold cross-validation for degree selection."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}