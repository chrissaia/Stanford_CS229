{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Lesson 10: Regularization (L2/L1)",
        "",
        "## Objectives",
        "- Implement ridge regression and lasso (coordinate descent).",
        "- Visualize coefficient shrinkage as \\(\\lambda\\) varies.",
        "- Interpret L1 vs L2 geometry."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## From the notes",
        "Regularized objective:",
        "\\[",
        "J(\theta) = \frac{1}{2m}\\|X\theta - y\\|^2 + \\lambda \\|\theta\\|_p.",
        "\\]",
        "L2 uses \\(p=2\\), L1 uses \\(p=1\\)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Intuition",
        "L2 shrinks coefficients smoothly; L1 encourages sparsity by creating corners in the constraint set."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data",
        "We simulate a linear dataset with correlated features to observe shrinkage."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "import numpy as np",
        "import matplotlib.pyplot as plt",
        "",
        "np.random.seed(42)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Synthetic data",
        "m = 100",
        "X_raw = np.random.randn(m, 3)",
        "true_theta = np.array([2.0, -1.5, 0.5])",
        "y = X_raw @ true_theta + np.random.normal(0, 0.5, size=m)",
        "X = np.c_[np.ones(m), X_raw]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Implementation: ridge regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "def ridge_theta(X, y, lam):",
        "    n = X.shape[1]",
        "    reg = lam * np.eye(n)",
        "    reg[0,0] = 0",
        "    return np.linalg.pinv(X.T @ X + reg) @ X.T @ y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Implementation: lasso (coordinate descent)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "def soft_threshold(rho, lam):",
        "    if rho < -lam:",
        "        return rho + lam",
        "    if rho > lam:",
        "        return rho - lam",
        "    return 0.0",
        "",
        "",
        "def lasso_cd(X, y, lam, num_iters=100):",
        "    m, n = X.shape",
        "    theta = np.zeros(n)",
        "    for _ in range(num_iters):",
        "        for j in range(n):",
        "            residual = y - X @ theta + theta[j] * X[:, j]",
        "            rho = (X[:, j] @ residual) / m",
        "            if j == 0:",
        "                theta[j] = rho",
        "            else:",
        "                theta[j] = soft_threshold(rho, lam)",
        "    return theta"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Experiments"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "lambdas = [0.0, 0.1, 0.5, 1.0]",
        "coefs_ridge = []",
        "coefs_lasso = []",
        "for lam in lambdas:",
        "    coefs_ridge.append(ridge_theta(X, y, lam))",
        "    coefs_lasso.append(lasso_cd(X, y, lam))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Visualizations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "plt.figure(figsize=(6,4))",
        "for i in range(X.shape[1]):",
        "    plt.plot(lambdas, [c[i] for c in coefs_ridge], label=f\"theta{i}\")",
        "plt.xlabel(\"lambda\")",
        "plt.ylabel(\"ridge coefficients\")",
        "plt.title(\"Ridge shrinkage\")",
        "plt.legend()",
        "plt.show()",
        "",
        "plt.figure(figsize=(6,4))",
        "for i in range(X.shape[1]):",
        "    plt.plot(lambdas, [c[i] for c in coefs_lasso], label=f\"theta{i}\")",
        "plt.xlabel(\"lambda\")",
        "plt.ylabel(\"lasso coefficients\")",
        "plt.title(\"Lasso shrinkage\")",
        "plt.legend()",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Takeaways",
        "- L2 regularization reduces variance by shrinking weights.",
        "- L1 regularization can produce sparse models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Explain it in an interview",
        "- Compare the geometry of L1 vs L2 constraints.",
        "- Discuss when sparsity is beneficial."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exercises",
        "1. Plot contours of the least-squares loss with L1/L2 balls.",
        "2. Standardize features and re-run lasso.",
        "3. Implement elastic net and compare coefficients."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}