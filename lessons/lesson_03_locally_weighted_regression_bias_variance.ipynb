{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Lesson 03: Locally Weighted Regression + Bias/Variance",
        "",
        "## Objectives",
        "- Implement locally weighted regression (LWR).",
        "- Observe how bandwidth \\(\tau\\) controls bias and variance.",
        "- Visualize predictions for different \\(\tau\\) values."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## From the notes",
        "LWR fits a local linear model around each query point with weights",
        "\\[",
        "w^{(i)} = \\exp\\left(-\frac{\\|x^{(i)} - x\\|^2}{2\tau^2}\right).",
        "\\]",
        "The weighted normal equation uses \\(W\\) as a diagonal matrix of weights."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Intuition",
        "Small \\(\tau\\) uses only nearby points (low bias, high variance). Large \\(\tau\\) approaches global linear regression (higher bias, lower variance)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data",
        "We generate a nonlinear 1D dataset to show why local models help."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "import numpy as np",
        "import matplotlib.pyplot as plt",
        "",
        "np.random.seed(42)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Nonlinear data",
        "m = 80",
        "X_raw = np.linspace(-3, 3, m)",
        "y = np.sin(X_raw) + np.random.normal(0, 0.2, size=m)",
        "X = np.c_[np.ones(m), X_raw]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Implementation: locally weighted regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "def lwr_predict(x_query, X, y, tau):",
        "    diffs = X[:, 1] - x_query",
        "    weights = np.exp(-(diffs ** 2) / (2 * tau ** 2))",
        "    W = np.diag(weights)",
        "    theta = np.linalg.pinv(X.T @ W @ X) @ (X.T @ W @ y)",
        "    return np.array([1, x_query]) @ theta",
        "",
        "",
        "def predict_curve(X_raw, X, y, tau):",
        "    return np.array([lwr_predict(xq, X, y, tau) for xq in X_raw])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Experiments",
        "We compare fits for different bandwidths."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "taus = [0.2, 0.5, 1.0]",
        "curves = {tau: predict_curve(X_raw, X, y, tau) for tau in taus}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Visualizations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "plt.figure(figsize=(6,4))",
        "plt.scatter(X_raw, y, alpha=0.6, label=\"data\")",
        "for tau, curve in curves.items():",
        "    plt.plot(X_raw, curve, label=f\"tau={tau}\")",
        "plt.xlabel(\"x\")",
        "plt.ylabel(\"y\")",
        "plt.title(\"LWR fits\")",
        "plt.legend()",
        "plt.show()",
        "",
        "plt.figure(figsize=(6,4))",
        "train_errors = []",
        "for tau in taus:",
        "    preds = curves[tau]",
        "    train_errors.append(np.mean((preds - y)**2))",
        "plt.bar([str(t) for t in taus], train_errors)",
        "plt.xlabel(\"tau\")",
        "plt.ylabel(\"MSE\")",
        "plt.title(\"Training error vs bandwidth\")",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Takeaways",
        "- LWR adapts to nonlinear structure without global feature engineering.",
        "- \\(\tau\\) tunes the bias-variance tradeoff."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Explain it in an interview",
        "- Describe the weighting kernel and the local normal equation.",
        "- Explain how \\(\tau\\) controls locality."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exercises",
        "1. Try a Gaussian vs Epanechnikov kernel and compare fits.",
        "2. Use LWR on a dataset with outliers and examine robustness.",
        "3. Derive the weighted normal equation."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}