{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# Lesson 11 - Learning Theory and VC Dimension\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Objectives\n- Visualize empirical risk vs true risk.\n- Simulate capacity vs generalization behavior.\n- Discuss VC dimension intuition.\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## From the notes\n\n**Learning theory**\n- Empirical risk minimization: minimize training error.\n- Generalization bounds depend on hypothesis class complexity (e.g., VC dimension).\n\n_TODO: Validate learning theory statements with CS229 main notes PDF._\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Intuition\nAs model capacity grows, training error decreases but generalization can worsen without enough data. VC dimension captures the richness of a hypothesis class.\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Data\nWe simulate fitting polynomials of increasing degree to show overfitting.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "import numpy as np\nimport matplotlib.pyplot as plt\n\nnp.random.seed(42)\n\ndef make_data(n=80):\n    x = np.linspace(-1, 1, n)\n    y = np.sin(3 * x) + 0.3 * np.random.randn(n)\n    return x, y\n\nx, y = make_data()\n\ndef poly_features(x, degree):\n    return np.vstack([x**d for d in range(degree+1)]).T\n\ndef fit_poly(x, y, degree):\n    X = poly_features(x, degree)\n    theta = np.linalg.pinv(X.T @ X) @ X.T @ y\n    return theta\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Experiments\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "degrees = range(1, 12)\ntrain_err = []\nval_err = []\nsplit = 60\nx_train, y_train = x[:split], y[:split]\nx_val, y_val = x[split:], y[split:]\nfor d in degrees:\n    theta = fit_poly(x_train, y_train, d)\n    train_err.append(np.mean((poly_features(x_train, d) @ theta - y_train)**2))\n    val_err.append(np.mean((poly_features(x_val, d) @ theta - y_val)**2))\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Visualizations\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "plt.figure(figsize=(6,4))\nplt.plot(list(degrees), train_err, label=\"train\")\nplt.plot(list(degrees), val_err, label=\"validation\")\nplt.title(\"Generalization vs model capacity\")\nplt.xlabel(\"polynomial degree\")\nplt.ylabel(\"MSE\")\nplt.legend()\nplt.show()\n\nplt.figure(figsize=(6,4))\nplt.scatter(x, y, alpha=0.6)\ntheta = fit_poly(x, y, 9)\nplt.plot(x, poly_features(x, 9) @ theta, color=\"black\")\nplt.title(\"High-capacity fit example\")\nplt.xlabel(\"x\")\nplt.ylabel(\"y\")\nplt.show()\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Takeaways\n- Training error is not a reliable measure of generalization.\n- Model complexity needs to be balanced with data size.\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Explain it in an interview\n- Explain what VC dimension captures in a hypothesis class.\n- Describe empirical risk minimization.\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Exercises\n- Simulate how validation error changes with more training data.\n- Explain why VC dimension influences generalization bounds.\n- Try a different hypothesis class (e.g., splines).\n"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "pygments_lexer": "ipython3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}