{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Lesson 18: Neural Network (1 Hidden Layer)",
        "",
        "## Objectives",
        "- Implement a 1-hidden-layer neural network with backprop.",
        "- Train on a nonlinear classification task.",
        "- Visualize decision boundaries and loss."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## From the notes",
        "Neural networks compose layers of affine transforms and nonlinearities. Backprop computes gradients via the chain rule."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Intuition",
        "Hidden layers allow nonlinear decision boundaries, while backprop efficiently computes parameter gradients."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data",
        "We generate a two-class concentric circles dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "import numpy as np",
        "import matplotlib.pyplot as plt",
        "",
        "np.random.seed(42)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Concentric circles",
        "m = 200",
        "r = np.random.rand(m)",
        "angles = 2 * np.pi * np.random.rand(m)",
        "X = np.c_[r * np.cos(angles), r * np.sin(angles)]",
        "y = (r > 0.5).astype(int)",
        "X = X.astype(float)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Implementation: 1-hidden-layer network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Network dimensions",
        "n_in = 2",
        "n_hidden = 8",
        "n_out = 1",
        "",
        "W1 = 0.1 * np.random.randn(n_in, n_hidden)",
        "b1 = np.zeros(n_hidden)",
        "W2 = 0.1 * np.random.randn(n_hidden, n_out)",
        "b2 = np.zeros(n_out)",
        "",
        "",
        "def sigmoid(z):",
        "    return 1 / (1 + np.exp(-z))",
        "",
        "",
        "def forward(X):",
        "    z1 = X @ W1 + b1",
        "    a1 = np.tanh(z1)",
        "    z2 = a1 @ W2 + b2",
        "    a2 = sigmoid(z2)",
        "    return z1, a1, z2, a2",
        "",
        "",
        "def train(X, y, lr=0.5, num_iters=500):",
        "    global W1, b1, W2, b2",
        "    history = []",
        "    y = y.reshape(-1,1)",
        "    for _ in range(num_iters):",
        "        z1, a1, z2, a2 = forward(X)",
        "        loss = -np.mean(y*np.log(a2+1e-9) + (1-y)*np.log(1-a2+1e-9))",
        "        history.append(loss)",
        "",
        "        # backprop",
        "        dz2 = a2 - y",
        "        dW2 = a1.T @ dz2 / len(X)",
        "        db2 = dz2.mean(axis=0)",
        "        da1 = dz2 @ W2.T",
        "        dz1 = da1 * (1 - np.tanh(z1)**2)",
        "        dW1 = X.T @ dz1 / len(X)",
        "        db1 = dz1.mean(axis=0)",
        "",
        "        W1 -= lr * dW1",
        "        b1 -= lr * db1",
        "        W2 -= lr * dW2",
        "        b2 -= lr * db2",
        "    return np.array(history)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Experiments"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "history = train(X, y)",
        "_, _, _, probs = forward(X)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Visualizations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "plt.figure(figsize=(6,4))",
        "plt.scatter(X[:,0], X[:,1], c=y, cmap=\"coolwarm\", alpha=0.7)",
        "plt.xlabel(\"x1\")",
        "plt.ylabel(\"x2\")",
        "plt.title(\"Neural network data\")",
        "plt.show()",
        "",
        "plt.figure(figsize=(6,4))",
        "plt.plot(history)",
        "plt.xlabel(\"iteration\")",
        "plt.ylabel(\"loss\")",
        "plt.title(\"Training loss\")",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Takeaways",
        "- A small network can model nonlinear boundaries.",
        "- Backprop efficiently computes gradients for all layers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Explain it in an interview",
        "- Walk through forward pass, loss, and backprop steps.",
        "- Mention how hidden layers enable nonlinearity."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exercises",
        "1. Increase hidden units and observe training dynamics.",
        "2. Replace tanh with ReLU and compare.",
        "3. Add L2 regularization on weights."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}