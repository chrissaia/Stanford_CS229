{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# Lesson 20 - Reinforcement Learning and Value Iteration\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Objectives\n- Define an MDP with states, actions, rewards, and transitions.\n- Implement value iteration for a small MDP.\n- Visualize value function convergence.\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## From the notes\n\n**MDP**\n- States $S$, actions $A$, transition $P(s'|s,a)$, rewards $R(s,a)$, discount $\\gamma$.\n- Bellman optimality: $V^*(s) = \\max_a \\sum_{s'} P(s'|s,a)(R(s,a)+\\gamma V^*(s'))$.\n\n_TODO: Validate RL notation with the CS229 main notes PDF._\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Intuition\nValue iteration repeatedly applies the Bellman optimality operator until the value function stabilizes.\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Data\nWe define a 3-state MDP with two actions.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "import numpy as np\nimport matplotlib.pyplot as plt\n\nnp.random.seed(42)\n\nstates = [0, 1, 2]\nactions = [0, 1]\ngamma = 0.9\n\n# Transition probabilities P[s, a, s']\nP = np.array([\n    [[0.7, 0.3, 0.0], [0.4, 0.6, 0.0]],\n    [[0.1, 0.6, 0.3], [0.0, 0.2, 0.8]],\n    [[0.0, 0.0, 1.0], [0.0, 0.0, 1.0]],\n])\n\nR = np.array([\n    [1.0, 0.5],\n    [0.0, 0.2],\n    [0.0, 0.0],\n])\n\ndef value_iteration(P, R, gamma=0.9, iters=50):\n    V = np.zeros(P.shape[0])\n    history = []\n    for _ in range(iters):\n        V_new = np.max(R + gamma * (P @ V), axis=1)\n        history.append(V_new.copy())\n        V = V_new\n    return V, np.array(history)\n\nV, history = value_iteration(P, R, gamma)\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Experiments\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "V\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Visualizations\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "plt.figure(figsize=(6,4))\nfor s in range(history.shape[1]):\n    plt.plot(history[:, s], label=f\"state {s}\")\nplt.title(\"Value iteration convergence\")\nplt.xlabel(\"iteration\")\nplt.ylabel(\"V(s)\")\nplt.legend()\nplt.show()\n\nplt.figure(figsize=(6,4))\nplt.bar(states, V)\nplt.title(\"Final value function\")\nplt.xlabel(\"state\")\nplt.ylabel(\"V(s)\")\nplt.show()\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Takeaways\n- Value iteration applies Bellman updates until convergence.\n- The discount factor controls the importance of future rewards.\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Explain it in an interview\n- Explain the Bellman optimality equation.\n- Describe the effect of changing gamma.\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Exercises\n- Add a terminal state with reward and recompute values.\n- Compare value iteration to policy iteration.\n- Change transition probabilities and observe stability.\n"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "pygments_lexer": "ipython3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}