0:03
All right. Hey, everyone. Morning and welcome back. Um, so what I'd like to do today
0:11
is continue our discussion of Naive Bayes and in particular, um, we've described how to use Naive Bayes in a generative learning algorithm,
0:20
to build a spam classifier that will almost work, right? And, and, and so today you see how Laplace smoothing is one other idea, uh,
0:28
you need to add to the Naive Bayes algorithm we described on Monday, to really make it work, um, for, say,
0:35
email spam classification, or, or for text classification. Uh, and then we'll talk about the different version of
0:40
Naive Bayes that's even better than the one we've been discussing so far. Um, talk a little bit about,
0:46
ah, advice for applying machine-learning algorithms. So this would be useful to you as you get started on your,
0:52
ah, CS229 class projects as well. This is a strategy of how to choose an algorithm and what to do first, what to do second,
0:58
uh, and then we'll start with, um, intro to support vector machines.
1:03
Okay? Um, so to recap, uh, the Naive Bayes Algorithm is
1:10
a generative learning algorithm in which given a piece of email, or Twitter message or some piece of text, um,
1:17
take a dictionary and put in zeros and ones depending on whether different words appear in
1:24
a particular email and so this becomes your feature representation for, say, an email that you're trying to classify as spam or not spam.
1:30
Um, so using the indicator function notation, um, X_j-, uh,
1:37
X_j- I've been trying to use the subscript J not consistently to denote the indexes and the features
1:43
and ith index in the training examples and you'll see I'm not being consistent with that. So X_j is whether or not the indicator for whether words j appears in an email.
1:54
And so, um, to build a generative model for this, uh, we need to model these two terms p of x given y and p of y.
2:04
Uh, so Gaussian distribution analysis models these two terms with a Gaussian and the Bernoulli respectively and Naive Bayes uses a different model.
2:13
And with Naive Bayes in particular p of x given y is modeled as a, um, product of the conditional probabilities of the individual features given the class label y.
2:24
And so the parameters that Naive Bayes model are, um, phi subscript y is the class prior.
2:30
What's the chance that y is equal to 1, before you've seen any features? As well as phi subscript J given y equals 0,
2:37
which is a chance of that word appearing in a non-spam, as well as phi subscript J given y equals 1 which is a chance of
2:45
that word appearing in spam email. Okay? Um, and so if you derive the maximum likelihood estimates,
2:56
you will find that the maximum likelihood estimates of, you know, phi y is this.
3:05
Right? Just a fraction of training examples, um, that was equal to spam and maximum likelihood estimates of this-
3:36
and this is just an indicator function notation, way of writing, um,
3:43
look, at all of your, uh, emails with label y equals 0 and contact y fraction of them,
3:49
did this feature X_j appear? Did this word X_j appear? Right? Um, and then finally at prediction time,
4:05
um, let's see,
4:10
you will calculate p of y equals 1 given X.
4:21
This is kinda according to Bayes rule. Okay?
4:40
Um, all right. So it turns out this algorithm will almost work and here's where it breaks down,
4:49
which is, um, you know, so actually eve- every year, there are some CS229 students and some machine learning students,
4:56
they will do a class project and some of you will end up submitting this to an academic conference. Right? Some, some- actually some,
5:02
some of CS229 class projects get submitted, you know, as conference papers pretty much every year.
5:07
One of the top machine learning conferences, is the conference NIPS. NIPS stands for Neural Information Processing Systems, um,
5:14
ah, and let's say that in your dictionary, you know, you have 10,000 words in your dictionary.
5:20
Let's say that the NIPS conference, the word NIPS corresponds to word number 6017, right?
5:27
In your, in your 10,000 word dictionary. But up until now,
5:33
presumably you've not had a lot of emails from your friends asking, "Hey, do you want to submit the paper to the NIPS Conference or not."
5:41
Um, and so if you use your current, you know, email, set of emails to find these maximum likelihood estimates of parameters,
5:50
you will probably estimate that, um, probability of seeing this word given that it's spam email, is probably zero.
6:02
Right? Zero over the number of, ah, examples that you've labeled as spam in your email.
6:09
So if, if you train up this model using your personal email, probably none of the emails you've received for the last few ones had the word NIPS in it, um, maybe.
6:17
Uh, and so if you plug in this formula for maximum likelihood estimate, the numerator is 0 and so your estimate of this is probably 0.
6:24
Um, and then similarly,
6:30
this is also 0 over, you know, the number of non-spam emails I guess.
6:35
Right. So that's what this is, is just this formula. Right? And, um, statistically it's just a bad idea to say that
6:47
the chance of something is 0 just because you haven't seen it yet and where this will cause the Naive Bayes algorithm to break down is,
6:56
if you use these as estimates of the, of the parameters, so this is your estimates parameter phi subscript 6017 given y equals 1.
7:06
This is phi subscript 6017 given y equals 0. Yes? And if you ever calculate this probability,
7:14
that is equal to a product from I equals 1 through n. Let's say you
7:19
have 10,000 words appear of X_i equals 1,
7:26
p of X_i given y, right?
7:32
And so if, um, you train your spam classifier on the emails you've gotten up until today,
7:37
and then after CS229, your project teammates sen- starts sending you emails saying, hey, you know, we like the class project.
7:44
Shall we consider submitting this class project to the NIPS conference? The NIPS conference deadline is usually in,
7:50
um, sort of May or June most years so, you know, finish your class project this December,
7:56
work on it some more by January, February, March, April next year and then maybe submit it to the conference May or June of 2019.
8:03
When you start getting emails from your friends saying, let's submit our papers to NIPS conference, then when you start to see the word NIPS in your email maybe in March of next year,
8:12
um, this product of probabilities will have a 0 in it, right?
8:19
And so this thing that I've just circled will evaluate to 0 because you multiply a lot numbers, one of which is 0.
8:25
Um, and in the same way this, well, this is also 0, right?
8:31
And this is also 0 because there'll be that one term in that product over there.
8:38
And so what that means is if you train a spam classifier today using all the data you have in your email inbox so far,
8:46
and if tomorrow or- or, you know, or two months from now, whenever. The first time you get an email from your teammates that has the word NIPS in it,
8:55
your spam classifier will estimate this probability as 0 over 0 plus 0, okay?
9:02
Now, apart from the divide by 0 error, uh, it turns out that, um,
9:08
statistically, it's just a bad idea, right? To estimate the probability of something as 0 just
9:13
because you have not seen it once yet, right? Um, so [NOISE] what I want to do is describe to you Laplace smoothing,
9:23
which is a technique that helps, um, address this problem. Okay? And, um, let's- let's- In order to motivate Laplace smoothing, let me, um,
9:34
use a- a- a- Yeah, Let me use a different example for now. Right? Um.
9:47
Let's see. All right. So, you know, several years ago, this is- this is all the data,
9:52
but several years ago- so- so let me put aside Naive Bayes, I want to talk about Laplace smoothing. We will come back to apply Laplace smoothing in Naive Bayes.
9:58
So several years ago, I was tracking the progress of the Stanford football team,
10:03
um, just a few years ago now. But that year on 9/12, um,
10:09
our football team played to Wake Forest and,
10:15
you know, actually these are all the, uh, all the stay games we played that year, right? And, um, uh, we did not win that game.
10:24
Then on 10/10, we played Oregon State and we did not win that game.
10:34
Arizona, we did not win that game.
10:39
We played Caltech, we did not win that game. [LAUGHTER].
10:46
And the question is, these are all the away games- almost all the out of state games we played that year.
10:53
And so you're, you know, Stanford football team's biggest fan. You followed them to every single out of state game and watched all these games.
10:58
The question is, after this unfortunate streak, when you go on- there's actually a game on New Year's Eve,
11:05
you follow them to their over home game, what's your estimate of the chances of their winning or losing?
11:12
Right? Now, if you use maximum likelihood, so let's say this is the variable x,
11:17
you would estimate the probability of their winning. Well, maximum likelihood is really count up the number of wins, right,
11:26
and divide that by the number of wins plus the number of losses.
11:33
And so in this case, um, you estimate this as 0 divided by number of wins with 0,
11:42
number of losses was 4, right? Which is equal to 0, okay? Um, that's kinda mean, right?
11:49
[LAUGHTER]. They lost 4 games, but you say, no, the chances of their winning is 0. Absolute certainty.
11:54
And- and- and just statistically, this is not, um, this is not a good idea.
11:59
Um, and so what Laplace smoothing,
12:07
what we're going to do is, uh, imagine that we saw the positive outcomes,
12:14
the number of wins, you know, just add 1 to the number of wins we actually saw and also the number of losses add 1.
12:24
Right? So if you actually saw 0 wins, pretend you saw one and if you saw 4 losses, pretend you saw 1 more than you actually saw.
12:31
And so Laplace smoothing, you're gonna end up adding 1 to the numerator and adding 2 to the denominator.
12:38
And so this ends up being 1 over 6, right? And that's actually a more reasonable may- maybe
12:45
it is a more reasonable estimate for the chance of, uh, them winning or losing the next game.
12:51
Um, uh, and the- there's actually a cert- certain- certain set of circumstances under which there's more estimates.
12:56
I didn't just make this up in thin air. Uh, Laplace, um, uh, you know, uh, it's an ancient that -- well known,
13:04
uh, very influential mathematician. He actually tried to estimated the chance of the sun rising the next day.
13:10
And the reasoning was, well, we've seen the sunrise all times and so, uh, but tha- that doesn't mean we should be absolutely certain
13:16
the sun will still rise tomorrow, right? And so his reasoning was, well, we've seen the sunrise 10,000 times, you know,
13:22
we can be really certain the sun will rise again tomorrow but maybe not absolutely certain because maybe something will go wrong
13:28
or who- who knows what will happen in this galaxy? Um , uh, uh, and so his reasoning
13:33
was- he derived the optimal estimate- way of estimating, you know, really the chance the sun will rise tomorrow.
13:39
And this is actually an optimal estimate under I'll say- I'll say the same assumptions, we don't need to worry about it.
13:45
But it turns out that if you assume that you are Bayesian, where the uniform Bayesian prior on the chance of the sun rising tomorrow.
13:52
So if the chance the sun rising tomorrow is uniformly distributed, you know, in the unit interval anywhere from 0 to 1,
13:58
then after a set of observations of this coin toss of whether the sun rises, this is actually a Bayesian optimal estimate of the chances of the sun rising tomorrow, okay?
14:06
If you don't understand what I just said in the last 30 seconds, don't worry about it. Um, uh, it's taught in sort of a Bayesian statistics- advanced Bayesian statistics classes.
14:14
But mechanically, what you should do is, uh, take this formula and add 1 to the number of
14:19
counts you actually saw for each of the possible outcomes. Um, and more generally,
14:24
uh, if y, er, excuse me.
14:32
If- if you're estimating probabilities for a k way random variable, um,
14:40
then you estimate the chance that X being i to be equal to,
14:54
um, so- so that's
15:03
the maximum likelihood estimate. And for the fast-moving,
15:08
you'd add one to the numerator and, um, you add k to the denominator.
15:14
Okay? So for Naive Bayes,
15:24
the way this mod- modifies your parameter estimates is this.
15:29
Um, I'm just gonna copy over the formula from above. Right?
15:44
Um, so that's the maximum likely estimate.
15:50
And with Laplace smoothing, you add one to the numerator and add two to the denominator and this
15:57
means that your estimates are probably- these probabilities they're never exactly 0 or exactly 1,
16:02
which takes away that problem of, you know, the 0 over 0. Okay. Um, and so if you implement this algorithm,
16:10
it's not- it's not like a great spam classifier but it's not terrible either. And one nice thing about this algorithm is is so simple, right?
16:17
Estimated parameters is just counting ,um, uh, uh can be done, you know, very efficiently, right, just- just by counting,
16:25
uh, and then- and classification time is just multiplying a bunch probabilities together. Uh, this is very confusing first algorithm. All right.
16:34
Any questions about this? Yeah.
16:40
[inaudible]? Oh sorry. This is y. Er, oh yes.
16:46
Thank you. Er, yes thank you.
16:58
All right. Oh, by the way, I- I was actually following the Stanford football team that year so,
17:04
you know, they lost. [LAUGHTER]. Because, okay, I love our football team. They're doing much better right now. That was a few years ago.
17:10
[LAUGHTER]. [NOISE].
17:19
All right. Um, [NOISE]
17:29
So, um, in- in the- examples we've talked about so far,
17:35
the features were binary valued. Um, and so, um,
17:41
actually one quick generalization, uh, when the features are multinomial valued,
17:49
um, then the generalization- actually here's one example. We talked about predicting housing prices, right?
17:56
That was our very first world meaning example. Let's say you have a classification problem instead,
18:01
which is you're listing a house you want to sell, what is the chance of this house to be sold within the next 30 days? So it's a classification problem.
18:07
Um, so if one of the features is the size of the house x, right,
18:13
then one way to turn the feature into a discrete feature would be to choose a few buckets,
18:20
assert the size is less than 400 square feet, uh, versus,
18:26
you know, 400 to 800 or 800 to 1200 or greater than 1200 square feet.
18:33
Then you can set the feature XI to one of four values, right?
18:38
So that is how you discretize a continuous valued feature to a discrete value feature. Um, and if you want to apply Naive Bayes to this problem,
18:47
then probability of x given y, this is just the same as before.
18:52
Product from i equals 1 through n of p of
19:00
xj given Y where now this can be a multinomial probability.
19:09
Right? Where if- if X now takes on one of four values there then, um, this can be a,
19:15
uh, estimators and multinomial problem. So instead of a Bernoulli distribution over two possible outcomes,
19:21
this can be a probably, uh, probability mass function probably over four possible outcomes if you discretize the size of a house into four values.
19:29
Um, and if you ever discretized variables, a typical rule of thumb in machine learning often we
19:34
discretize variables into 10 values, into 10 buckets. Uh, just as a- it often seems to work well enough.
19:41
I- I drew 4 here so I don't have to write all 10 buckets. But if you ever discretize var- variables,
19:46
you know, most people will start off with discretizing things into 10 values.
20:00
All right. Now, uh, right.
20:09
And so this is how you can apply Naive Bayes on other problems as well including cost line, for example, if a house is likely to be sold in the next 30 days.
20:17
Now, um, there's, uh, there's a different variation on Naive Bayes that I want to describe to
20:26
you that is actually much better for the specific problem of text classification.
20:31
Uh and so our feature representation for x so far was the following, right?
20:38
With a dictionary a, aardvark, buy,
20:50
So let's say you get an email that's, you know, a very spammy email that's "Drugs, buy drugs now",
20:57
[LAUGHTER] This is meant as an illustrative example,
21:03
I'm not telling any of you to buy drugs. [LAUGHTER] Um, so if,
21:13
uh, if you have a dictionary of 10,000 words, then I guess- let's say a is worth 1, aardvark is worth 2,
21:18
uh, just to, you know, make this example concrete. Let's say the word buy is word 800, drugs is word 1,600,
21:25
and let's say now is the word- is the 6,200th word in your, uh, 10,000 words in the sorted dictionary.
21:32
Um, then the representation for x will be, you know, 0, 0, [NOISE] right?
21:38
And they put a 1 there, and a 1 there, and a 1 there. Okay? Now, one, one- so, um,
21:43
one interesting thing about Naive Bayes is that it throws away the fact that the word drugs has appeared twice, right?
21:49
So that's losing a little bit of information, um, uh, and, and in this feature representation, um,
21:57
you know, each feature is either 0 or 1, right? And that's part of why it throws away the information that's, uh,
22:03
where the one-word drugs appear twice, and maybe should be given more weight for your- in your classifier.
22:09
Um, [NOISE] there's a different representation, uh, which is specific to text.
22:20
And I think text data has a, has a property that they can be very long or very short. You can have a five-word email,
22:26
or a 1,000-word email, um, and somehow you're taking very short or very long
22:31
emails and just mapping them to a feature vector that's always the same length. Just a different representation [NOISE] for, um,
22:39
this email, which is, uh, for that email that says, "Drugs, buy drugs now", we're gonna represent it as a four-dimensional feature vector, [NOISE] right?
22:53
And so this is going to be, um, n-dimensional for an email of length n.
22:59
So rather than a 10,000-dimensional feature vector, we now have a four-dimensional feature vector,
23:05
but now xj is,
23:15
um, an index from 1 to 10,000 instead of just being 0 or 1.
23:21
Okay? And, uh, n is- and I guess n varies by training example.
23:27
So ni is the, uh, length of email i.
23:40
So the longer email, this vector, the feature vector x will be longer,
23:45
and the shorter email, this feature vector will be shorter, okay?
23:52
So, um, let's see. Uh, just to give names to the algorithms we're gonna develop,
24:01
these are- these are really very confusing, very horrible names. But this is what the community calls them.
24:07
That the, the model we've talked about so far is sometimes called the Multivariate Bernoulli.
24:20
And that model, uh, so Bernoulli means coin tosses, so multivariate means, you know,
24:26
there are 10,000 Bernoulli random variables in this model whereas as a Multivariate Bernoulli event model.
24:31
An event comes with statistics I guess. Um, and the new representation we're gonna talk about is called
24:37
the [NOISE] Multinomial Event Model.
24:44
Uh, these two names are- are- are- frankly, these two names are quite confusing. But these are the names that, uh, I think- actually,
24:51
one of my friends Andrew McCallum, uh, as far as I know, wrote the paper that named these two algorithms.
24:56
But- but I think these are- these are the names we seem to use.
25:12
Um, and so, with this new model,
25:18
um, we're gonna build a generative model, and because it's a generative model,
25:24
or model p of x, y which can be factored as follows and using the Naive Bayes assumption,
25:33
we're going to assume that p of x given y is product from i equals 1 through n,
25:40
of j equals 1 through n, of p of xj, given y, and then times, you know, p of y.
25:49
Is that second term, right? Now, one of the, uh, uh, one, one of the reasons these two models were very-
25:57
were frankly actually very confusing to the machine learning community, is because this is exactly the equation [NOISE] that,
26:03
you know, you saw on Monday, when we described Naive Bayes for the first time, um, that, you know, this, you know,
26:09
p of x given y is part of probabilities. Right? So this is exactly, uh, so this, this equation looks cosmetically identical,
26:16
but with this new model, the second model, the confusingly named Multinomial Event Model, um,
26:22
the definition of xj and the definition of n is very different, right?
26:28
So instead of a product from 1 through 10,000, there's a product from 1 through the number of words in the email,
26:34
and this is now instead a multinomial probability. Rather than a binary or Bernoulli probability.
26:40
Okay? Um, and it turns out that, uh, well,
26:46
[NOISE] with this model, the parameters are same as before.
26:52
Phi y is probability of y equals 1, and also, um, the other parameters of this model, phi k,
27:00
given y equals 0, is a chance of xj equals k,
27:08
given y equals 0. Right? And- and just to make sure you understand the notation. See if this makes sense.
27:14
So this probability is the chance of word
27:20
blank being blank if label y equals 0.
27:29
So what goes into those two blanks?
27:38
Actually, what goes in the second blank? Uh, let's see. Well- well, yeah?
28:02
[inaudible]. Yes. Right. So it's the chance of the third word in the email,
28:09
being the word drugs, or the chance of the second in the email being buy, or whatever. And one part of, um,
28:16
why we implicitly assume, mainly why this is tricky, is that, uh, we assume that this probability doesn't depend on j, right?
28:25
That for every position in the email, for the- the chance that the first word being drugs is same as chance of the second word being drugs,
28:31
is same as the third word being drugs, which is why, um, on the left-hand side j doesn't actually appear on the left-hand side, right.
28:39
Makes sense? Any questions about this? No?
28:44
Okay. All right. Um, and so the way you calculate the probability, the way you would,
28:49
um, uh, and, and so the way that, uh, given a new email,
28:55
a test email, um, uh, you would calculate this probability is by, you know,
29:01
plugging these parameters that you estimate from the data into this formula. Okay? [NOISE]
29:26
Um, oh, and then, um, I wrote down, uh, [NOISE] right.
29:33
And then, and then the other set of the parameters is this. [NOISE] Right. Kind of just with y equals 1,
29:40
is that y equals 0. And then for the maximum likelihood estimate of the parameters, I'll just write out one of them.
29:45
[NOISE] Your estimate of, uh, the chance of a given word is really anywhere in any position,
29:55
being word k. What's the chance of some word in a non-spam email being the word drugs, let's say?
30:01
Um, the chance of that is equal to [NOISE]
30:23
I find that- well, this indicates a function notation. It looks complex. I'll just say in a second,
30:30
uh, what this actually means.
30:35
So the denominator, um, so this space means- so- and so if you figure out what
30:41
the English meaning of this complicated formula is, this basically says, "Look at all the words in all of your non-spam emails,
30:48
all the emails of y equals 0, and look at all of the words in all of the emails, and so all of those words,
30:54
what fraction of those words is the word drugs?" And that's, uh, your estimate of the chance of the word drugs
31:00
appearing in the non-spam email in some position in that email, right? And so, um, in math,
31:06
the denominator is sum of your training set, indicator is not spam,
31:11
times the number of words in that email. So the denominator ends up being
31:17
the total number of words in all of your non-spam emails in your training set, um, and the numerator as some of your training set,
31:24
sum from i equals 1 through m, indicates a y equals 0.
31:29
So, you know, count up only the things for non-spam email, and for the non-spam email j equals 1 through ni,
31:37
go over the words in that email and see how many words are that word k. Right.
31:43
And so, uh, uh, if in your training set you have, um, uh, ah, you know,
31:48
100,000 words in your non-spam emails and 200 of them are the word drugs,
31:55
that occurs, uh, you know, 200 times, then this ratio will be 200 over 100,000. Okay? Oh, and then lastly, um,
32:06
[NOISE] to implement Laplace smoothing with this, you would,
32:12
um, add 1 to the numerator as usual, and then, um, let's see.
32:18
Actually, what- what- what- what would you add to the denominator? Uh-
32:30
Uh, wait. But what is k? Not k, right? k is a variable. So k indexes into, ah,
32:37
the words? What do you have?
32:43
About 10,000. 10,000. Cool. How come? Why 10,000? [inaudible].
32:51
Cool. Yeah. Yeah. All right. Yeah, Right.
32:57
Oh, I think I just realized why you say k I think, uh, overloading notation. When defining the possibility, I think I used k as the number of possible outcomes.
33:04
Yeah, but here k is an index. Yeah. Right? So, um, uh, see I want a numerator and add to number of the
33:11
possible outcomes in the denominator which in this case was there 10,000. So, um, uh, so this is the probability of, um,
33:20
X being equal to the value of k, where k ranges from 1-10,000 if you have a dictionary size.
33:30
If you have a list of 10,000 words you're modeling. And so the number of possible values for X is 10,000,
33:36
so you add 10,000 to the denominator. Makes sense? Cool. Yeah. Question?
33:42
[inaudible]. Oh, what do you do if the word's not in your dictionary? So, um, uh, there are two approaches to that.
33:50
One is, um, just throw it away. Just ignore it, disregard it, that's one. Uh, second approach, is to take the rare words and map them to
33:58
a special token which traditionally is denoted UNK for unknown words. So, um, if in your training set, uh,
34:06
you decide to take just the top 10,000 words in- into your dictionary, then everything that's not in the top 10,000 words can map to
34:12
your unknown word token or the unknown words special symbol. Yeah. [inaudible].
34:22
Oh, why did I write the run before? Oh, this is an indicator function notation. Uh, uh, so indicator function uh,
34:30
boy- so if- if, um, and so this is- this notation, right?
34:36
Means uh- well, so indicator of, you know, 2 equals 1 plus 1. This is true.
34:43
An indicator of, you know, 3 equals 5 is- is 0, is false.
34:50
So that's the- yeah, um, cool. Yes, uh, but this is a- this is a little formula
34:57
that's either true or false depending on whether y-i is 0. Uh, I guess if y-i is 01 this- this is the same as not y-i I guess,
35:06
so 1 minus y-i will give us 0- yeah. Cool. Okay great.
35:13
Um, all right. So I think both of the models, ah, ah, including the details that maximum likelihood estimate are written out in,
35:22
um, more detail in the lecture notes. Um, so, you know,
35:30
when would you use the Naive Bayes algorithm. It turns out Naive Bayes algorithm is actually not very competitive with other learning algorithms.
35:37
Uh, so for most problems you find that logistic regression,um, will work better in terms of delivering a higher accuracy than Naive Bayes.
35:47
But the- the- the advantages of Naive Bayes is, uh, first it's computationally very efficient,
35:54
and second it's relatively quick to implement, right? And it also doesn't require an iterative gradient descent thing,
35:59
and the number of lines of code needed to implement Naive Bayes is relatively small. So if you are, uh, facing a problem,
36:08
way you go is to implement something quick and dirty, then Naive Bayes is- is maybe a reasonable choice.
36:14
Um, and I think, um, you know as you work on your class projects,
36:20
I think some of you probably a minority will try to invent a new machine learning algorithm,
36:25
and write a research paper. Um, and I think, you know, inventing the machine learning algorithm is a great thing to do.
36:32
It helps a lot of people on a lot different applications so that's one. Um, the majority of class projects in CS229 will try
36:39
to apply a learning algorithm to a project that you care about.
36:44
Apply to a research project you're working on somewhere in Stanford or apply to a fun application you wanna
36:49
build or apply to a business application for some of you taking this on SCPD, taking this remotely.
36:55
And if your goal is not to invent a brand new learning algorithm, but to take the existing algorithms and apply them,
37:01
then rule of thumb that's suggested here is, um, ah, when you get started on a machine learning project,
37:08
start by implementing something quick and dirty. That's been implemented in most complicated possible learning algorithms.
37:14
Start by implementing something quickly, and, uh, train the algorithm, look at how it performs,
37:19
and then use that to deep out the algorithm, and keep iterating on- on that. So I think, you know,
37:25
we're- we're- that's at Stanford. So we're very good at coming up with very complicated algorithms. But if your goal is to make something,
37:32
um, work for an application, rather than inventing a new learning algorithm and publishing a paper on a new technical, you know, contribution.
37:39
If you- if your main goal is, uh, you're working on an application on- on understanding news better or improving the environment or estimating prices or whatever.
37:49
Uh, and your primary objective is just make an algorithm work. Then rather than, uh,
37:55
building a very complicated algorithm at the onset, um, I would recommend implementing something quickly,
38:02
uh, so that you can then better understand how it's performing, and then do error analysis which we'll talk about later,
38:08
and use that to drive your development. Um, you know one- one- one analogy I sometimes make is that,
38:16
um, if you are, uh, uh, let's see.
38:21
So if you're writing a new computer program with 10,000 lines of code, right? One approach is to write all 10,000 lines of code first,
38:30
and then to try compiling it for the first time, right. And that's clearly a bad idea, right? And it's a, you know, you should write small modules, run it,
38:38
it test it- unit testing, and then build up a program incrementally. Rather than write 10,000 lines of code, and then start to see what syntax errors you're getting for the first time.
38:46
Um, and I think it's similar for machine learning. Uh, instead of building a very complicated algorithm from the get-go, um,
38:53
you build a simpler algorithm, test it, and then- and then use the- see what it's doing wrong,
38:58
see what it's doing wrong to improve from there. You often end up, um, uh, getting to a better performing algorithm faster.
39:07
Um, so here's- here's- here's one example. This is actually something I used to work on. I- I actually started a conference on email and anti-spam.
39:15
My student worked on spam classification many years ago. And, um, it turns out that when your'e starting out on a new application problem,
39:24
um, it's hard to know what's the hardest part of the problem, right. So if you want to build an anti-spam classifier,
39:31
there are lots of you could work on. For example, spammers will deliberately misspell words.
39:36
Uh, you know, a lot of mortgage spam, right, refinance your mortgage or whatever. But instead of writing th- the words uh,
39:44
mortgage spammers will write M-0-R-T-G-A-G-E.
39:53
Right. Or instead of G-A-G-E, maybe, uh, slash slash, right.
39:59
But all of us as people have no trouble reading this as a word mortgage but uh, this will trip up a spam filter.
40:05
This might map the word to- to an unknown word. There it was off by just a letter and it hasn't seen this before,
40:11
and that's the lightest way to slip by this spam filter. So that's one idea for improving, um,
40:16
spam or- actually one of our PhD students [inaudible] actually wrote a paper mapping this back to words like that.
40:22
So the spam filter can see the words the way that humans see them, right. So- so that's one idea. Um, another idea might be a lot of spam email spoofs email headers.
40:32
[NOISE] You know, uh, spam has often tried to hide where the email truly came from, uh,
40:41
by spoofing the email header that, you know, address and other information. Um, ah, an- an- another thing you might do is, ah,
40:50
try to fetch the URLs that are referred to in the email, and then analyze the web pages that you get to.
40:55
Right, there are a lot of things that you could do to improve a spam filter. And any one of these topics could easily be three months or six months of research.
41:04
But when you are building say a new spam filter for the first time, how do you actually know which of these is the best investments of your time.
41:11
So my advice to, ah, those who work on projects, if your primary goal is to just get this thing to work,
41:17
is to not so-somewhat arbitrarily dive in, and spend six months on improving this or spend,
41:24
you know, six months on trying to analyze email headers. But you instead implement a more basic algorithm.
41:31
Almost implement something quick and dirty. And then look at the examples that your learning algorithm is still misclassifying.
41:37
And you'll find that, if after you've implemented a quick and dirty algorithm, you find that your sp- anti-spam algorithm is
41:43
misclassifying a lot of examples with these deliberately misspelled words. It's only then that you have more evidence that it's worth
41:49
spending a bunch of time solving the misspelled words, the deliberately misspelled words problem.
41:55
Right. When you implement a spam filter, and you see that it's not misclassifying a lot of examples of these misspelled words,
42:00
then I would say don't bother. Go work on something else instead or at least- at least treat that as a low priority.
42:06
Okay. So one of the uses of, um, GDA Gaussian discriminant analysis as well as Naive Bayes is that- is,
42:13
uh, they're not going to be the most accurate algorithms. If you want the highest classification accuracy,
42:19
their are other algorithms like logistic regression or SVM which we talked about, or neural networks we'll talk about later,
42:25
which will almost always give you higher classification accuracy than these algorithms. But the advantage of Gaussian discriminant analysis,
42:32
and Naive Bayes is that, um, they are very quick to train or it's non-iterative.
42:38
Uh, uh, this is just counting, and GDA is just computing means and co-variances, right. So it's very competition efficient,
42:44
and also they are- they are simple to implement. So it can help you implement that quick and dirty thing that helps you,
42:51
um, get going more quickly. And so I think for your project as well,
42:57
I would advise most of you to uh, uh, you know, as you start working on your project,
43:02
I would advise most of you to, um, don't spend weeks designing exactly what you're going to do.
43:08
Uh, if you have an applicant- if- if you- if you- if you have an applied project, but instead get a data set,
43:13
uh, and apply something simple. Start with logistic regression not- not a neural network or not- not something more complicated.
43:20
Or start with Naive Bayes, and then see how that performs, and then- and then go from there.
43:25
Okay? All right. So that's it for, uh, Naive Bayes, um,
43:32
and generative learning algorithms. The next thing I wanna do is move on to a different cla- type of classifier, ah,
43:40
which is a support vector machine. Um, let me just check any questions about this before I move on. Yeah.
43:47
[inaudible].
44:06
Sorry you can use logistic regression with. [OVERLAPPING] Discrete variables [inaudible]
44:21
Oh I see yeah right yes so yes, uh, right. So one of the weaknesses of
44:27
the Naive Bayes Algorithm is that it treats all of the words as completely, you know, separate from each other.
44:33
And so the words one and two are quite similar and the words, you know, like mother and father are quite similar.
44:39
Uh, and so wi- wi-with this, uh, feature representation, it doesn't know the relationship between these words.
44:45
So, um, in machine learning there are other ways of representing words, uh, there's a technique called word embeddings,
44:51
um-[NOISE] In which you
44:57
choose the feature representation that encodes the fact that the words one and two are quite similar to each other.
45:03
Uh, the words mother and father are quite similar to each other. Yeah the words, um, whatever London and Tokyo are
45:10
quite similar to each other because they are both city names. Uh, and so, uh, this is a technique that I was not planning to teach here but that is taught in CS 230.
45:20
So in- in- in neural networks [NOISE] , right, but you can also read up on word embeddings or look at some of
45:27
the videos and resources from CS 230 if you want to learn about that. Uh, so the word embeddings techniques.
45:33
These are techniques from neural networks really. Will reduce the number of training examples you need so they are a good text classifier because it comes in with
45:40
more knowledge baked in, right. Cool. Anything else?
45:45
[NOISE] Cool.
45:50
By the way I do this in the other classes too. In some of the other classes, somebody's got a question they go, no we don't do that we just covered that in
45:57
CS 229 so [LAUGHTER].
46:11
Actually CS224N I think also covers this. Yeah, The NLP class, yeah, pretty sure, actually I am sure they do.
46:22
Okay so, [NOISE] su-support vector machines, SVMs.
46:34
Um, let's say the classification problem,
46:41
[NOISE].
46:49
Right, where the data set looks like this, uh, and so you want an algorithm to find, you know,
46:55
like a nonlinear decision boundary, right? So the support vector machine will be an algorithm to help us
47:02
find potentially very very non-linear decision boundaries like this. Now one way to build a classifier like this would be to use logistic regression.
47:11
But if this is X 1, this is X 2, right, so logistic regression will fit the three lines of data,
47:19
Gaussian discriminant analysis will end up with a straight line decision boundary. So one way to apply logistic regression like this would be to take
47:25
your feature vector X 1 X 2 and map it to a high dimensional feature vector with,
47:31
you know, X 1, X 2, X 1 squared, X 2 squared X 1, X 2 maybe X 1 cubed,
47:38
X 2 cubed and so on. And have a new feature vector which we would call phi of x. That- that has these high-dimensional features right, now, um,
47:48
it turns out if you do this and then apply logistic regression to this augmented feature vector, uh,
47:55
then logistic regression can learn non-linear decision boundaries. Uh, with these other features it's just regression
48:01
and you actually learn the decision boundary. This is- there's a- there's a shape of an ellipse, right. Um, but randomly choosing
48:08
these features is little bit of a pain right. I- I- I don't know. What I- I- I actually don't know what,
48:14
you know, type of a, uh, set of features could get you a decision boundary like that right.
48:20
Rather than just an ellipse and more complex as your boundary. Um, and what we will see with support vector machines is that we
48:28
will be able to derive an algorithm that can take say input features X 1, X 2,
48:33
map them to a much higher dimensional set of features. Uh, and then apply a linear classifier,
48:41
uh, in a way similar to logistic regression. But different in details that allows you to learn very non-linear decision boundaries.
48:48
Okay. Um, and I think, uh, you know, a support vector machine, one of the- actually one of the reasons, uh,
48:55
support vector machines are used today is- is a relatively turn-key algorithm. And what I mean by that is it doesn't have too many parameters to fiddle with.
49:03
Uh, even for logistic regression or for linear regression. You know you might have to tune the gradient descent parameter,
49:11
uh, tune the learning rate sorry, tune the learning rate alpha. And that's just another thing to fit in with. We`ll try a few values and hope you didn't mess up how you set that value.
49:19
Um, support vector machine today has a very, uh,
49:24
robust, very mature software packages that you can just download to train the support vector machine on- on any on,
49:30
you know, on a problem and you just run it and the algorithm will, kind of, converge without you having to worry too much about the details.
49:37
Um, so I think in the grand scheme of things today I would say support vector machines are not as effective as neural networks for many problems.
49:45
But, um, uh, but one great property of support vector machines is- is- is turn key.
49:50
You kind of just turn the key and it works and there isn't as many parameters like the learning rate and other things that you had to fiddle with.
49:57
Okay, um so the road map is,
50:08
uh, we're going to develop the following set of ideas. We talked about the optimal [NOISE] margin classifier today, and, uh,
50:20
we'll start with the separable case
50:26
and what that means is going to start off with datasets, um, that we assume look like this and that are linearly separable.
50:35
Right, and so the optimal margin classifier is the basic building block for the support vector machine,
50:41
and, uh, we'll first derive an algorithm, uh, that' ll be- that will have some similarities to
50:47
logistic regression but that allows us to scale, uh, in important ways that to find a linear classifier
50:55
for training sets like this that we assume for now can be linearly separated. Um, so we'll do that today.
51:01
And then what you'll see on Wednesday is, um, excuse me, next Monday,
51:07
which is next Monday is an idea called kernels. And the kernel idea is one of the most powerful ideas in machine learning.
51:15
Is, um, how do you take a feature vector x, maybe this is R 2,
51:20
right, and map it to a much higher dimensional set of features.
51:27
In our example there that was R 5, right, and then train an algorithm on this high dimensional set of features.
51:36
And- and the cool thing about kernels is that this high dimensional set of features may not be R 5.
51:41
It might be R100,000 or it might even be R infinite.
51:47
Um, and so with the kernel formulation we're gonna take our original set of features that you are given for the houses you're trying to sell.
51:56
For, uh, you know, medical conditions you're trying to predict and map this two-dimensional feature vector space
52:02
into maybe infinite dimensional set of features. And, um, what this does is it relieves us
52:08
from a lot of the burden of manually picking features, right, like do you want to have square root of X 1
52:13
or maybe X 1, X 2 to the power of two thirds. So you just don't have to fiddle with these features too much
52:19
because the kernels will allow you to choose an infinitely large set of features.
52:24
Okay, um, and then finally, uh, we'll talk about the inseparable case.
52:30
[NOISE] So we're gonna do this today and then
52:35
this next, uh, Monday okay.
52:53
So [NOISE] and by the way I, you know, th-the machine learning world has become a little bit funny.
53:00
I think that if you read in the news the media talks a lot about machine learning, the media just talks about,
53:07
you know, neural networks all the time, right? And you'll hear about neural networks and deep learning a little bit later in this class. But if you look at what actually happens in practice in machine learning.
53:16
Uh, the set of algorithms is actually used in practice, is actually much wider than neural networks and deep learning.
53:22
So- so we do not live in a neural networks only world. We actually use many, many tools in machine learning.
53:28
It's just that deep learning attracts the attention of the media in some way there's quite disproportionate to what I find useful, you know,
53:38
I knew that's like- I loved that, you know but- but they're not- they're not the only thing in the world,
53:43
uh, and so yeah and then late last night I was talking to an engineer, uh, about factor analysis which we'll learn about later in CS229
53:51
right, there's an unsupervised learning algorithm and there's an application, uh, that one of my teams is working on in manufacturing.
53:58
Where we're gonna use factor analysis or something very similar to it. Which- which is totally not a neural network technique.
54:04
Right. But still there, there are all these other techniques that including support vector machines and Naive Bayes I think do get used and are important.
54:14
All right so let's start developing the optimal margin classifier.
54:20
[NOISE]
54:26
So, um, first, let me define the functional margin,
54:32
which is, uh, informally, the functional margin of the classifier is how well- how,
54:38
how confidently and accurately do you classify an example. Um, so here's what I mean.
54:43
Uh, we're gonna go to binary classification, and we're gonna use logistic regression, right?
54:50
So, so let's, let's start by motivating this with logistic regression [NOISE].
54:56
So this, this is a classifier H of theta equals the logistic function of pi to theta transpose x.
55:01
And so, um, if you turn this into a binary classification, if, if,
55:07
if you have this algorithm predict not a probability but predict 0 or 1, then what this classifier will do is, uh, predict 1.
55:17
If theta transpose x is greater than 0, right?
55:23
Um, and predict 0 otherwise.
55:29
Okay. Because theta transpose x greater than 0, this means that, um, g of theta transpose x is greater than 0.5 [NOISE],
55:40
and you can have greater than or greater than equal to, it doesn't matter. It is, it's exactly 0.5, it doesn't really matter what you do.
55:46
Um, and so you predict 1 if theta transpose x is greater than equal to 0,
55:52
meaning that the upper probability- the estimated probability of a class being 1 is greater than 50/50, and so you predict 1.
55:58
And if theta transpose x is less than 0, then you predict that this class is 0. Okay. So this is what will happen if you have, um,
56:06
logistic regression output 1 or 0 rather than output a probability, right.
56:11
So in other words, this means that if y_i is equal to 1, right?
56:21
Then hope or we want that
56:27
theta transpose x_i is much greater than 0.
56:33
Uh, this double greater than sign, it means much greater, right? Um, uh, because if the true label is 1,
56:42
then if the algorithm is doing well, hopefully theta transpose x, right?
56:48
Will be faster there, right? So the output probability is very, very close to 1. And if indeed theta transpose x is much greater than 0,
56:56
then g of theta transpose x will be very close to 1 which means that is,
57:01
it's giving a very good, very accurate prediction. Very correct and confident prediction, right?
57:07
That, that equals 1. Um, and if y_i is equal to 0,
57:13
then what we want or what we hope, is that theta transpose xi is much less than 0, right?
57:21
Because, uh, if this is true, then the algorithm is doing very well on this example. Okay.
57:29
So, um.
57:38
So the functional margin which we'll define in a second, uh, captures [NOISE] this idea that if a classifier has a large functional margin,
57:47
it means that these two statements are true, right? Um, so looking ahead a little bit,
57:55
there's a different thing we'll define in a second which is called the geometric margin and that's the following.
58:03
And for now, let's assume the data is, is linearly separable.
58:09
Okay. Um, right.
58:15
So let's say that's the data set. [NOISE] Now,
58:20
[NOISE]
58:29
that seems like a pretty good decision boundary for separating the positive [NOISE] and negative examples. [NOISE] Um, that's another decision boundary in red,
58:39
that also separates the positive negative examples. But somehow the green line looks much better than the red line, okay?
58:46
So, uh, why is that? Well, the red line comes really close to a few of the training examples,
58:54
whereas the green line, you know, has a much bigger separation, right?
59:02
Just has a much bigger distance from the positive and negative examples. So even though the red line and the [NOISE] green line both, you know,
59:10
perfectly separate the positive and negative examples, the green line has a much bigger separation,
59:18
uh, which is called the geometric margin. But there's a much bigger geometric margin meaning a physical separation from the trained examples even as it separates them.
59:27
Okay. Um, and so what I'd like to do in the,
59:32
uh, next several, I guess in the next, next, next 20 minutes is formalize definite functional margin,
59:38
formalize definition geometric margin, and it will pose the, the, I guess the optimal margin classifier which based in
59:45
the algorithm that tries to maximize the geometric margin. So what the rudimentary SVM does,
59:50
what the SVM and low-dimensional spaces will do, also called the optimal margin classifier, is pose an optimization problem to try to find
59:58
the green line to classify these examples, okay? So, um, [NOISE] now,
1:00:09
um, in order to develop SVMs, I'm going to change the notation a little bit again. You know, because these algorithms have different properties, um,
1:00:18
using slightly different notation to describe them, makes then the math a little bit easier. So when developing SVMs,
1:00:25
we're going to use, um, minus 1 and plus 1 to denote the class labels.
1:00:33
And, um, we're going to have a H output.
1:00:47
So rather than having a hypothesis output a probability like you saw in logistic regression,
1:00:54
the support vector machine will output either minus 1 or plus 1. And so, uh, g of z becomes minus 1 or 1, um, actually.
1:01:07
So output 1 if z is greater than equal to 0, and minus 1 otherwise, okay.
1:01:16
So instead of a smooth transition from 0 to 1, we have a hard transition, an abrupt transition from negative 1 to, um, plus 1.
1:01:26
[NOISE]
1:01:37
And finally, where previously we had for logistic regression, right?
1:01:48
Where, uh, this was R N plus 1 with x_0 equals 1.
1:01:54
For the SVM, we will have h of, I'll just write this out.
1:02:14
Okay. Um, so for the SVM, the parameters of the SVM will be the parameters w and b.
1:02:22
And hypothesis applied to x will be g of this, and where dropping the x_0 equals 1 [NOISE] constraint.
1:02:30
So separate out w and b as follows. So this is a standard notation used to develop support vector machines.
1:02:37
Um, and one way to think about this, is if the parameters are, you know, theta 0, theta 1,
1:02:42
theta 2, theta 3, then this is a new b, and this is a new w. Okay?
1:02:48
So you just separate out the, the, the, uh, theta 0 which was previously multiplying to x_o, right?
1:02:59
And so um, uh, yeah, right. And so this term here becomes sum from i equals 1 through N,
1:03:08
uh, w_i x_i plus b, right? Since we've gotten rid of [NOISE] x_0.
1:03:20
[NOISE].
1:03:36
All right. So let me formalize the definition of a functional margin.
1:03:41
[NOISE] So um, ah,
1:03:49
so the parameters w and b are defined as linear classifier, right, so you know,
1:03:55
wh- what- the formulas we just wrote down the parameters w and b, defines the a- a, uh, uh,
1:04:00
re- really defines a hyperplane. Ah, but defines a line, or in high dimensions it'd be a plane or a hyperplane that defines a straight line,
1:04:08
ah, ah, separating out the positive and negative examples. And so we're gonna say the functional margin of the,
1:04:15
actually my hyperplane [NOISE]
1:04:42
Okay, so the functional margin of a hyperplane defined by this with respect to one training example.
1:04:50
We're going to write as this, um, and hyperplane just means straight line,
1:04:55
right, but in high dimension. So this is linear classifiers, so its just, you know, functional margin of this classifier with respect to one training example,
1:05:02
we're going to define as this. And so if you compare this with the equations we had up there,
1:05:09
um, you know, if y equals 1 we hope for that, if y equals 0, we hope for that. So really what we hope for is for
1:05:16
our classifier to achieve a large functional margin, right? And so, um, so if y_i equals 1 then what we want or what we hope for,
1:05:29
um, is that w transpose x_i plus b is greater than,
1:05:36
much greater than 0, and that the label is equal to minus 1. [NOISE] Then we want or we hope that [NOISE] this is much smaller than 0.
1:05:49
Um, and if you, kind of, combine these two statements, if you take y_i,
1:05:57
right, and multiply it with, er, that, [NOISE] then, you know,
1:06:04
these two statements together is basically saying that you hope that Gamma hat i is much greater than 0, right,
1:06:11
because y_i now is plus 1 or minus 1 and, uh, uh, and so y is equal to 1 you want this to be very, very large.
1:06:19
If y_i is negative 1, you want this to be a very, very large negative number. Um, and so either way it's just saying that you hope this would be very large, okay?
1:06:29
So we just hope that. [NOISE] And- and as an aside,
1:06:40
ah, one property of this as well is that, um,
1:06:48
so long as Gamma hat i is greater than 0, that means the algorithm,
1:06:55
um, right, is equal to y_i.
1:07:02
[NOISE] Ah, so- so- so long as the, um, functional margin,
1:07:07
so long as this Gamma hat i is greater than 0, it means that, ah, either this is bigger than 0,
1:07:14
this is less than 0 depending on the sign of the label. And it means that the algorithm gets
1:07:20
this one example correct at least, right? And if- if much greater than 0 then it means, you know,
1:07:26
so if it is greater than 0 it means in- in the logistic regression case it means that, the prediction is at least a little bit above 0.5,
1:07:32
a little bit below 0.5, probably 0 so that at least gets it, right? And if it is much greater than 0 much less than 0,
1:07:38
then that means it, you know, the probability of output in the logistic regression case is either very close to 1 or very close to 0.
1:07:46
So one other definition, [NOISE]
1:08:00
I'm gonna define the functional margin with respect to the training set to be Gamma hat, equals min over i of Gamma hat i,
1:08:09
where here i [NOISE] equals ranges over your training examples. Okay. So, um, this is a worst-case notion,
1:08:16
but so this definition of a function margin, on the left we defined functional margins with respect to a single training example,
1:08:23
which is how well are you doing on that one training example? And we'll define the function margin with respect to the entire training set as,
1:08:29
how well are you doing on the worst example in your training set? Okay, ah, this is a little bit of a plateau notion and we're for now,
1:08:37
for today, we're assuming that the training set is linearly separable. So we're gonna assume that the training set,
1:08:42
you know, it looks like this. [NOISE] And that you can separate it on a straight line [NOISE] that will relax this later,
1:08:49
but because we're assuming, just for today, that the training set is, um, linearly separable,
1:08:54
we'll use this kind of worst-case notion and define the functional margin to be the functional margin of the worst training example.
1:09:01
Okay? [NOISE]. Now, one thing
1:09:11
about the definition of the functional margin is, it's actually really easy to cheat and increase the functional margin, right?
1:09:18
And one thing you can do, um, in regards to this formula is if you take w,
1:09:23
you know, and multiply it by 2 and take b and multiply it by 2.
1:09:29
[NOISE] then, um, everything here just multiplies by two and you've doubled the functional margin,
1:09:36
right, but you haven't actually changed anything meaningful. Okay, so- so one, one way to cheat on the functional margin is just by scaling
1:09:43
the parameters by 2 or instead of 2 maybe you can [NOISE] multiply all your parameters by 10 and then you've actually
1:09:49
increased the functional margin of your training examples as 10x, but, ah, this doesn't actually change the decision boundary, right?
1:09:55
It doesn't actually change any classification, just to multiply all of your parameters by a factor of 10.
1:10:00
Um, so one thing you could do is, ah,
1:10:06
replace, one thing you could do,
1:10:11
um, would be to normalize the length for your parameters. So for example, hypothetically you could impose a constraint,
1:10:20
the normal w is equal to 1, or another way to do that would be to take w and b and replace it with w
1:10:28
over normal b and replace b with, [NOISE] right,
1:10:33
just the value of parameters through by the magnitude, by the- by the Euclidean length of the parameter vector w,
1:10:40
and this doesn't change any classification, It's just rescaling the parameters. Ah, but, ah, but, but that it prevents, you know,
1:10:46
display of cheating on the functional margin. Okay. Um, and in fact,
1:10:53
more generally you could actually scale w and b by any other values you want and- and it doesn't- doesn't matter, right?
1:10:59
You can choose to replace this by w over 17 and b over 17 or any other number or any,
1:11:06
right, and the classification stays the same. Okay. So we'll come back and use this property, in a little bit. Okay. [NOISE]
1:11:24
All right. So to find the functional margin, let's define the geometric margin. An- and you'll see in a second how
1:11:30
the geometric and functional margin relate to each other. Um, so les- let's,
1:11:36
let's define the, uh, geometric margin with respect to a single example. Which is, um- so let's see- let's say you have a classifier.
1:11:49
All right, so given parameters w and b that defines a linear classifier and the equation,
1:11:56
wx plus b equals 0 defines the equation of a straight line. Uh, so the axes here are x_1 and x_2,
1:12:03
and then half of this plane you know, in this half of the plane, you'll have w transpose x plus b is greater than 0.
1:12:10
And in this half, you'll have w transpose x plus b is less than 0. And in between this- the straight line given by
1:12:18
this equation w transpose x plus b equals 0, right. And so given parameters w and b, the upper right,
1:12:25
that's where your cost high will predict y equals 1 and the lower left is where it'll predict y is equal to negative 1, okay.
1:12:33
Now, let's say you have one training example here, right? So that's a training example, x_i, y_i.
1:12:43
And, uh, let's say it's a positive example, okay?
1:12:51
And so, um, your classifiers classify this example correctly, right?
1:12:58
Because in the upper right half- half plane- here in this half plane w transpose x plus b is greater than 0.
1:13:05
And so in the- in this upper-right region, uh, your classifier is predicting plus 1, right?
1:13:13
Whereas in this lower half region would be predicting h of x equals negative 1.
1:13:19
Right, and that's why this straight line where it switches from predicting negative to positive is the decision boundary.
1:13:26
So what we're going to do is define this distance, um,
1:13:35
to be that geometric margin of this training example,
1:13:42
is the Euc- the Euclidean distance is what will define to be the geometric margin.
1:13:47
So let me just write down what that is. [NOISE]
1:14:03
So the geometric margin of,
1:14:10
you know, the classifier of the hyperplane defined by w, b with respect to one example x_i, y_i.
1:14:20
This is going to be gamma i equals w transpose x plus b over [NOISE] the normal w. [NOISE]
1:14:32
Um, and let's see I'm not proving why this is the case, the proof is given in the lecture notes but, uh,
1:14:38
the lecture notes shows why this is the right formula for measuring the Euclidean distance that I just drew [NOISE] in the picture up there, okay.
1:14:45
Uh, but, and then, I'm not proving this here but the proof is given in the lecture notes but this turns out to be the way you compute the Euclidean distance between that example and uh,
1:14:54
and the decision boundary, okay? Um, and uh, a- and this is [NOISE] for the positive example I guess.
1:15:04
Uh, more generally, um, I'm going to define the geometric margin to be equal to this,
1:15:16
uh, and this definition applies to positive examples and the negative examples, okay?
1:15:23
And so the relationship between the geometric margin and the functional margin is
1:15:28
that the geometric margin is equal to the functional margin divided by
1:15:34
the norm of w. [NOISE]
1:15:52
Finally, um, the geometric margin with respect to the training set is, um,
1:16:10
where again uses worst-case notion of, uh- look through all your training examples and pick the worst possible training example,
1:16:19
um, and that is your geometric margin on the training set.
1:16:25
Uh, an- and so I hope the- sorry, I hope the notation is clear, right. So gamma hat was the functional margin and
1:16:35
gamma is the geometric margin,
1:16:44
okay? And so, um,
1:17:06
what the optimal margin classifier does is [NOISE] ,
1:17:17
um, choose the parameters w and
1:17:23
b to maximize the geometric margin, okay?
1:17:33
Um, so in other words, thi- this- the optimal margin classifiers is the baby SVM, you know, it's like,
1:17:39
a SVM for linearly separable data, uh, at least for today.
1:17:46
[NOISE] And so the optimal margin classifier will choose that straight line, because that straight line maximizes the distance or maximizes the geometric margin
1:17:55
to all of these examples, okay? Now, uh, how you pose this mathematically,
1:18:04
there are a few steps of this derivations I don't want to do but I'll, I'll just describe the beginning step and
1:18:10
the last step and leave the in bet- in between steps to the lecture notes. But it turns out that, um,
1:18:16
one way to pose this problem is to maximize gamma w and b of gamma.
1:18:23
So you want to maximize the geometric margin subject to that.
1:18:42
Subject to the every training example, um, uh, must have geometric margin,
1:18:49
uh, uh, greater than or equal to gamma, right? So you want gamma to be as big as possible subject to
1:18:56
that every single training example must have at least that geometric margin. [NOISE] This causes you to maximize the worst-case geometric margin.
1:19:03
And it turns out this is, um- not in this form, this is in a convex optimization problems.
1:19:09
So it's difficult to solve this without a gradient descent and initially known local optima and so on. But it turns out that via a few steps of V writing,
1:19:16
you can reformulate this problem as, um, into the equivalent problem which is a minimizing norm of w subject
1:19:24
to the geometric margin, right.
1:19:32
Um, so it turns out- so I hope this problem makes sense, right? So this problem is just you know solve for w and b to make sure that
1:19:40
every example has a geometric margin greater or equal to gamma and you want gamma to be as big as possible.
1:19:45
So this is the way to formulate optimization problem that says, ''Maximize the geometric margin.''
1:19:50
And what we show in the lecture notes is that, uh, through a few steps, uh,
1:19:56
you can rewrite this optimization problem into the following equivalent form which is to try to minimize the norm of w, uh, subject to this.
1:20:04
And maybe one piece of intuition to take away is, um, uh, the smaller w is the bigger, right?
1:20:11
Th- th- the, the less of a normalization division effect you have, right? Uh, but the details I gave you in the lecture notes, okay?
1:20:19
Um, but this turns out to be a convex optimization problem and if you optimize this, then you will have the optimal margin classifier and they're
1:20:27
very good numerical optimization packages to solve this optimization problem. And if you give this a dataset then, you know,
1:20:33
assuming your data's separable [NOISE] and we'll fix that assumption, uh, when we convene next week, then you have the optimal management classifier
1:20:40
which is really a baby SVM and we add kernels to it, then you have the full complexity of the SVM norm, okay?
1:20:47
All right, let's break for today, uh, see, see you guys next Monday.