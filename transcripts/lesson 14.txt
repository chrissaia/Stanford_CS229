Introduction
0:03
All right. [NOISE] Um, let's get started. So, um, let's see, logistical reminder,
0:11
uh the class midterm, um, is this Wednesday and it's 48-hour take-home midterm.
0:17
Um, and the logistical details you can find, uh, at this Piazza post, okay?
0:23
So the midterm will start Wednesday evening. You have 48 hours to do it and then submit it online through Gradescope, uh,
0:29
and because of the midterm, there won't be a section, uh, this Friday, okay?
0:34
Oh and the midterm will cover everything up to and including EM, uh, which we'll spend most of today talking about, okay?
0:41
Certainly don't look so stressed. It'll be fun. [LAUGHTER]. Maybe. All right.
0:47
Um, so what I'd like to do today is start our foray into, uh, unsupervised learning.
0:54
Uh, so far I've spent a lot of time on supervised learning algorithms including advice on
0:59
how to apply supervised learning algorithms. These pens are great. In which you'd have, you know,
1:05
positive examples and negative examples and you run logistic regression or something or SVM or
1:12
something to find the line- find the decision boundary between them. Um, in unsupervised learning,
Unsupervised learning
1:18
you're given unlabeled data. So rather than given data with x and y,
1:24
you're given only x. And so your training set now looks like X1, X2, up to Xm.
1:34
And you're asked to find something interesting about the data. Uh, so the first unsupervised learning algorithm we'll talk about
First unsupervised learning algorithm
1:40
is clustering in which given a dataset like this, hopefully, we can have an algorithm that can figure
1:47
out that this dataset has two separate clusters. Um, and so one of the most common uses of clustering is, uh, market segmentation.
Market Segmentation
1:56
If you have a website, you know, selling things online, you have a huge database of many different users and run clustering
2:02
to decide what are the different market segments, right? So there may be, you know, people of a certain age range, of a certain gender,
2:08
people of a different age range, different level of education, people that live in the East Coast versus West Coast versus other parts of the country.
2:14
But by clustering you can group people into, uh, different groups, right?
2:19
So, um, I want to show you an animation of, um,
2:24
really the most commonly used er, er, clustering algorithm called k-means clustering.
2:30
And let me show you an animation of what k-means does and then we'll write- write out the math an- an- and tell you how you can implement it.
2:37
So, um, let's say you're given data like this. So all these are unlabeled examples.
2:42
Uh, so just x plotted here. And we want an algorithm to try to find maybe the two clusters here.
2:47
Uh, the first step of k-means is to pick two points denoted by the two crop- two crosses called cluster centroids and,
2:56
uh, the cluster centroids are your best guess for where the centers of the two clusters you're trying to find.
3:01
And then k-means is an iterative algorithm and repeatedly you do two things. So first thing is, go through each of your training examples.
3:09
Oh I'm sorry. Oh okay. Thank you. All right. Let me know if that happens again. Okay. Right. So, uh, you guys saw that, right?
3:16
So. Right near two cluster centroids. So the first thing you do is go through each of your training examples, the green dots and for each of them you color them either red
3:24
or blue depending on which is the closer cluster centroid. So here we've taken every dot and colored it in red or blue
3:31
depending on which side it is- which cluster centroid it's closer to. And then, uh, the second thing you do, uh, is,
3:39
uh, look at all the blue dots and compute the average, right? Just find the mean of all the blue dots,
3:45
um, and move the blue cluster centroid there. And similarly, look at all the red dots- and look at only the red dots
3:52
and find a mean- finding the- oh now what's wrong with this? Let's say- oh this thing though is very strange.
3:58
Right. Apparently, if I keep moving my mouse, it doesn't do that. All right. Thank you. Uh, and then find the mean of all the red dots and move your,
4:06
uh, red cluster centroid there. So let me do that, right? So the cluster centroids move as follows, um,
4:14
to the mean of the red and the blue dots and this is just a standard arithmetic average, right? Uh, and then you repeat again where you, er,
4:22
look at each of the dots and color it either red or blue depending on which cluster centroid is closer.
4:28
So when I recolor every point based on, you know, what's closer, so that's the new set of colors, right?
4:35
Um, and then the second part of the algorithm was again look at the blue dots, find the mean,
4:41
look at the red dots, find the mean, and then move the cluster centroids over. [NOISE] Excuse me, uh,
4:47
to that mean, okay? Um, and so, er, and it turns out if you keep running the algorithm, nothing changes.
4:55
So the algorithm has converged. So if you look at this picture and you repeatedly color each point red or blue depending on which cluster centroid is closer, nothing changes.
5:04
And you repeatedly look at each of the two clusters of color dots and compute a mean and move the clu- clu- clusters there, nothing changes.
5:11
So this algorithm has converged even if you keep on running these two steps, okay? So um, let's see.
5:20
Let's write down in math what we just did. [NOISE] All right. So this is,
5:31
um, a clustering algorithm and specifically this is a k-means clustering algorithm.
K-means clustering
5:42
So your dataset now does not come with any labels.
5:48
Um, and so in, uh, k-means, step one is, uh, initialize the cluster centroids, right?
Initialize the cluster centroids
6:02
I'm gonna call them Mu_1 up to Mu_k, uh, randomly, okay?
6:11
So this was a step where you plop down the red cross and the blue cross. Uh, and when they did it on the PowerPoints, you know,
6:19
I did it as if we're just choosing these as random vectors. In practice a good way of the- actually the most common way
6:26
to select a random initial cluster centroid isn't quite what I showed, is to actually pick k examples out of your training set and just
6:33
set the cluster centroids to be equal to k randomly chosen the examples, right? So in a low-dimensional space like a 2D plot,
6:40
you know, you can do on the diagram, it doesn't really matter but when you work with very high dimensional datasets,
6:45
the more common way to initialize these is just pick, you know, k training examples and set the cluster centroids to be
6:51
at exactly the location of those examples. But then low dimensionless spaces it- it- you know,
6:58
it doesn't make a big difference. Um, and then next you repeat until convergence.
7:04
Um, step one is-
7:28
right? So this is a- well
7:38
now I'll just write this down,
8:14
okay? Um, so does that make sense?
8:20
So the two steps you would alternate between the first one is set Ci for every value of i.
8:26
So for every example, set Ci equal to, you know, either 1 or 2 depending on whether, er,
8:33
that example Xi is closer to cluster centroid one or cluster centroid two, right? So ju- just take each point and color either red or blue.
8:40
Uh, or and represent that by setting Ci equals 1 or 2,
8:45
er, if you have two clusters. If k is equal to 2, right? Oh yeah. [inaudible]
8:56
Oh. Er, the notes say L1 norm squared?
9:02
From this morning? Uh, what notes were sent out this morning?
9:08
[inaudible]. Oh that's red. It shouldn't be L1 norm.
9:14
Uh, if it says L1 norm, that's a mistake. Sorry about that. Er, but usually- an- and it turns out whether you use
9:22
L2 norm and L2 norm squared that gives you the same answer because the algorithm is the same either way. But it is usually- do we have a typo on the notes?
9:28
[inaudible]. Oh I see. Oh got it. Oh- oh- oh okay.
9:34
Let's say in notes we wrote that. Okay. Cool. But by default, when we write that norm we actually use- we mean L2 norm.
9:40
Yeah, right? But by- by default this is the L2 norm of x if is unspecified.
9:46
Er, if it's L1 norm, we usually write this. So L2 norm is more common and with or without the square it you get the same result.
9:53
Okay. Cool. Thank you. All right. So let's color the dots. Um, paint each dot either red or blue.
10:01
Uh, and then, um, uh, for this, um, this is,
10:07
you know, some key examples and take all the examples assigned to a certain cluster, right?
10:12
Assigned to cluster j and set Mu_j to be average of all the points assigned to that cluster J. Yeah. [inaudible]
10:20
Oh sure. Er, no that does not work.
10:27
Uh, you know, I don't think -I don't know if I have- all right.
10:34
Now, that the black markers are working, um, this is better? All right let me try to use this?
10:40
Is there a part of this that's unclear? If this part you can't see, I'll write it out more clearly.
10:45
Yes. Go ahead. Let's go ahead. [inaudible]. Oh sure. How I do it, lights in front?
10:51
[inaudible].
11:01
Got it. Let there be light. All right. Awesome great.
11:07
That was an easy request to satisfy, great, you know, okay. I guess we'll actually look at it for another minute.
11:14
All right. Is that okay? Thank you.
11:26
Okay. This wasn't part of it.
11:32
Okay. All right. Now, I can move it up. [NOISE].
11:45
All right. Um, so it turns out that, uh, um, this algorithm can be proven to converge.
11:52
Um, the, exactly why it is written out in the lecture notes. But it turns out, if you write this as
11:57
a cost function, right?
12:08
Um, so the cost function for a certain set of assignments of, uh,
Cost function
12:14
points of examples to cluster centroids and for a certain set of positions of the cluster centroids.
12:19
So, so c, these are the assignments and these are the centroids, right?
12:29
So, so this cost here is sum of your training set of what's the square distance between each point,
12:35
and the cluster centroid it is assigned to, right? So it turns out, um, I want to prove this, uh,
12:41
little bit more detail in lecture notes but I'm going to prove this. It turns out that on every iteration, k-means would drive this cost function down,
12:49
um, and so, you know, beyond a certain point this cost function, it can't go even, it can't go, uh, uh, any lower.
12:55
Well, this, this can't go below 0, right? And so this shows that k-means must converge, or at least this function must converge because it's, uh, a
13:02
strictly non-negative function that's going down on every iteration. So at some point, it has to stop going down,
13:08
and then you could declare k-means are converged. Um, in practice, if you run k-means in a very,
13:13
very large data set, then as you plot the number of iterations, uh, j may go down,
13:19
and you know, and, and just because of, a lack of compute or lack of patience, you might just stop this running after a while.
13:25
It is going down too slowly. So that's sort of k-means in practice where maybe it hasn't totally converged, we just cut it off and call it good enough.
13:33
Um, now, uh, uh, the most frequently asked question I get for k-means is how do you choose k?
13:40
It turns out that, um, when I use k-means, I still usually choose k by hand.
13:46
And so, and, and this is why. Which is in unsupervised learning, um, sometimes it's just ambiguous, right?
13:54
How many clusters there are [NOISE].
14:01
Right? Um, with this dataset, some of you will see two clusters, and some of you will see four clusters,
14:09
and it's just inherently ambiguous what is the right number of clusters. So there are some formulas you can find online,
14:16
the criteria like AIC and BIC for automatically choosing the number of clusters. In practice, I tend not to use them because, uh, um,
14:23
I usually look at the downstream application of what you actually want to use k-means for in order to make a decision on the number of clusters.
14:30
So for example, if you're doing a market segmentation, um, you know, because your marketers want to design different marketing campaigns, right?
14:38
For different groups of users, then your marketers might have the bandwidth to design four separate marketing campaigns,
14:44
but not 100 marketing campaigns. So that would be a good reason to choose four clusters rather than 100 clusters. So it's often, uh, uh,
14:50
if you look at the purpose of what you're doing this for. Um, I think in the previous exercise, uh, in the homework, you see a, um,
14:56
image compression, uh, exercise where you want to cluster, uh, colors into smaller number of clusters.
15:03
You implement this. This is actually one of the most fun exercises I think. Um, uh, uh, that, uh,
15:08
uh, but so there you'd, you know, be saying, well, how much do you want to compress the image to decide how many clusters to,
15:14
to try to use, okay? So I usually, um, pick the number of clusters, you know, either manually or looking at what you want to use k-means cluster for.
15:23
Um, when we're trying to cluster news articles, uh, the Google News example, I think I showed in the first lecture. You say, well, how many clusters is going to make sense for,
15:30
for, for news articles, okay? All right. So good. So, uh, yeah? [inaudible].
15:41
Oh sure. Well, k-means get stuck on local minima. Yes, k-means gets stuck on sort of local minima sometimes.
15:47
And so, if you're worried about local minima, the thing you can do is, uh, run k-means, say, 10 times, or 100 times,
15:54
or 1000 times from different random initializations of the cluster centroids. And then run it, you know, say 100 times, uh,
16:00
and then pick whichever run results in the lowest value for this cost function, okay?
16:09
All right. Um, so you'll play with this more in, um, uh, in the programming exercise.
16:18
Now, um, there's a, there's a problem that seems closely related.
16:24
Um, but, but it's actually
16:29
quite different ways to write the algorithms which is density estimation. So, so let me motivate this.
Density Estimation
16:36
Um, I actually have a- well, right, sometime back had some friends working on a problem which I'll simplify a little bit,
16:44
um, of, uh, uh, you know, you have aircraft engines coming off the assembly line. All right. And every time an aircraft engine comes off the assembly line,
16:51
you measure some features of these engines. You measure some features about the vibration, and you measure some features about the heat that the aircraft engine is producing.
17:01
And, um, let's say that you get a dataset, right, that looks like this, okay?
17:10
And, um, the anomaly detection problem
17:19
is if you get a new aircraft engine that comes off the assembly line,
17:25
and if the vibration feature takes on this value, and the heat feature takes on this value, is that aircraft engine an anomalous one,
17:32
is it an unusual one, right? And so the application of this is, um, that as your aircraft engine comes off the assembly line,
17:39
if you see a very unusual signature in terms of the vibrations and the heat the aircraft engine is generating,
17:45
then probably something is wrong with this aircraft engine, and you have your people, have your, have your team inspect it further or test it further,
17:52
uh, before you ship the airplane, before you ship the engine to a, to a airplane maker and then something goes wrong in the air, and there's a,
17:57
there's a major accident, or major disaster, right? And so anomaly detection, uh, uh,
Anomaly Detection
18:03
is most commonly done, or one of the common ways to, um,
18:09
implement anomaly detection is the model p of x which is given all of these blue examples,
18:15
given all of these dots, can you model what is the density from which x was drawn?
18:21
So then if p of x is very small,
18:26
then you flag an anomaly, right? Meaning that, Gee, I think something's funny here, uh,
18:32
and maybe someone should inspect this aircraft engine a little bit further.
18:38
Um, so anomaly detection is used for, a task like this, for an inspection task like this.
18:43
Um, it's used for, um, uh, many years ago, I was actually working with some telecoms providers, you know, uh, uh,
18:50
helping out telecoms company on, um, anomaly detection to figure out if something's
18:55
gone wrong with a cell tower network, right? So if one day one of the cell towers start throwing off
19:00
network patterns that seem very unusual, then maybe something's wrong with that cell tower, like something's gone wrong.
19:06
We sent out the technicians to fix it. Uh, it is also used for computer security. If a computer, say if a computer at Stanford starts sending out very strange,
19:15
you know, um, uh, uh, network traffic, that's very unusual relative to everything it's done before, relative what this is,
19:20
is a very anomalous network traffic, then maybe IT staff should have a look to see if that particular computer has been hacked.
19:27
So these are some of the applications of anomaly detection. And the good way to do this is, given an unlabeled data set, model p of x.
19:34
And then if you have very low probability samples, you flag that as a possible anomaly for further study.
19:41
Now, given this dataset, um, uh, how do you model this?
19:46
One interesting thing about this green dot is that neither the vibration nor the heat signature is actually out of range, right?
19:53
You know, like there are a lot of aircraft engines with vibrations in that range. There are a lot of aircraft engines with heat in that range.
19:59
So neither feature by itself is actually that unusual. It's actually the combination of the two that is unusual.
20:05
Um, and so thus, thus, what I want to do is, uh, come up with an algorithm to model this.
20:10
And in fact, we'll come up with an algorithm that can model, you know, maybe, maybe your data density looks like this,
20:16
maybe more of an L shape like that. But how do you model p of x with the data coming from an L shape?
20:23
Um, and it turns out that there is no textbook distribution, right? You know, there isn't, you know, if you look at a simple exponential family of model,
20:31
the types of distributions, there is no distribution for modeling very, very complex distributions like this.
20:38
So what we're going to talk about is, um, the mixture of Gaussians model which we look at data like this,
Mixture of Gaussians Volatile
20:44
and say, it looks like this data actually comes from two Gaussian. There's one Gaussian, maybe there's one type of aircraft engine that, that, that, you know,
20:51
is drawn from a Gaussian like the one below, and a separate aircraft- type of aircraft engine that's drawn from a Gaussian like that above.
20:59
And this is why there's a lot of probability [NOISE] mass in the L-shaped region, but very low probability outside that L-shaped region, right?
21:07
And, and, and these ellipses I'm drawing are the contours of these two Gaussians, right? And so, um, what I'd like to do next is,
21:16
uh, develop the mixture of Gaussians model, um, which is useful for anomaly detection,
21:22
and, and, uh, uh, and, and then this will lead us to our second unsupervised programming algorithm, okay?
21:30
So, um, in order to make the mixture of Gaussians model a bit easier to develop,
21:39
let me just use a one-dimensional example where x is in R, okay?
21:45
So, um, let's see. So let's say that, uh, we gather a data set that looks like this.
21:52
[NOISE]
22:02
Right. So it's just one row number. So it's just on num- number line I plotted a few dots.
22:07
Um, so it looks like this data maybe comes from two Gaussians. Right? It looks like, you know, there's some data from this Gaussian.
22:14
And there's some data from that Gaussian on the right. Um, and is- and if only we knew.
22:24
Right? Which example had come from which Gaussian, if, if we knew that these examples had come from Gaussian 1,
22:32
which I want to denote with crosses. And if only we knew- no, that was here. What- but actually this is fine. I'll leave that one there.
22:39
If only we knew that these examples had come from Gaussian 2 which I'm going to draw with Os,
22:46
then we just fit Gaussian 1 to the crosses, fit Gaussian 2 to the Os and then we'd be pretty much done. Right? Um, oh, and, and, and sorry.
22:53
And so these are the two Gaussians. And so the overall density would be something like this. Right? Tha- that's the probability.
23:00
A lot of probability mass on left. A lot of probability mass on the right, low, less probability mass on the, uh, in sort of in, in the middle.
23:07
Okay? So the overall density I'll just draw again, would be, low high, low high something like that.
23:13
Right? Um, but the reason- and,
23:18
and, and if you actually had these labels. If you knew that these examples came from Gaussian 1, those examples come from Gaussian 2,
23:25
then you can actually use an algorithm very similar to GDA, Gaussian discriminant analysis to fit this model.
23:31
Uh, that the problem with this density estimation problem is, you just see this data and maybe the data came from two different Gaussians.
23:39
But you don't know which example actually came from which Gaussian. Okay? So the EM algorithm or the expectation-maximization algorithm will allow us to, uh,
23:48
fit a model despite not knowing which Gaussian each example that come from.
24:07
So let me first write down the, um, mixture of Gaussians model.
24:16
Uh, and then we'll describe the EM algorithm for this.
24:23
So let's imagine- let's suppose that as a,
24:32
um, so the term we sometimes use is latent, but latent just means hidden or unobserved.
24:43
Um, random variables z. Right?
24:54
And x_i, z_i,
25:17
um.
25:37
Okay? So- this part here. So let's imagine that, um,
25:44
there's some hidden random variable z and, and the term latent just means hidden or unobserved. Right? It means that it exists but you don't get to see the value directly.
25:52
So when I say latent, it just means hidden or unobserved. So let's imagine that there's a hidden or latent random variable z and,
25:59
uh, x_i and z_i have this joint distribution. And this, this, this is very, very similar to the model you saw in Gaussian discriminant analysis.
26:07
But z_i is multinomial with some set of parameters Phi. For a mixture of two Gaussians,
26:14
this would just be Bernoulli with two values. But if it were a mixture of k Gaussians then z, you know,
26:19
can take on values from 1 through k. [NOISE] Right? Um, and it was two Gaussians it'll just be Bernoulli.
26:26
And then once you know that one example comes from, uh. Gaussian number j, then x condition that z_i is equal to j.
26:36
That is drawn from a Gaussian distribution with some mean and some covariance Sigma.
26:42
Okay? So the two unimportant ways. This is different than GDA.
26:47
Um, one, well, I've set z to be 1 of k values instead of one of two values.
26:52
And GDA, Gaussian discriminant analysis. We had z, you know, uh, why the labels y took on one of two values.
26:59
Uh, and then second is, I have Sigma j instead of Sigma. So by, by convention when we fit mixture of Gaussians models,
27:06
we let each Gaussian have his own covariance matrix Sigma. But you can actually force it to be the same way you want. So- but these are the trivial differences.
27:13
Uh, the most significant difference is that, in Gaussian discriminant analysis, we had labeled examples x_i, y_i.
27:22
Where z- y was observed. Right? And then the main difference between this and Gaussian discriminant analysis is,
27:29
now we have replaced that with this latent or hidden random variable z_i that you do not get to see in the training set.
27:35
Okay? So now, uh,
27:50
actually you guys are right. These pens are terrible. All right. Oh, that was better.
27:56
Cool. All right.
28:05
So if we need the z_i's.
28:12
Right? Then we can use, um, maximum likelihood estimation.
28:19
All right? So if only we knew the value of the z_i's, which we don't. But if only we did, then we could use
28:24
maximum likelihood estimation or MLE to estimate everything. You know. So we would write the log likelihood of the parameters.
28:32
Right? Equals sum, um, log p of x_i,
28:38
z_i, you know, given the parameters. Right? And then you take the derivative,
28:46
set the derivatives equal to 0 and then you guys did this in problem set 1. Right? And, and then you would find that Phi j is equal to
28:53
1 over m. Right?
29:22
Okay. So if only you knew the values of the z_i's, uh, then you could use maximum likelihood estimates,
Maximum Likelihood Estimates
29:29
um, will- and, and this is what you get. And this is pretty much the formulas. Actually the- the- these two are exactly the formulas,
29:36
uh, we had for, uh, Gaussian discriminant analysis. Except with replace y with z.
29:42
Right? And then there's some other formula for Sigma that's written in the lecture notes. But I won't, but I won't write down here.
29:48
Okay? Um, but the reason we can't use this,
29:53
use these formulas is we don't actually know what are the values of z. So what we will do in
30:00
the EM algorithm
30:08
is two steps. Um, in the first step,
30:15
we will, uh, guess the value of the z's.
30:20
And in the second step we will use these equations using the values of this z's we just guessed.
30:26
So let me- so, so sometimes in, um, the machine learning is something that's called- there's a bootstrap procedure
30:31
where you get something that runs an algorithm. You're using your guesses and then you update your guesses and then run the algorithm again.
30:38
Let me, let me make that concrete by writing this down.
30:54
So the EM algorithm has two steps. The E-step, um,
31:09
also called the expectation step is set to w i j.
31:26
So w i j, um, is going to be the probability that z_i is equal to j.
31:35
Okay? Um, given all the parameters. And, and much as we did with, um,
31:41
generative learning algorithms, right, with generative learning algorithms, we used Bayes' rule to estimate the probability of y given x,
Bayes Rule
31:50
and so to compute this, you use a similar Bayes' rule type of calculation.
31:56
And so this would be [NOISE].
32:06
Oops, right, um, where,
32:22
for example this term here P of x_i given z_i equals j.
32:29
This would be a Gaussian density, right? This comes from a Gaussian density with mean Mu j and covariance Sigma j, right?
32:38
And so this term here would be 1 over 2 Pi, to the N over 2 Sigma j,
32:46
so one-half e to the negative one-half.
32:59
All right. And then this term here, I guess this would be Phi j,
33:05
that's just a Bernoulli probability, remember z is multinomial. Right, so z is multinomial with parameters Phi.
33:15
So I guess the parameters Phi for multinomial distributions tell you, what's the chance of z being 1, 2, 3, 4,
33:22
and so on up to k, and so the chance of z_i being equal to k is just- chance of z_i being equal to j is just Phi j right?
33:30
It's just v to the off one of the parameters in your multinomial probability for,
33:35
um, for the odds of z being different values. okay? And so, um, and similarly the terms in the denominator.
33:42
This term here is from Gaussian and that second term is from the, um, multinomial probability that you have for z.
33:50
And so that's how you plug in all of these numbers and use Bayes rule and use this equation to compute given- all given the position of all these Gaussians,
33:59
what is the chance of w i j taking on a certain value, okay.
34:06
And, and so to make this really concrete, you remember how I guess 1 or 0s, or the other way, um,
34:15
If you were to look at these, uh, if you were to scan from right to left, remember how, you know,
34:20
you get a sigmoid function, or the sigmoid can be this way or this way or it depends on the sign. I guess if these are positive samples these are negatives.
34:27
You have a sigmoid function like this. And so w i j is just the height of this Sigma,
34:34
it's just a chance of, you know, each of these examples being,
34:39
coming from either the z equals 1 or z equals 0 and then you store all of these numbers in the variables w i j.
34:48
Okay. So w i j is just to compute the posterior choice of this, this example coming from the left Gaussian versus the right Gaussian.
34:55
You just saw that in the variable w i j. So that's the E-step,
35:10
um, and you compute the w i j for every single training example i.
35:16
Right? I think it's the M-step is, um, yeah.
35:23
[BACKGROUND]
35:29
Sorry, is this what? [BACKGROUND]
35:36
Oh this one. Yes. Sorry, yes. Thank you, there we go, thank you. Yes, there's a following Gaussian.
35:43
Okay. So in the- so the E-step tells us, you know,
35:49
we're trying to guess the values of the z's right, when we figure out what's the probability of z being 1, 2, 3, 4 up to k was stored here.
35:55
And then in the M-step, what we're going to do is use the formulas we have for maximum likelihood estimation,
36:04
and I want you to compare these with the equations I had above, right.
36:26
Okay. Well, I hope you see. So these equations are a lot like the equations above,
36:33
except that instead of indicator z_i equals j, we replaced it with w i j, right?
36:42
Which by the way is the expected value of this indicator function.
36:50
Right, because the expected value of an indicator function is just equal to the probability of that thing in the middle being true.
36:59
Okay? Um, and then,
37:04
and then there's a formula for Sigma j as well that you can get from the lecture notes, but i won't, I won't write down here.
37:10
Okay. So, um, one intuition of,
37:17
um, this mixture of Gaussians algorithm is that it's a little bit like k-means but with soft assignment.
37:24
So in k-means, in the first step we would take each point and just assign it to one of the k,
37:30
k cluster centroids, right? And if it was a little bit closer to the red cluster centroid than the blue cluster centroid,
37:37
we would just assign it to the red cluster centroid. So even if it was just a little bit closer to one cluster centroid than another,
37:42
k means we just make what's called a hard assignment meaning, you know, whatever cluster centroid it's closest to,
37:47
we just assigned it 100 percent of that ce- cluster centroid. So yeah, EM is,
37:54
uh, you can think, uh, EM implements a softer way of assigning points to
38:01
the different cluster centroids because instead of just picking the one closest Gaussian center and assigning it there,
38:08
it uses these probabilities and gives it a waiting, in terms of how much is assigned to Gaussian 1 versus Gaussian 2.
38:15
Um, and the second updates, you know, the means accordingly, right? Sum over all the x_i's to the extent they're assigned to
38:21
that cluster centroid divided by the number of examples assigned to that cluster centroid. Okay? So, so, so that's one intuition be- between EM and k-means,
38:33
um, and in a second, uh, uh, but, but when you run this algorithm,
38:39
it turns out that this algorithm will converge with some caveats I'll get to later,
38:45
and this will find a pretty decent estimate, um, of the parameters, you know,
38:52
of say fitting a mixture of two Gaussians model. Okay? So this is, um,
39:01
the- a- and so if you are given a dataset of say airplane engines, you can run this algorithm for the mixture of two Gaussians.
39:08
And then when a new airplane engine rolls off the assembly line, um, um, so,
39:13
so after you're fitting the k-means algorithm, you now have a- after fitting the EM algorithm,
39:19
you now have a joint density of a P of x comma z. And so the density for x is just sum of all the values of z of P of x comma z.
39:30
And so, and so
39:40
a mixture of Gaussians can fit distributions that look like this, it can fit distributions that look like this, right?
39:46
These are, these are both mixtures of two Gaussians. So this gives you a very rich family of models to fit very complicated distributions.
39:54
And now that, um, right, and you can also fit, I don't know, something like this.
40:00
So this is a mixture of two Gaussians, I guess one thin narrow Gaussian here and one much wider fatter Gaussian.
40:06
So a mixture of two Gaussians can actually fit a model of different things, um, uh, can fit a lot- and a mixture of more than two Gaussians can fit even richer models.
40:15
And so by doing this, you can now model P of x for many complicated densities,
40:21
or including this one, right, this example I had just now. This will allow you to fit
40:27
a probability density function that puts a lot of probability models on, on a region that looks like this. And so when you have a new example you can evaluate P of x,
40:35
and if P of x is large, then you can say nope this looks okay and the P of x is less than Epsilon.
40:43
You can flag an anomaly and say take a look- take another look at this here.
40:48
Okay? So, um, I kind just wrote down this algorithm,
40:55
with a little bit of a hand-wavy explanation for how it's derived, right? So like I said, if only you knew the values of C and just use maximum likelihood estimation,
41:03
so let's guess the values of z and plug that into the formulas of maximum likely estimation. It turns out that hand-wavy explanation works,
41:11
in the particular case of, um, the EM mixtures of Gaussians but that there is a more formal way of
41:17
deriving the EM algorithm that shows that this is a maximum likelihood estimation algorithm,
41:24
and that they converge at at least a local optimum. Um, and in particular, there- what we'll do is show that if your goal is, um,
41:33
uh given a model P of x, z parameterized by Theta,
41:39
if your goal is to maximize P of x, right? Oh, excuse me.
41:51
So this is what maximum likelihood is supposed to do. That EM is exactly trying to do that, okay.
41:57
So, um, I'll go on in a minute to present this more general derivation,
42:03
the - the form of general derivation of the EM algorithm tha- that doesn't rely on this hand-wavy argument of
42:08
I guess it's easier use maximum likelihood with the guess values. So I'll do the the rigorous derivation of EM in a minute.
42:16
But before I do that, let me just pause and check if there any questions. Yeah.
42:29
[inaudible]. Um, yeah,
42:38
uh, maybe- let's see. Maybe I'll help to not think of them as weights. Um, yeah, I think thi- this is actually the weighting you assigned to a certain Gaussian,
42:49
so there's one intuition, uh, hen - hence the weights, but, um, um, let me think,
42:57
what's going to explain this? So one way to think of this as wij is
43:03
how much xi is assigned to,
43:13
you know, to- to- to the um, µj Gaussian.
43:23
So, um, wij is the strength of how
43:29
strongly you want to assign that training example xi to that cluster or to that- to that particular Gaussian.
43:36
Um, and so this is the number of 2, 0, and 1 right? And, uh, the strength of all the assignments,
43:41
and every point is a sign with a total strength equal to 1, because all these properties must sum up to 1.
43:47
And so, when I take this point and assign it, you know, 0.8 to a more close Gaussian and 0.2 to a more distant Gaussian.
43:55
And this is our guess for, you know, well there's an 80% chance that it came with that Gaussian and a 20% chance it came with the second Gaussian. That makes sense?
44:03
[inaudible].
44:10
Oh I see. So let's see. Um, so when you're running the EM algorithm, you never know what are the true values of z, right?
44:17
You're- you're given a data set, so you're only told the x's, and as far as we know, uh,
44:23
these airplane engines were generated off, you know, two different Gaussians. Maybe there are two separate assembly processes.
44:29
You know, one from the, uh, uh, one from plant number one, one from plant number two,
44:34
and maybe they actually operate a little bit differently, but by the time they merge onto one, um,
44:39
uh, you know, by- by the time the two suppliers of aircraft engines get to you, they've been mixed together,
44:45
and so you can't tell anymore which aircraft engine came from proce- plant one and which pla- aircraft engine came from plant two.
44:52
Um, and they you know there are two plants, where you just see the stream of aircraft engines, you're hypothesizing that there are two types.
44:58
And so in every iteration of EM, you're taking each, uh, aircraft engine and guessing, you know, for this one,
45:06
I think there's 80% chance that it came from process one, and a 20% chance it came from process two, so that's the E-step.
45:11
And then in the M-step, you look at all the engines that you're kind of guessing were generated by process one,
45:18
and you update your Gaussian to be a better model for all of the things that were- that you kind of think were generated by process one.
45:25
And if there's something that you're absolutely sure came from process one, then it has a weight of one close to one in this.
45:31
Do you think that was something that, you know, there's a 10% chance it comes from process one, then that example is given a lower weight and now you
45:38
update the mean for that Gaussian. That make sense? Cool. All right.
45:50
So, [NOISE] 33 minutes.
46:24
Yeah. Okay cool. All right.
46:31
Well I still remember when, um, I was an undergrad doing a summer internship at AT&T Bell Labs.
46:37
Um, and then someone a few offices down had learned about EM for the mixture of Gaussians for the first time, and he was running it on his computer,
46:44
and he's going around to every single office. Saying, "Oh my God you've got to check this out, this is unbelievable look at what this algorithm can do for three mixes of Gaussian. "
46:51
So tha- that shows you, those are the type of people I hang out with [LAUGHTER].
47:05
All right. Um, so in order to derive yo - you know, so -so this is a slightly hand-wavy argument.
47:10
As I uh, let's get- let's guess the values of the z's. Let's just have these weights and plug them into maximum likelihood.
47:15
Um, what I would like to do is give a more rigorous de- derivation for why EM Algorithm is a reasonable algorithm,
47:22
and why it's a maximum likelihood estimation algorithm and why we can expect it to converge.
47:28
And it turns out that rather than just proving, you know, that this is a sound algorithm, what we'll see on Wednesday is that this view of EM, uh,
47:36
allows us to derive EM in a- in a more correct way for other models as well, the mixtures of Gaussian.
47:42
On, on Wednesday, we'll talk about, uh, uh a model called factor analysis, it lets you model Gaussians in extremely high dimensional spaces,
47:50
where if you have 1,000 dimensional data, but only 30 examples, how do you fit a Gaussian into that? So we'll talk about that on Wednesday.
47:56
And it turns out this derivation of EM we're going to go about- through now is crucial for,
48:01
um, applying EM accurately in- in- in problems like that. Okay, so.
48:07
Uh, in order that live up to that derivation, let me describe, um, Jensen's inequality.
Jensen's Inequality
48:15
So let f be a, a convex function.
48:24
Um, to do EM, we're actually going to need a concave function, so it'll be all minus of everything,
48:30
but we'll get to that in a second. But so, a convex function means the second derivative is greater than 0,
48:42
or in other words, it looks like that, right? So that's a convex function. Uh, let x be a random variable.
48:57
Then f of the expected value of x is less than or equal to the expected value of x.
49:08
Okay. Now, [NOISE] um, [NOISE]
49:25
maybe, um, here's an example. All right.
49:31
So here's the, um, let's see,
49:37
there's the function f of x, and let's say that these are the values 1, 2, 3, 4, 5.
49:45
And suppose that X is equal to 1 with
49:50
probability one-half and is equal to 5 with probability one-half, right?
49:58
Just for the illustration. Then here is f of 1.
50:09
Here is f of 5. Um, here is f of 3.
50:16
And f of 3 is f of the expected value of X, right, because so the expected value of X.
50:24
And sometimes I write this without the square brackets, right. It's the average of X is equal to 3.
50:30
Um, and so the expected value, excuse me, f of the expected value of X is equal to this value,
50:38
whereas the expected value of f of x is the mean of f of 1 and f of 5.
50:52
All right. So the expected value of f of x. F of x is a 50% chance of being f of 1,
50:57
and a 50% chance of being f of 5. And so the expected value of f of x is equal to this value in the middle.
51:04
It's really take these two, right. Take this value and this value and take their mean. So is this value up here, and,
51:11
and this value is the expected value of f of x.
51:17
Okay. And so in this example the expected value of f of x is greater than f of the expected value of X,
51:25
right, as, as predicted by Jensen's inequality. Um, I'm gonna just draw one illustration that may or may not help,
51:31
and some of my friends like it, I sometimes use it but if it's confusing then don't worry about it. But it turns out that if you draw a line that connects these two,
51:39
then the midpoint of this line, um, is the height of f of expected value of x,
51:45
right, so the height of this. You know, so, so given these two points, this point and this point, if you draw this line,
51:51
it's called a chord, um, then the height of this point is the expected value of f of x.
51:59
And this point is,
52:08
um, f of the expected value of x. Right. And in any convex function,
52:18
you know, really take any convex function. That's also convex function. If you draw any chord,
52:24
that green point is always higher, right, than that green point which is why- which is
52:34
another way of seeing why Jensen's inequality holds true. Okay. If this visualization doesn't help don't worry
52:40
about it but it's just a- actually what a lot of my friends do is we keep on forgetting which direction Jensen's inequality goes.
52:49
[LAUGHTER] Why are we not using Jensen [LAUGHTER] that's not great. So a lot of my friends don't remember, we draw this picture and draw that chord,
52:55
and we quickly figure out which way the inequality goes. Um, all right.
53:02
So one addendum further.
53:19
If f is strictly greater than 0. And so if this is the case,
53:25
we say f is strictly convex.
53:36
Then-
53:48
Okay. So, um, let's see, a straight line is also a convex function, right.
53:54
So this is a convex function, this is a convex function and this is a convex function. It turns out a straight line, that's also a convex function.
54:00
But so in this addendum is saying that if f is a strictly convex function meaning basically it's now a straight line, right.
54:07
And a bit modern, it's not a straight line. But if the curvature if it's always bending up, uh,
54:14
then the only way for the left and right-hand sides to be equal is if x is a constant,
54:20
meaning it's a random variable that always takes on the same value. Okay. So Jensen's inequality says that,
54:27
you know, um, left-hand side is going to be the same as right hand side. Sorry, I think I reversed the order of these two for that equation that doesn't have it.
54:35
But so Jensen equality says left-hand side is always less than or equal to the right-hand side, and the only way it's equal as if X, you know,
54:43
is a random variable that always takes on the same value. Okay, yeah. What if- What if the value of f of 1 was equal to the value of f of 3. Wouldn't that?
54:56
Yeah. So it turns out what if value f of 1 is equal to the value of f of 3. It turns out, it does.
55:02
Vary. So let's see. So one way that [NOISE] could happen would be if the function were like that.
55:08
And then if you take the- draw the chord, take the mean it's still higher. [inaudible] The point of f of 1. [inaudible]
55:20
Then it's important. If, if you kind of flat that part here, then the function is not strictly convex.
55:26
And so it's still less than equal to but it's not, but they can't be equal to if x is random.
55:31
Okay. So and we'll use this in a little bit.
55:38
We'll actually end up using this. Um, and again for the strict probabilistic, you know,
55:43
if those of you that, I don't know, take classes in advanced probability, the technical way of saying x is a constant is x is equal to EX with probability 1.
55:59
You know, I, I think that for all practical human purposes you do not need to worry about this. But I think if you [LAUGHTER] take a class in measure theory.
56:07
The Professor in measure theory will be happy if you say this and you say x is a constant but maybe, maybe none of you.
56:13
Okay. Just don't worry about it. Um, oh yes.
56:18
Okay. Now, um, just one more addendum, um, to this is that
56:25
the form of Jensen's inequality we are going to use is actually a form for a concave function.
56:31
So instead of convex, um, I'm gonna say concave. And so, you know,
56:36
a concave function is just a negative of a convex function, right. If you take a convex function and take the negative of that, it becomes concave.
56:44
And so the whole thing works with the- with everything flipped around the other way.
56:52
Okay. And yep, so this is strictly concave.
57:01
Okay. So the form of Jensen's inequality we are gonna use is actually the, um,
57:07
concave form of Jensen's inequality, and we're actually going to apply it to the log function.
57:13
So the log function, right. Log x looks like this. And so that's a concave function. And so the inequality we'll use would be
57:19
in this direction that I have in orange. All right.
57:30
So here's the density estimation problem.
Density Estimation Problem
58:00
Meaning, density estimation means you want to estimate P of x. All right. So we have a model
58:08
for P of x, z, with parameters theta.
58:14
And so, you know, instead of writing out mu, sigma- mu, sigma, and phi,
58:21
like we did for the mixture of Gaussians. I'm just gonna capture all the parameters you have. Whatever your parameters are, I'm just gonna capture them in one variable theta.
58:29
And you only observe x.
58:36
So your training set looks like that.
58:42
So the, um, log likelihood of the parameters theta is equal
58:50
to some of your training examples log P of x_i, parameterized by theta.
58:59
Um, and this in turn is log of sum over z,
59:08
P of x_i, z_i parameterized by theta, right.
59:17
Because P of x, you know, is just taking the joint distribution and summing out, marginalizing out z_i.
59:26
Okay. [NOISE] And so what we want is maximum likelihood estimation
Maximum Likelihood Estimation
59:35
which is define the value of theta that maximizes this log-likelihood.
59:44
And what we would like to do is derive an EM, derive an algorithm which will turn out to be an EM algorithm as
59:52
an iterative algorithm for finding the maximum likelihood estimates of the parameters theta.
59:58
[NOISE]
1:00:05
So, um, let me draw a picture of that,
1:00:11
I'd like you to keep in mind as we go through the math, which is, you know, the, the horizontal axis
1:00:18
is the space of possible values of the parameters Theta. And so there's some function O of Theta that you try to maximize.
1:00:33
This right. And so what EM does is, um,
1:00:38
let's say you initialize Theta as some value, you know, maybe randomly initialize, um,
1:00:45
sim- similar to the k-means cluster centroids. Where just randomly initialize your mu's with a mixture of Gaussian's.
1:00:52
What the EM algorithm does is in the E-step, we're going to construct a lower bound shown in green here for the log-likelihood.
1:01:03
And this lower bound, this green curve has two properties. One is that it is a lower bound.
1:01:08
So everywhere you look, you know, over all values of Theta, the green curve lies below the blue curve.
1:01:14
So this is a lower bound. And the second property that the green curve has is that it is
1:01:20
equal to the blue curve at the current value of Theta. Okay. So what the E-step does,
1:01:28
uh, which you'll see later on, and just keep this picture in mind as we go through the E-step and the M-step is, um, construct the lower bound that looks like this, right.
1:01:37
Oh, and, and also, uh, to, uh, to foreshadow probably the derivation. Right? There- there was an addendum to Jensen's inequality where we said,
1:01:45
well, under these conditions it holds with equality. Right. E of f of x equals f of e of x. We said, "Well, the two things are equal with under certain conditions."
1:01:53
Um, we want things to be equal. We want the green curve to be equal to the blue curve at the old value of Theta.
1:01:59
So we- we'll use that addendum to Jensen's inequality when we drive that. Um, so this E-step is draw the green curve.
1:02:08
And then what the M-step does is it takes a green curve, and then it finds the maximum.
1:02:16
Actually, certainly stroke [inaudible] so I'll draw in green. What the M-step does is it takes the green curve,
1:02:22
and it finds the maximum. And one step of EM will then move Theta from this green value to this red value.
1:02:33
Okay. So the E-step constructs the green curve, and the M-step, uh,
1:02:39
finds the maximum of the green curve. And this is one iteration of EM.
1:02:44
The second iteration of EM, now that you're at this red thing is will construct a new lower bound,
1:02:50
and then again, you use a different lower bound. Everywhere the red curve is below the blue curve, and the values are equal at this new value.
1:02:58
That's the E-step, and then M-step will maximize this red curve,
1:03:05
um, and so on. Now you're here. Construct another thing, do that.
1:03:13
Right. And you can kinda tell that as you keep running EM, this is constantly trying to increase L of Theta.
1:03:21
Trying to increase the log-likelihood, until it converges to a local optimum. Okay. Um, the EM algorithm does converge only to local optimum.
1:03:28
So if, you know, there was another even bigger thing there that it may never find its way over to that other- that, uh, better optimum.
1:03:36
But the EM algorithm by repeatedly doing this, will hopefully converge to a pretty good local optimum.
1:03:42
Okay. All right. So let's write on how we do that.
1:03:50
Um, let me think.
1:03:57
Actually, let me use the other board. No, I think this is okay. All right.
1:04:10
So I've already said that our goal is to find the parameters theta that maximize this.
1:04:16
[NOISE] All right. Uh, and so that equation we said are just now is sum over i log,
1:04:26
sum over zi, p of xi comma zi given Theta.
1:04:37
Okay. So this is just what we had written down, I guess, uh, on the left.
1:04:42
What I'm going to do next is, um, divide by- [NOISE]
1:04:55
multiply and divide by this.
1:05:07
Okay. Um, where Qi of zi is a probability distribution,
1:05:26
i.e., the sum over zi, Qi of zi equals 1.
1:05:36
Okay. So I'm going to multiply and divide by some probability distribution, and we'll, we'll decide later
1:05:42
how to come up with this probability distribution Qi, right. But, you know, I'm allowed to construct a probability distribution and multiply and divide by the same thing.
1:05:49
Right. Now, if you look at this,
1:05:54
all right, let's put square brackets here. If this Qi, that is the probability distribution meaning that sum over zi Qi,
1:06:01
zi sums over- sums to 1. Then this thing inside is, um,
1:06:07
equal to sum over i log of an expected value of zi
1:06:15
drawn from the Qi distribution of [NOISE] right, actually, if I,
1:06:23
let me use colors to make this clearer.
1:06:42
Right. So the way you compute the expected value of z-, you know,
1:06:47
some function of zi is you sum over all the possible values of zi of the probability of zi times whatever that function is.
1:06:55
So this equation is just the expected value with respect to zi drawn from that Qi distribution of that thing in the square brackets,
1:07:03
in the purple square brackets. Now, using the, um,
Concave form of Jensen's Inequality
1:07:17
concave form of Jensen's inequality, we have that this is greater than
1:07:25
or equal to [NOISE].
1:07:54
So this is a form of Jensen's inequality where, um, f of E, x is greater than or equal to E of f of x,
1:08:07
where here, um, this is the logarithmic function.
1:08:13
Right. So the log function is a concave function. It looks like that. And so, um, using the,
1:08:20
I guess here using, using the form Jensen's inequality with the signs reversed, um.
1:08:25
Right, f of Ex is greater than equals E of fx. So you get log of expectation is greater than equal to expectation of the log, all right.
1:08:36
And then finally, let me just take this expectation and unpack it one more time.
1:08:42
So this is now sum over i, sum over zi. [NOISE].
1:09:00
Okay. So I just took this expected value and turned it back into the sum of the random variable probability, times that thing.
1:09:10
Okay. So, um, if you remember this picture from the middle,
1:09:18
what we wanted to do was to construct a function, construct this green curve.
1:09:23
There's a lower bound for the blue curve. And if you view this formula here as a function of Theta right,
1:09:33
so your x, um, x is just your data, and z is a variable you sum over.
1:09:38
So this whole thing is the function of Theta, right? Because x's are fixed, z is just something you f- sum over.
1:09:43
So this whole formula here, this is a function of the parameters Theta. And what we've shown is that this thing, you know,
1:09:52
this formula here, this is a lower bound for the log-likelihood,
1:09:58
uh, for- for, for, for this thing. I guess this is L of Theta.
1:10:03
So- go ahead. [inaudible].
1:10:12
Oh, how I got to this equation? Uh, sure. Um, let me think.
1:10:17
So let's see. What's a good way to do this?
1:10:27
Um, uh, yeah. Let's say that z takes on values from 1 through 5, right.
1:10:34
Let's say z takes on values from 1 through 10. So you roll a 10 sided dice. And I want to compute, um, you know,
1:10:41
the expected value of, uh, some function of, of some function g, g of z.
1:10:47
Right. Then the expected value of g of z is sum of all the possible values of z of the probability that you get that z,
1:10:55
times g of z. Right. So that's, that's what's the expected value is of a function of a random variable.
1:11:03
And, and this is- and the expected value of z is sum over z, P of z times z.
1:11:10
That's the average of random variable. And so, um, in the notation that we have,
1:11:17
the probability of z taking on different values is denoted by Qi of z,
1:11:23
which is why we wind up with that formula. Does that makes sense? Does it?
1:11:29
Okay. Is that okay? Does that make sense? Yeah. All right. If, if one of these steps doesn't make sense, let me know.
1:11:36
Th- other questions?
1:11:44
Okay. All right.
1:11:54
Hope that makes sense. [NOISE]. Um. [NOISE]
1:12:04
Now, one of the things we want when constructing
1:12:10
this green lower bound is we want that green lower bound to be equal to the blue function at this point, right?
1:12:16
And this is actually how you guarantee that when you optimize the green function. By improving on the green function,
1:12:22
you're improving on the blue function. So we want this lower bound to be tight. Right, the, the two functions be equal, tangent to each other.
1:12:29
So in other words we want this inequality to hold with equality. So we want, um, yeah,
1:12:36
so we want the left hand side and the right hand side to be equal for the current value of Theta, right?
1:12:44
[NOISE]
1:13:03
So on a given iteration of EM where
1:13:12
the current parameters are equal to Theta, we want,
1:13:51
we want- I know this was a lot of math but, you know, we want the left and right hand sides to be equal to each other.
1:13:58
Right. Because that's what it means for, uh, for the lower bound to be tight,
1:14:06
for the green color to be exactly touching the blue curve as we construct that lower bound.
1:14:12
And so for this to be true,
1:14:21
we need the random variable inside to be a constant. So we need P of x_i, z_i,
1:14:28
divided by Qi of z_i to be equal to const- to, to a constant.
1:14:37
Meaning that no matter what value of z_i you plug in, this should evaluate to the same value.
1:14:45
In other words, the ratio between the numerator and denominator must be the same. Um, unfortunately so far,
1:14:52
we have not yet specified, how we choose this distribution for z_i, right. So, so far the only constraint we have is that
1:14:59
Qi has to be a probability density- has to be a probability distribution over z_i, but you could choose one of the distributions you want for z_i.
1:15:06
And it turns out that, um, uh,
1:15:11
we can set Qi of z_i to be proportional to p of x_i,
1:15:22
z_i parameterized by Theta. And this means that for any value of z,
1:15:27
you know, so z_indicates as it could from Gaussian one and Gaussian two. Right. So this means that the chance of Gaussian one is
1:15:34
proportional to the chance of Gaussian one versus Gaussian two. Whether z_i takes on one or two is proportional to this.
1:15:42
And I don't want to prove it but one way to ensure this, and this is proven in the lecture notes.
1:15:48
But it turns out that one way to ensure. Um, well so the Qis need to sum to 1.
1:15:53
So one way to ensure that this is proportional to the right-hand side is to just take the right-hand side.
1:16:01
Sorry. Let me move here.
1:16:08
So one- so let's see.
1:16:15
Right. So the Qis have to sum to 1. And so one way to ensure the proportionality is to just take the right-hand side,
1:16:26
and normalize it to sum to 1.
1:16:37
Um, and after, after a couple of steps that are in the lecture notes but I don't want to do here,
1:16:44
you can show that this results in sending Qi of z_i to be equal to that,
1:16:49
that posterior probability, okay? And so, um, sorry I skipped a couple steps here.
1:16:58
You can get from the lecture notes, but it turns out that if you want this to be a constant meaning whether you plugged in z_i equals 1 or z_i equals 2 or whatever,
1:17:06
these evaluate to the same constant. The only way to do that is make sure the numerator and denominator are proportional to each other.
1:17:14
And because Qi of z_i is a density that must sum to 1. One way to make sure they're proportional is to just
1:17:21
set this to be with the right-hand side but normalize the sum to 1. Okay. And we derived this a little bit more carefully in the lecture notes.
1:17:28
So just to summarize,
1:17:38
this gives us the EM algorithm. Let's take all of this- everything we just did and wrap in the EM algorithm.
1:17:45
In the E-step, we're going to set Qi of z_i equal to that.
1:18:02
And previously this was the w_i_js. Right. So instead of- so previously, we're restoring these probabilities in the variables you call w_i_js.
1:18:13
And then in the M-step,
1:18:24
we're going to take that lower bound that we constructed, which is this function,
1:18:38
and maximize it with respect to Theta. Okay. Um, and so remember in the M-step we
1:18:46
constructed this thing on the right-hand side as a lower bound for the log-likelihood. And so for the fixed value of Q,
1:18:53
you can maximize this with respect to Theta and that updates the Theta, you know, maximizing the green lower boundary,
1:18:59
that's what the M-step does. And if you iterate these two steps, then you find that this should converge to a local optima.
1:19:08
Okay. Oh and just maybe that's the obvious question. Um, why don't we try to maximize right Theta,
1:19:16
uh, why are we trying to maximize the log-likelihood directly? It turns out that if you take the mixture of Gaussians model,
1:19:23
try to take derivatives of this and set derivatives equal to 0, there's no known way to solve for the value of Theta that maximizes the log-likelihood.
1:19:30
But you find that for the mixture of Gaussians model and for many models including factor analysis that we talked about on Wednesday,
1:19:36
if you actually plug in the Gaussian density- uh, if you actually plug in that mixture of Gaussians model for P,
1:19:43
um, and take, you know, take, take derivatives, set derivatives equal to 0 and solve, you will be able to find an analytic solution to maximize this M step,
1:19:51
and that'll be exactly what we had worked out in the early derivation of the EM algorithm. Okay. But so this derivation shows that,
1:19:58
uh, the EM algorithm, you know, is a maximum likelihood estimation algorithm with
1:20:04
optimization solved by constructing lower bounds and optimizing lower bounds, okay?
1:20:10
All right. Um, that's it for today, and only it's stuff up to here,
1:20:15
right, and so this stuff will be up to the midterm but we'll talk about factor analysis
1:20:21
a lot on Wednesday, but it will not be on the midterm. Okay. So let's break for today, and I'll see you guys on Wednesday.