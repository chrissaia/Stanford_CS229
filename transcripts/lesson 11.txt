Search in video
0:03
Hello everyone. Uh, welcome to CS229. Um, today we're going to talk about,
0:10
um, deep learning and neural networks. Um, we're going to have two lectures on that,
0:16
one today and a little bit more of it on, ah, Monday. Um, don't hesitate to ask questions during the lecture.
0:24
Ah, so stop me if you don't understand something and we'll try to build intuition around neural networks together.
0:30
We will actually start with an algorithm that you guys have seen, uh, previously called logistic regression.
0:36
Everybody remembers logistic regression? Yes. Okay. Remember it's a classification algorithm.
0:41
Um, we're going to do that. Explain how logistic regression can be interpreted as
0:46
a neural network- specific case of a neural network and then, we will go to neural networks. Sounds good?
0:54
So the quick intro on deep learning.
1:02
So deep learning is a- is a set of techniques that is let's say a subset of
1:09
machine learning and it's one of the growing techniques that have been used in the industry specifically for problems in computer vision,
1:16
natural language processing and speech recognition. So you guys have a lot of different tools and,
1:21
and uh, plug-ins on your smartphones that uses this type of algorithm. Ah, the reason it came, uh,
1:28
to work very well is primarily the, the new computational methods. So one thing we're going to see to- today, um,
1:37
is that deep learning is really really computationally expensive and we- people had to
1:44
find techniques in order to parallelize the code and use GPUs specifically in order to graphical processing units,
1:51
in order to be able to compute, uh, the, the, the, the computations in deep learning.
1:56
Ah, the second part is the data available has been growing after,
2:05
after the Internet bubble, the digitalization of the world. So now people have access to large amounts of data and this type of algorithm has
2:13
the specificity of being able to learn a lot when there is a lot of data. So these models are very flexible and the more you give them data,
2:21
the more they will be able to understand the salient feature of the data. And finally algorithms.
2:28
So people have come up with, with new techniques, uh, in order to use the data,
2:35
use the computation power and build models. So we are going to touch a little bit on all of that,
2:40
but let's go with logistic regression first.
2:48
Can you guys see in the back? Yeah? Okay, perfect.
2:54
So you remember, uh, what logistic regression is? What- we are going to fix a goal for us,
3:01
uh, that, uh, is a classification goal. So let's try to,
3:07
to find cats in images. So find cats in images.
3:15
Meaning binary classification. If there is a cat in the image,
3:22
we want to output a number that is close to 1, presence of the cat,
3:28
and if there is no cat in the image, we wanna output 0.
3:37
Let- let's say for now, ah, we're constrained to the fact that there is maximum one cat per image, there's no more.
3:43
If you are to draw the logistic regression model, that's what you would do. You would take a cat.
3:49
So this is an image of a cat. I'm very bad at that. Um, sorry.
3:58
In computer science, you know that images can be represented as 3D matrices.
4:04
So if I tell you that this is a color image of size 64 by 64,
4:11
how many numbers do I have to represent those pixels? [BACKGROUND] Yeah, I heard it,
4:21
64 by 64 by 3. Three for the RGB channel, red, green, blue.
4:29
Every pixel in an image can be represented by three numbers. One representing the red filter, the green filter, and the, and the blue filter.
4:37
So actually this image is of size 64 times 64 times 3.
4:43
That makes sense? So the first thing we will do in order to use logistic regression to find if there is a cat in this image,
4:50
we're going to flatten th- this into a vector.
4:55
So I'm going to take all the numbers in this matrix and flatten them in a vector.
5:00
Just an image to vector operation, nothing more. And now I can use my logistic regression because I have a vector input.
5:09
So I'm going to, to take all of these and push them in an operation that
5:16
we call th- the logistic operation which has one part that is wx plus b,
5:24
where x is going to be the image. So wx plus b,
5:31
and the second part is going to be the sigmoid. Everybody is familiar with the sigmoid function?
5:38
Function that takes a number between minus infinity and plus infinity and maps it between 0 and 1. It is very convenient for classification problems.
5:45
And this we are going to call it y hat, which is sigmoid of what you've seen in class previously,
5:51
I think it's Theta transpose x. But here we will just separate the notation into w and b.
6:03
So can someone tell me what's the shape of w? The matrix W, vector matrix.
6:14
Um, what?
6:24
Yes, 64 by 64 by 3 as a- yeah.
6:29
So you know that this guy here is a vector of 64 by 64 by 3, a column vector.
6:37
So the shape of x is going to be 64 by 64 by 3 times 1.
6:46
This is the shape and this, I think it's- that if I don't know,
6:52
12,288 and this indeed because we want y-hat to be one-by-one,
7:00
this w has to be 1 by 12,288.
7:06
That makes sense? So we have a row vector as our parameter. We're just changing the notations of the logistic regression that you guys have seen.
7:16
And so once we have this model, we need to train it as you know. And the process of training is that first,
7:23
we will initialize our parameters.
7:29
These are what we call parameters. We will use the specific vocabulary of weights and bias.
7:38
I believe you guys have heard this vocabulary before, weights and biases.
7:44
So we're going to find the right w and the right b in order to be able,
7:50
ah, to use this model properly. Once we initialized them, what we will do is that we will optimize them,
7:58
find the optimal w and b,
8:05
and after we found the optimal w and b, we will use them to predict.
8:20
Does this process make sense? This training process? And I think the important part is to understand what this is.
8:28
Find the optimal w and b means defining your loss function which is the objective.
8:33
And in machine learning, we often have this, this, this specific problem where you have a function that you know you want to find,
8:41
the network function, but you don't know the values of its parameters. In order to find them, you're going to use
8:46
a proxy that is going to be your loss function. If you manage to minimize the loss function, you will find the right parameters.
8:53
So you define a loss function, that is the logistic loss.
9:01
Y log of y hat plus 1 minus y log of 1 minus y hat up.
9:11
You guys have seen this one. You remember where it comes from? Comes from a maximum likelihood estimation,
9:18
starting from a probabilistic model. And so the idea is how can I minimize this function.
9:26
Minimize, because I've put the minus sign here. I want to find w and b that
9:32
minimize this function and I'm going to use a gradient descent algorithm. Which means I'm going to
9:38
iteratively compute the derivative of the loss with respect to my parameters.
9:48
And at every step, I will update them to make this loss function go a little down at every iterative step.
9:55
So in terms of implementation, this is a for loop. You will loop over a certain number of iteration and at every point,
10:02
you will compute the derivative of the loss with respect to your parameters. Everybody remembers how to compute this number?
10:09
Take the derivative here, you use the fact that the sigmoid function has a derivative that is sigmoid times 1 minus sigmoid,
10:18
and you will compute the results. We- we're going to do some derivative later today. But just to set up the problem here.
10:26
So, the few things that I wanna- that I wanna touch on here is,
10:31
first, how many parameters does this model have? This logistic regression? If you have to, count them.
10:45
So this is the numb- 089 yeah, correct. So 12,288 weights and 1 bias. That makes sense?
10:53
So, actually, it's funny because you can quickly count it by just counting the number of edges on the- on the- on the drawing plus 1.
11:01
Every circle has a bias. Every edge has a weight because ultimately this
11:08
operation you can rewrite it like that, right? It means every weight has- every weight corresponds to an edge.
11:17
So that's another way to count it, we are going to use it a little further. So we're starting with not too many parameters actually.
11:23
And one thing that we notice is that the number of parameters of our model depends on the size of the input.
11:29
We probably don't want that at some point, so we are going to change it later. So two equations that I want you to remember is,
11:38
the first one is neuron equals linear plus activation.
11:44
So this is the vocabulary we will use in neural networks. We define a neuron as an operation that has two parts,
11:51
one linear part, and one activation part and it's exactly that. This is actually a neuron.
11:58
We have a linear part, wx plus b and then we take the output of this linear part and we put it in an activation,
12:07
that in this case, is the sigmoid function. It can be other functions, okay?
12:12
So this is the first equation, not too hard. The second equation that I wanna set now is the model
12:20
equals architecture plus parameters.
12:30
What does that mean? It means here we're, we're trying to train a logistic regression in order to,
12:37
to be able to use it. We need an architecture which is the following,
12:42
a one neuron neural network and the parameters w and b.
12:47
So basically, when people say we've shipped a model, like in the industry, what they're saying is that they found the right parameters,
12:55
with the right architecture. They have two files and these two files are predicting a bunch of things, okay?
13:02
One parameter file and one architecture file. The architecture will be modified a lot today.
13:08
We will add neurons all over and the parameters will always be called w and b,
13:13
but they will become bigger and bigger. Because we have more data, we want to be able to understand it.
13:19
You can get that it's going to be hard to understand what a cat is with only that, that, that many parameters.
13:25
We want to have more parameters. Any questions so far?
13:31
So this was just to set up the problem with logistic regression. Let's try to set a new goal,
13:38
after the first goal we have set prior to that. So the second goal would be, find cat,
13:49
a lion, iguana in images.
13:56
So a little different than before, only thing we changed is that we want to now to detect three types of animals.
14:04
Either if there's a cat in the image, I wanna know there is a cat. If there's an iguana in the image, I wanna know there is an iguana.
14:09
If there is a lion in the image, I wanna know it as well. So how would you modify the network
14:15
that we previously had in order to take this into account?
14:22
Yeah? Yeah, good idea.
14:27
So put two more circles, so neurons, and do the same thing. So we have our picture here with the cats.
14:36
So the cat is going to the right.
14:41
64 by 64 by 3 we flatten it, from x1 to xn.
14:47
Let's say n represents 64, 64 by 3 and what I will do, is that I will use three neurons that are all computing the same thing.
14:57
They're all connected to all these inputs, okay?
15:06
I connect all my inputs x1 to xn to each of these neurons, and I will use a specific set of notation here. Okay.
15:43
Y_2 hat equals a_2_1 sigmoid of
15:48
w_ 2_1 plus x plus b_2_1.
15:56
And similarly, y_3 hat equals a_3_1,
16:01
which is sigmoid of w_ 3_1 x plus b_3_1.
16:10
So I'm introducing a few notations here and we'll, we'll get used to it, don't worry.
16:16
So just, just write this down and we're going to go over it. So [NOISE] the square brackets here represent what we will call later on a layer.
16:26
If you look at this network, it looks like there is one layer here. There's one layer in which neurons don't communicate with each other.
16:35
We could add up to it, and we will do it later on, more neurons in other layers. We will denote with square brackets the index of the layer.
16:43
The index, that is, the subscript to this a is the number identifying the neuron inside the layer.
16:50
So here we have one layer. We have a_1, a_2, and a_3 with square brackets one to identify the layer. Does that make sense?
16:58
And then we have our y-hat that instead of being a single number as it was before,
17:04
is now a vector of size three. So how many parameters does this network have?
17:11
[NOISE]
17:25
How much? [BACKGROUND] Okay. How did you come up with that?
17:30
[BACKGROUND]. Okay. Yeah, correct. So we just have three times the, the thing we had before because we added
17:37
two more neurons and they all have their own set of parameters. Look like this edge is a separate edge as this one.
17:43
So we, we have to replicate parameters for each of these. So w_1_1 would be the equivalent of what we had for the cat,
17:49
but we have to add two more, ah, parameter vectors and biases.
17:54
[NOISE] So other question, when you had to train this logistic regression,
18:01
what dataset did you need? [NOISE]
18:14
Can someone try to describe the dataset. Yeah.
18:19
[BACKGROUND] Yeah, correct. So we need images and labels with it labeled as cat,
18:28
1 or no cat, 0. So it is a binary classification with images and labels. Now, what do you think should be the dataset to train this network? Yes.
18:42
[BACKGROUND]
18:49
That's a good idea. So just to repeat. Uh, a label for an image that has a cat would probably be
18:59
a vector with a 1 and two 0s where the 1 should represent the prese- the presence of a cat.
19:07
This one should represent the presence of a lion and this one should represent the presence of an iguana.
19:14
So let's assume I use this scheme to label my dataset. I train this network using the same techniques here.
19:22
Initialize all my weights and biases with a value, a starting value,
19:27
optimize a loss function by using gradient descent, and then use y-hat equals, uh, log to predict.
19:37
What do you think this neuron is going to be responsible for?
19:46
If you had to describe the responsibility of this neuron. [BACKGROUND]
19:53
Yes. Well, this one. Lion. Yeah, lion and this one iguana.
20:00
So basically the, the, the way you- yeah, go for it. [BACKGROUND]
20:07
That's a good question. We're going to talk about that now. Multiple image contain different animals or not.
20:12
So going back on what you said, because we decided to label our dataset like that,
20:17
after training, this neuron is naturally going to be there to detect cats. If we had changed the labeling scheme and I said
20:24
that the second entry would correspond to the cat, the presence of a cat, then after training,
20:29
you will detect that this neuron is responsible for detecting a cat. So the network is going to evolve depending on the way you label your dataset.
20:37
Now, do you think that this network can still be robust to different animals in the same picture?
20:44
So this cat now has, uh, a friend [NOISE] that is a lion.
20:49
Okay, I have no idea how to draw a lion, but let's say there is a lion here and because there is a lion,
20:58
I will add a 1 here. Do you think this network is robust to this type of labeling?
21:03
[BACKGROUND]
21:17
It should be. The neurons aren't talking to each other. That's a good answer actually. Another answer. [BACKGROUND]
21:31
That's a good, uh, intuition because the network, what it sees is just 1, 1, 0, and an image.
21:37
It doesn't see that this one corresponds to- the cat corresponds to the first one and the second- and the lion corresponds to the second one.
21:44
So [NOISE] this is a property of neural networks, it's the fact that you don't need to tell them everything.
21:49
If you have enough data, they're going to figure it out. So because you will have also cats with iguanas,
21:55
cats alone, lions with iguanas, lions alone, ultimately, this neuron will understand what it's looking for,
22:02
and it will understand that this one corresponds to this lion. Just needs a lot of data.
22:08
So yes, it's going to be robust. And that's the reason you mentioned. It's going to be robust to that because the three neurons aren't communicating together.
22:17
So we can totally train them independent- independently from each other. And in fact, the sigmoid here,
22:23
doesn't depend on the sigmoid here and doesn't depend on the sigmoid here. It means we can have one, one,
22:28
and one as an output. [NOISE] Yes, question. [BACKGROUND]
22:35
You could, you could, you could think about it as three logistic regressions. So we wouldn't call that a neural network yet.
22:41
It's not ready yet, but it's a three neural network or three logistic regression with each other.
22:48
[NOISE]. Now, following up on that, uh, yeah, go for it. A question. [BACKGROUND]
22:58
W and b are related to what? [BACKGROUND] Oh, yeah. Yeah. So, so usually you would have Theta transpose x,
23:07
which is sum of Theta_i_x_i, correct? And what I will split it is,
23:14
I will spit it in sum of Theta_i_x_i plus Theta_0 times 1.
23:21
I'll split it like that. Theta_0 would correspond to b and these Theta_is would correspond to w_is, make sense?
23:28
Okay. One more question and then we move on. [BACKGROUND]
23:45
Good question. That's the next thing we're going to see. So the question is a follow up on this,
23:52
is there cases where we have a constraint where there is only one possible outcome?
23:59
It means there is no cat and lion, there is either a cat or a lion, there is no iguana and lion,
24:04
there's either an iguana or a lion. Think about health care. There are many, there are many models that are made to detect,
24:14
uh, if a disease, skin disease is present on- based on cell microscopic images.
24:20
Usually, there is no overlap between these, it means, you want to classify a specific disease among a large number of diseases.
24:27
So this model would still work but would not be optimal because it's longer to train.
24:32
Maybe one disease is super, super rare and one of the neurons is never going to be trained. Let's say you're working in a zoo where there is
24:39
only one iguana and there are thousands of lions and thousands of cats. This guy will never train almost,
24:45
you know, it would be super hard to train this one. So you want to start with another model that- where you put the constraint, that's, okay,
24:51
there is only one disease that we want to predict and let the model learn, with all the neurons learn together by creating interaction between them.
25:00
Have you guys heard of softmax? Yes? Somebody, ah, I see that in the back.
25:07
[LAUGHTER] Okay. So let's look at softmax a little bit together. So we set a new goal now,
25:15
which is we add a constraint which is an unique animal on an image.
25:30
So at most one animal on an image. So I'm going to modify the network a little bit.
25:38
We're go- we have our cat and there is no lion on the image, we flatten it,
25:45
and now I'm going to use the same scheme with the three neurons, a_1, a_2, a_3.
26:02
But as an output, what I am going to use is an exponent, a softmax function.
26:11
So let me be more precise, let me, let me actually introduce another notation to make it easier.
26:18
As you know, the neuron is a linear part plus an activation.
26:24
So we are going to introduce a notation for the linear part,
26:29
I'm going to introduce Z_11 to represent the linear part of the first neuron.
26:36
Z_112 to introduce the linear part of the second neuron.
26:43
So now our neuron has two parts, one which computes Z and one which computes a, equals sigmoid of Z.
26:49
Now, I'm going to remove all the activations and make these Zs and I'm going to use the specific formula.
27:23
So this, if you recall, is exactly the softmax formula.
27:30
[NOISE] Yeah.
27:50
Okay. So now the network we have, can you guys see or it's too small? Too small?
27:57
Okay, I'm going to just write this formula bigger and then you can figure out the others, I guess, because,
28:04
e of Z_3, 1 divided by sum from
28:10
k equals 1 to 3 of e, exponential of ZK_1.
28:15
Okay, can you see this one? So here is for the third one.
28:21
If you are doing it for the first one, you will add- you'll just change this into a 2, into a 1 and for the second one into a 2.
28:27
So why is this formula interesting and why is it not robust to this labeling scheme anymore?
28:33
It's because the sum of the outputs of this network have to sum up to 1. You can try it.
28:40
If you sum the three outputs, you get the same thing in the numerator and on the denominator and you get 1. That makes sense?
28:49
So instead of getting a probabilistic output for each,
28:56
each of y, if, each of y hat 1, y hat 2, y hat 3, we will get a probability distribution over all the classes.
29:04
So that means we cannot get 0.7, 0.6, 0.1, telling us roughly that there is probably a cat and a lion but no iguana.
29:13
We have to sum these to 1. So it means, if there is no cat and no lion,
29:19
it means there's very likely an iguana. The three probabilities are dependent on each
29:24
other and for this one we have to label the following way,
29:31
1, 1, 0 for a cat, 0, 1, 0 for a lion or 0,
29:36
0, 1 for an iguana. So this is called a softmax multi-class network.
29:46
[inaudible].
30:03
You assume there is at least one of the three classes, otherwise you have to add a fourth input that will represent an absence of an animal.
30:10
But this way, you assume there is always one of these three animals on every picture.
30:21
And how many parameters does the network have? The same as the second one.
30:28
We still have three neurons and although I didn't write it, this Z_1 is equal to w_11,
30:34
x plus b_1, Z_2 same, Z_3 same. So there's 3n plus 3 parameters.
30:43
So one question that we didn't talk about is, how do we train these parameters?
30:54
These, these parameters, the 3n plus 3 parameters, how do we train them? You think this scheme will work or no?
31:01
What's wrong, what's wrong with this scheme?
31:06
What's wrong with the loss function specifically?
31:15
There's only two outcomes. So in this loss function, y is a number between 0 and 1,
31:23
y hat same is the probability, y is either a 0 or 1, y hat is between 0 and 1,
31:28
so it cannot match this labeling. So we need to modify the loss function.
31:34
So let's call it loss three neuron.
31:40
What I'm going to do is I'm going to just sum it up for the three neurons.
32:05
Does this make sense? So I am just doing three times this loss for each of the neurons.
32:14
So we have exactly three times this. We sum them together.
32:19
And if you train this loss function, you should be able to train the three neurons that you have.
32:25
And again, talking about scarcity of one of the classes. If there is not many iguana,
32:31
then the third term of this sum is not going to help this neuron train towards detecting an iguana.
32:40
It's going to push it to detect no iguana.
32:45
Any question on the loss function? Does this one make sense? Yeah? [inaudible]
33:01
Yeah. Usually, that's what will happen is that the output of this network once it's trained, is going to be a probability distribution.
33:08
You will pick the maximum of those, and you will set it to 1 and the others to 0 as your prediction. One more question, yeah.
33:17
[inaudible]
33:29
If you use the 2-1. If you use this labeling scheme like 1-1-0 for this network,
33:35
what do you think it will happen? It will probably not work.
33:43
And the reason is this sum is equal to 2, the sum of these entries,
33:48
while the sum of these entries is equal to 1. So you will never be able to match the output to the input to the label. That makes sense?
33:56
So what the network is probably going to do is it's probably going to send this one to one-half,
34:01
this one to one-half, and this one to 0 probably, which is not what you want.
34:07
Okay. Let's talk about the loss function for this softmax regression.
34:12
[NOISE] Because you know
34:22
what's interesting about this loss is if I take this derivative,
34:27
derivative of the loss 3N with respect to W2_1.
34:35
You think is going to be harder than this derivative, than this one or no?
34:40
It's going to be exactly the same. Because only one of these three terms depends on W12.
34:46
It means the derivative of the two others are 0. So we are exactly at the same complexity during the derivation.
34:52
But this one, do you think if you try to compute,
34:58
let's say we define a loss function that corresponds roughly to that. If you try to compute the derivative of the loss with respect to W2,
35:05
it will become much more complex. Because this number, the output here that is going to impact the loss function directly,
35:14
not only depends on the parameters of W2, it also depends on the parameters of W1 and W3.
35:20
And same for this output. This output also depends on the parameters W2. Does this makes sense? Because of this denominator.
35:28
So the softmax regression needs a different loss function and a different derivative.
35:34
So the loss function we'll define is a very common one in deep learning, it's called the softmax cross entropy.
35:41
Cross entropy loss.
35:47
I'm not going to- to- into the details of where it comes from but you can get the intuition of yklog.
36:14
So it, it surprisingly looks like the binary croissant,
36:19
the binary, uh, the logistic loss function. The only difference is that we will sum it up on all the- on all the classes.
36:33
Now, we will take a derivative of something that looks like that later. But I'd say if you can try it at home on this one,
36:41
uh, it would be a good exercise as well. So this binary cross entropy loss is very
36:47
likely to be used in classification problems that are multi-class.
36:54
Okay. So this was the first part on logistic regression types of networks.
37:00
And I think we're ready now with the notation that we introduced to jump on to neural networks.
37:06
Any question on this first part before we move on?
37:14
So one- one question I would have for you. Let's say instead of trying to predict if there is a cat or no cat,
37:22
we were trying to predict the age of the cat based on the image. What would you change? This network.
37:32
Instead of predicting 1-0, you wanna predict the age of the cat.
37:37
What are the things you would change? Yes.
37:44
[inaudible].
37:53
Okay. So I repeat. I, I basically make several output nodes where each of them corresponds to one age of cats.
38:02
So would you use this network or the third one? Would you use the three neurons neural network or the softmax regression?
38:10
Third one. The third one. Why? You have a unique age. You have a unique age,
38:16
you cannot have two ages, right. So we would use a softmax one because we want the probability distribution along the edge, the ages.
38:24
Okay. That makes sense. That's a good approach. There is also another approach which is using directly a regression to predict an age.
38:33
An age can be between zero and plus infi- not plus infinity- [LAUGHTER]. -zero and a certain number.
38:39
[LAUGHTER] And, uh, so let's say you wanna do a regression,
38:45
how would you modify your network? Change the Sigmoid.
38:51
The Sigmoid puts the Z between 0 and 1. We don't want this to happen. So I'd say we will change the Sigmoid.
38:57
Into what function would you change the Sigmoid? [inaudible]
39:10
Yeah. So the second one you said was? [inaudible] Oh, to get a Poisson type of distribution.
39:16
Okay. So let's, let's go with linear. You mentioned linear. We could just use a linear function,
39:23
right, for the Sigmoid. But this becomes a linear regression. The whole network becomes a linear regression.
39:29
Another one that is very common in, in deep learning is called the Rayleigh function. It's a function that is almost linear,
39:36
but for every input that is negative, it's equal to 0. Because we cannot have negative h,
39:41
it makes sense to use this one. Okay. So this is called rectified linear units, ReLU.
39:51
It's a very common one in deep learning. Now, what else would you change? We talked about linear regression.
39:58
Do you remember the loss function you are using in linear regression? What was it? [inaudible]
40:06
It was probably one of these two; y hat minus y. This comparison between the output label and y-hat,
40:14
the prediction, or it was the L2 loss; y-hat minus y in L2 norm.
40:20
So that's what we would use. We would modify our loss function to fit the regression type of problem.
40:25
And the reason we would use this loss instead of the one we have for a regression task is because in optimization,
40:33
the shape of this loss is much easier to optimize for a regression task than it is for a classification task and vice versa.
40:39
I'm not going to go into the details of that but that's the intuition. [NOISE] Okay.
40:45
Let's go have fun with neural networks. [NOISE]
41:11
So we, we stick to our first goal. Given an image, tell us if there is cat or no cat.
41:24
This is 1, this is 0. But now we're going to make a network a little more complex.
41:30
We're going to add some parameters. So I get my picture of the cat. [NOISE] The cat is moving.
41:43
Okay. And what I'm going to do is that I'm going to put more neurons than before.
41:50
Maybe something like that. [NOISE]
42:22
So using the same notation you see that
42:34
my square bracket- So using the same notation, you see that my square bracket here is two
42:40
indicating that there is a layer here which is the second layer [NOISE] while this one is the first layer and this one is the third layer.
42:56
Everybody's, er, up to speed with the notations? Cool. So now notice that when you make a choice of architecture,
43:06
you have to be careful of one thing, is that the output layer has to have the same number of neurons as you
43:14
want, the number of classes to be for reclassification and one for a regression.
43:25
So, er, how many parameters does this - this network have?
43:31
Can someone quickly give me the thought process?
43:36
So how much here?
43:41
Yeah, like 3n plus 3 let's say. [inaudible].
43:59
Yeah, correct. So here you would have 3n weights plus 3 biases.
44:05
Here you would have 2 times 3 weights plus 2 biases because you have
44:10
three neurons connected to two neurons and here you will have 2 times 1 plus 1 bias.
44:16
Makes sense. So this is the total number of parameters. So you see that we didn't add too much parameters.
44:23
Most of the parameters are still in the input layer. Um, let's define some vocabulary.
44:31
The first word is Layer. Layer denotes neurons that are not connected to each other.
44:36
These two neurons are not connected to each other. These two neurons are not connected to each other. We call this cluster of neurons a layer.
44:42
And then this has three layers. So we would use input layer to define the first layer,
44:48
output layer to define the third layer because it directly sees the outputs and we would call the second layer a hidden layer.
44:58
And the reason we call it hidden is because the inputs and the outputs are hidden from this layer.
45:04
It means the only thing that this layer sees as input is what's the previous layer gave it.
45:10
So it's an abstraction of the inputs but it's not the inputs. Does that make sense? And same, it doesn't see the output,
45:19
it just gives what it understood to the last neuron that will compare the output to the ground truth.
45:26
So now, why are neural networks interesting? And why do we call this hidden layer?
45:31
Um, it's because if you train this network on cat classification with a lot of images of cats,
45:39
you would notice that the first layers are going to understand the fundamental concepts of the image,
45:45
which is the edges. This neuron is going to be able to detect this type of edges.
45:52
This neuron is probably going to detect some other type of edge. This neuron, maybe this type of edge.
45:59
Then what's gonna happen, is that these neurons are going to communicate what they found on the image to the next layer's neuron.
46:05
And this neuron is going to use the edges that these guys found to figure out that, oh, there is a - their ears.
46:12
While this one is going to figure out, oh, there is a mouth and so on if you have
46:18
several neurons and they're going to communicate what they understood to the output neuron that is going to construct the face of
46:24
the cat based on what it received and be able to tell if there is a cat or not. So the reason it's called hidden layer is because we - we
46:33
don't really know what it's going to figure out but with enough data, it should understand very complex information about the data.
46:39
The deeper you go, the more complex information the neurons are able to understand. Let me give you another example which is a house prediction example.
46:49
House price prediction. [NOISE]
47:13
So let's assume that our inputs are number of bedrooms,
47:20
size of the house, zip code, and wealth of the neighborhood, let's say.
47:32
What we will build is a network that has three neurons in the first layer and one neuron in the output layer.
47:40
So what's interesting is that as a human if you were to build, uh,
47:45
this network and like hand engineer it, you would say that, uh, okay zip codes and wealth or - or sorry.
47:54
Let's do that. Zip code and wealth are able to tell us about the school quality in the neighborhood.
48:03
The quality of the school that is next to the house probably.
48:10
As a human you would say these are probably good features to predict that. The zip code is going to tell us if the neighborhood is walkable or not, probably.
48:23
The size and the number of bedrooms is going to tell us what's the size of the family that can fit in this house.
48:33
And these three are probably better information than these in order to finally predict the price.
48:40
So that's a way to hand engineer that by hand, as a human in order
48:45
to give human knowledge to the network to figure out the price.
48:51
In practice what we do here is that we use a fully-connected layer - fully-connected.
49:02
What does that mean? It means that we connect every input of a layer,
49:08
every - every input to the first layer, every output of the first layer to the input of the third layer and so on.
49:15
So all the neurons among lay - from one layer to another are connected with each other.
49:20
What we're saying is that we will let the network figure these out. We will let the neurons of the first layer figure out
49:27
what's interesting for the second layer to make the price prediction. So we will not tell these to the network,
49:33
instead we will fully connect the network [NOISE] and so on.
49:41
Okay. We'll fully connect the network and let it figure out what are the interesting features.
49:47
And oftentimes, the network is going to be able better than the humans to find these - what are the features that are representative.
49:53
Sometimes you may hear neural networks referred as, uh, black box models.
49:59
The reason is we will not understand what this edge will correspond to. It's - it's hard to figure out that this neuron is
50:07
detecting a weighted average of the input features. Does that make sense?
50:16
Another word you might hear is end-to-end learning. The reason we talked about end-to-end learning is because we have an input,
50:25
a ground truth, and we don't constrain the network in the middle.
50:30
We let it learn whatever it has to learn and we call it end-to-end learning because we are just training based on the input and the output.
50:37
[NOISE]
51:15
Let's delve more into the math of this network. The neural network that we have here which has an input layer,
51:22
a hidden layer and an output layer. Let's try to write down the equations that run the inputs and pro - propagate it to the output.
51:31
We first have Z_1, that is the linear part of the first layer,
51:36
that is computed using W_1 times x plus b_1.
51:44
Then this Z_1 is given to an activation, let's say sigmoid, which is sigmoid of Z_1.
51:52
Z_2 is then the linear part of the second neuron which is going to
51:58
take the output of the previous layer, multiply it by its weights and add the bias.
52:07
The second activation is going to take the sigmoid of Z_2.
52:14
And finally, we have the third layer which is going to multiply its weights,
52:20
with the output of the layer presenting it and add its bias.
52:26
And finally, we have the third activation which is simply the sigmoid of the three.
52:38
So what is interesting to notice between these equations and the equations that we wrote here,
52:47
is that we put everything in matrices. So it means this a_3 that I have here, sorry,
52:57
this here for three neurons I wrote three equations, here for three neurons
53:05
in the second layer I just wrote a single equation to summarize it. But the shape of these things are going to be vectors.
53:13
So let's go over the shapes, let's try to define them. Z_1 is going to be x which is n by 1 times
53:21
w which has to be 3 by n because it connects three neurons to the input.
53:29
So this z has to be 3 by 1. It makes sense because we have three neurons.
53:37
Now let's go, let's go deeper. A_1 is just the sigmoid of z_1 so it doesn't change the shape.
53:45
It keeps the 3 by 1. Z_2 we know it,
53:50
it has to be 2 by 1 because there are two neurons in the second layer and it helps us figure out what w_2 would be.
53:58
We know a_1 is 3 by 1. It means that w_2 has to be 2 by 3.
54:04
And if you count the edges between the first and the second layer here you will find six edges, 2 times 3.
54:12
A_2, same shape as z_2. Z_3, 1 by 1,
54:18
a_3, 1 by 1, w_3, it has to be 1 by 2,
54:23
because a_2 is 2 by 1 and same for b. B is going to be the number of neurons so 3 by 1,
54:32
2 by 1, and finally 1 by 1. So I think it's usually very helpful,
54:39
even when coding these type of equations, to know all the shapes that are involved. Are you guys, like, totally okay with the shapes,
54:47
super-easy to figure out? Okay, cool. So now what is interesting is that we will try to vectorize the code even more.
54:59
Does someone remember the difference between stochastic gradient descent and gradient descent. What's the difference?
55:06
[inaudible]
55:14
Exactly. Stochastic gradient descent is updates, the weights and the bias after you see every example.
55:21
So the direction of the gradient is quite noisy. It doesn't represent very well the entire batch,
55:27
while gradient descent or batch gradient descent is updates after you've seen the whole batch of examples.
55:33
And the gradient is much more precise. It points to the direction you want to go to.
55:40
So what we're trying to do now is to write down these equations if
55:48
instead of giving one single cat image we had given a bunch of images that either have a cat or not a cat.
55:54
So now our input x. So what happens for
56:06
an input batch of m examples?
56:20
So now our x is not anymore a single column vector,
56:28
it's a matrix with the first image corresponding to x_1,
56:34
the second image corresponding to x_2 and so on until the nth image corresponding to x_n.
56:42
And I'm introducing a new notation which is the parentheses superscript corresponding to the ID of the example.
56:55
So square brackets for the layer, round brackets for the idea of the example we are talking about.
57:05
So just to give more context on what we're trying to do. We know that this is a bunch of operations.
57:12
We just have a, a network with inputs, hidden, and output layer.
57:17
We could have a network with 1,000 layer. The more layers we have the more computation and it quickly goes up.
57:24
So what we wanna do is to be able to parallelize our code or, or our computation as much as possible by giving
57:31
batches of inputs and parallelizing these equations. So let's see how these equations are modified when we give it a batch of m inputs.
57:40
I will use capital letters to denote the equivalent of the lowercase letters but for a batch of input.
57:51
So Z_1 as an example would be W_1,
57:57
let's use the same actually, W_1 times X plus B_1.
58:03
So let's analyze what Z_1 would look like. Z_1 we know that for every,
58:11
for every input example of the batch we will get one Z_1 which should look like this.
58:28
Then we have to figure out what have to be the shapes of this equation in order to end up with this.
58:34
We know that Z_1 was 3 by 1. It mean, it means capital Z_1 has to be 3 by
58:42
m because each of these column vectors are 3 by 1 and we have m of them.
58:50
Because for each input we forward propagate through the network, we get these equations. So for the first cat image we get these equations,
58:56
for the second cat image we get again equations like that and so on.
59:06
So what is the shape of x? We have it above. We know that it's n by n. What is the shape of w_1?
59:17
It didn't change. W_1 doesn't change. It's not because I will give 1,000 inputs to
59:23
my network that the parameters are going to be more. So the parameter number stays the same even if I give more inputs.
59:31
And so this has to be 3 by n in order to match Z_1. Now the interesting thing is that there is an algebraic problem here.
59:42
What is the algebraic problem? We said that the number of parameters doesn't change.
59:48
It means that w has the same shape as it has before, as it had before.
59:54
B should have the same shape as it had before, right? It should be 3 by 1. What's the problem of this equation?
1:00:06
Exactly. We're summing a 3 by m matrix to a 3 by 1 vector.
1:00:14
This is not possible in math. It doesn't work. It doesn't match. When you do some summations or subtraction,
1:00:20
you need the two terms to be the same shape because you will do an element-wise addition or an ele- element-wise subtraction.
1:00:29
So what's the trick that is used here? It's a, it's a technique called broadcasting.
1:00:41
Broadcasting is that- is the fact that we don't want to change the number of parameters, it should stay the same.
1:00:47
But we still want this operation to be able to be written in parallel version.
1:00:53
So we still want to write this equation because we want to parallelize our code, but we don't want to add more parameters, it doesn't make sense.
1:00:59
So what we're going to do is that we are going to create a vector b tilde
1:01:05
1 which is going to be b_1 repeated three times.
1:01:11
Sorry, repeated m times.
1:01:23
So we just keep the same number of parameters but just repeat them in order to be able to write my code in parallel.
1:01:31
This is called broadcasting. And what is convenient is that for those of you who, uh, the homeworks are in Matlab or Python?
1:01:40
Matlab. Okay. So in Matlab, no Python? [LAUGHTER]. [NOISE] Thank you. Um, Python. So in Python,
1:01:48
there is a package that is often used to to code these equations. It's numPy. Some people call it numPy, I'm not sure why.
1:01:55
So numPy, basically numerical Python,
1:02:01
we directly do the broadcasting. It means if you sum this 3 by m matrix with a 3 by 1 parameter vector,
1:02:12
it's going to automatically reproduce the parameter vector m times so that the equation works. It's called broadcasting. Does that make sense?
1:02:20
So because we're using this technique, we're able to rewrite all these equations with capital letters.
1:02:27
Do you wanna do it together or do you want to do it on your own? Who wants to do it on their own?
1:02:35
Okay. So let's do it on their own [LAUGHTER] on your own.
1:02:40
So rewrite these with capital letters and figure out the shapes. I think you can do it at home, wherever,
1:02:46
we're not going to do here, but make sure you understand all the shapes. Yeah. [inaudible] How how is this
1:02:56
[inaudible]?
1:03:05
So the question is how is this different from principal component analysis? This is a supervised learning algorithm that
1:03:12
will be used to predict the price of a house. Principal component analysis doesn't predict anything.
1:03:17
It gets an input matrix X normalizes it, ah, computes the covariance matrix and then figures out what are
1:03:25
the pri- principal components by doing the the eigenvalue decomposition. But the outcome of PCA is,
1:03:31
you know that the most important features of your dataset X are going to be these features.
1:03:39
Here we're not looking at the features. We're only looking at the output. That is what is important to us. Yes.
1:03:48
In the first lecture when did you say that the first layers is the edges in an [inaudible].
1:03:58
So the question is, can you explain why the first layer would see the edges? Is there an intuition behind it? It's not always going to see the edges,
1:04:05
but it's oftentimes going to see edges because um, in order to detect a human face,
1:04:11
let's say, you will train an algorithm to find out whose face it is. So it has to understand the faces very well.
1:04:17
Um, you need the network to be complex enough to understand very detailed features of the face.
1:04:23
And usually, this neuron, what it sees as input are pixels.
1:04:29
So it means every edge here is the multiplication of the weight by a pixel.
1:04:34
So it sees pixels. It cannot understand the face as a whole because it sees only pixels.
1:04:42
It's very granular information for it. So it's going to check if pixels nearby have
1:04:48
the same color and understand that there is an edge there, okay? But it's too complicated to understand the whole face in the first layer.
1:04:55
However, if it understands a little more than a pixel information, it can give it to the next neuron.
1:05:02
This neuron will receive more than pixel information. It would receive a little more complex-like edges,
1:05:09
and then it will use this information to build on top of it and build the features of the face.
1:05:14
So what I'm trying to sum up is that these neurons only see the pixels, so they're not able to build more than the edges.
1:05:19
That's the minimum thing that they can- the maximum thing they can do. And it's it's a complex topic,
1:05:25
like interpretation of neural network is a highly researched topic, it's a big research topic. So nobody figured out exactly how all the neurons evolve.
1:05:36
Yeah. One more question and then we move on.
1:05:43
Ah, how [inaudible].
1:05:51
So the question is how [OVERLAPPING]. [inaudible]. -how do you decide how many neurons per layer? How many layers?
1:05:57
What's the architecture of your neural network? There are two things to take into consideration I would say. First and nobody knows the right answer, so you have to test it.
1:06:05
So you you guys talked about training set, validation set, and test set. So what we would do is,
1:06:11
we would try ten different architectures, train it, train the network on these,
1:06:16
looking at the validation set accuracy of all these, and decide which one seems to be the best. That's how we figure out what's the right network size.
1:06:24
On top of that, using experience is often valuable. So if you give me a problem,
1:06:29
I try always to gauge how complex is the problem. Like cat classification, do you
1:06:36
think it's easier or harder than day and night classification? So day and night classification is I give you an image,
1:06:43
I asked you to predict if it was taken during the day or during the night, and on the other hand you want there's a cat on the image or not.
1:06:49
Which one is easier, which one is harder?
1:06:54
Who thinks cat classification is harder? Okay. I think people agree.
1:07:00
Cat classification seems harder, why? Because there are many breeds of cats. Can look like different things.
1:07:05
There's not many breeds of nights. um, I guess. [LAUGHTER] Um, one thing that might be challenging in the day and night classification,
1:07:12
is if you want also to figure it out in house like i- inside, you know maybe there is a tiny window there and I'm able to tell that is the day
1:07:22
but for a network to understand it you will need a lot more data than if only you wanted to work outside, different.
1:07:27
So these problems all have their own complexity. Based on their complexity, I think the network should be deeper.
1:07:34
The comp- the more complex usually is the problem, the more data you need in order to figure out the output,
1:07:39
the more deeper should be the network. That's an intuition, let's say. Okay. Let's move on guys because I think we have about what, 12 more minutes?
1:07:57
Okay. Let's try to write the loss function
1:08:02
for this problem. [NOISE].
1:08:19
So now that we have our network, we have written this propagation equation and I we call it forward propagation,
1:08:26
because it's going forward, it's going from the input to the output. Later on when we will, we will derive these equations,
1:08:33
we will call them backward propagation, because we are starting from the loss and going backwards.
1:08:39
So let's let's talk about the optimization problem. Optimizing w_1, w_2, w_3, b_1, b_2, b_3.
1:08:57
We have a lot of stuff to optimize, right? We have to find the right values for these and remember model equals architecture plus parameter.
1:09:03
We have our architecture, if we have our parameters we're done. So in order to do that, we have to define an objective function.
1:09:13
Sometimes called loss, sometimes called cost function.
1:09:18
So usually we would call it loss if there is only one example in the batch,
1:09:23
and cost, if there is multiple examples in a batch.
1:09:32
So the loss function that, let- let's define the cost function.
1:09:38
The cost function J depends on y hat n_y. Okay. So y hat,
1:09:46
y hat is a_3. Okay. It depends on y hat n_y,
1:09:57
and we will set it to be the sum of the loss functions L_i,
1:10:05
and I will normalize it. It's not mandatory, but normalize it with 1/n. So what does this mean?
1:10:14
It's that we are going for batch gradient descent. We wanna compute the loss function for the whole batch, parallelize our code,
1:10:23
and then calculate the cost function that will be then derived to give us the direction of the gradients.
1:10:31
That is, the average direction of all the de-de- derivation with respect to the whole input batch.
1:10:38
And L_i will be the loss function corresponding to one parameter.
1:10:45
So what's the error on this specific one input, sorry not parameter, and it will be the logistic loss.
1:11:08
You've already seen these equations, I believe. So now, is it more complex to take
1:11:16
a derivative with respect to J like of J with respect to the parameters or of L?
1:11:22
What's the most complex between this one, let's say we're taking the derivative with respect to w_2, compared to this one?
1:11:38
Which one is the hardest? Who thinks J is the hardest?
1:11:47
Who think it doesn't matter? Yeah, it doesn't matter because derivation is is a linear operation, right?
1:11:56
So you can just take the derivative inside and you will see that if you know this, you just have to take the sum over this.
1:12:03
So instead of computing all derivatives on J, we will com- compute them on L, but it's totally equivalent.
1:12:09
There's just one more step at the end. Okay. So now we defined our loss function, super.
1:12:19
We defined our loss function and the next step is optimize. So we have to compute a lot of derivatives. [NOISE]
1:12:41
And that's called backward propagation. [NOISE] So the question
1:12:53
is why is it called backward propagation? It's because what we want to do ultimately is this.
1:12:59
For any l equals 1-3,
1:13:05
we want to do that, wl equals wl minus Alpha derivative of j with respect to wl,
1:13:21
and bl equals bl minus Alpha derivative of j with respect to bl.
1:13:29
So we want to do that for every parameter in layer 1, 2, and 3.
1:13:35
So it means, we have to compute all these derivatives, we have to compute derivative of the cost with respect to w1,
1:13:41
w2, w3, b1, b2, b3. You've done it with logistic regression,
1:13:47
we're going to do it with a neural network, and you're going to understand why it's called backward propagation.
1:13:52
Which one do you want to start with? Which derivative? You wanna start with the derivative with respect to w1,
1:13:58
w2, or w3, let's say. Assuming we'll do the bias later. W what?
1:14:06
W1? You think w1 is a good idea. I do- don't wanna do w1.
1:14:13
I think we should do w3, and the reason is because if you look at this loss function,
1:14:22
do you think the relation between w3 and this loss function is easier to understand or
1:14:28
the relation between w1 and this loss function? It's the relation between w3 and this loss function.
1:14:34
Because w3 happens much later in the- in the network. So if you want to understand how much should we move w1 in order to make the loss move?
1:14:42
It's much more complicated than answering the question how much should w3 move to move the loss.
1:14:47
Because there's much more connections if you wanna compete with w1.
1:14:52
So that's why we call it backward propagation is because we will start with the top layer, the one that's the closest to the loss function,
1:14:58
derive the derivative of j with respect to w1.
1:15:08
Once we've computed this derivative which we are going to do next week,
1:15:14
once we computed this number, we can then tackle this one.
1:15:22
Oh, sorry. Yeah. Thanks. Yeah. Once we computed this number,
1:15:28
we will be able to compute this one very easily. Why very easily?
1:15:33
Because we can use the chain rule of calculus. So let's see how it works. What we're- I'm just going to give you, uh,
1:15:40
the one-minute pitch on- on backprop, but, uh, we'll do it next week together. So if we had to compute this derivative,
1:15:48
what I will do is that I will separate it into several derivatives that are easier.
1:15:53
I will separate it into the derivative of j with respect to something, with the something, with respect to w3.
1:16:00
And the question is, what should this something be? I will look at my equations.
1:16:07
I know that j depends on Y-hat, and I know that Y-hat depends on z3.
1:16:13
Y-hat is the same thing as a3, I know it depends on z3. So why don't- why don't I include z3 in my equation?
1:16:21
I also know that z3 depends on w3, and the derivative of z3 with respect to w2 is super easy,
1:16:26
it's just a2 transpose. So I will just make a quick hack and say that
1:16:32
this derivative is the same as taking it with respect to a3,
1:16:37
taking derivative of a3 with respect to z3, and taking the derivative of z3 with respect to w3.
1:16:49
So you see? Same, same derivative, calculated in different ways.
1:16:55
And I know this, I know these are pretty easy to compute.
1:17:01
So that's why we call it backpropagation, it's because I will use the chain rule to compute the derivative of w3,
1:17:07
and then when I want to do it for w2, I'm going to insert, I'm going to insert the derivative with z3 times the derivative of
1:17:19
z3 with respect to a2 times the derivative of a2 with respect to z2,
1:17:28
and derivative of z2 with respect to w2.
1:17:33
Does this make sense that this thing here is the same thing as this?
1:17:42
It means, if I wanna compute the derivative of w2, I don't need to compute this anymore,
1:17:48
I already did this for w3. I just need to compute those which are easy ones, and so on.
1:17:54
If I wanna compute the derivative of j with respect to w1,
1:17:59
I'm going to- I'm not going to decompose all the thing again, I'm just going to take the derivative of j with respect to
1:18:06
z2 which is equal to this whole thing. And then I'm gonna multiply it by the derivative of
1:18:13
z2 with respect to a1 times derivative of a1 with respect to z1 times the derivative of z1 with respect to w1.
1:18:26
And again, this thing I know it already, I computed it previously just for this one.
1:18:32
So what's, what's interesting about it is that I'm not gonna redo the work I did,
1:18:38
I'm just gonna store the right values while back-propagating and continue to derivate. One thing that you need to notice though is that, look,
1:18:46
you need this forward propagation equation in order to remember what should be the path to take in
1:18:53
your chain rule because you know that this derivative of j with respect to w3,
1:18:59
I cannot use it as it is because w3 is not connected to the previous layer. If you look at this equation,
1:19:05
a2 doesn't depend on w3, it depends on z3. Sorry, like, uh, my bad,
1:19:12
it depends- no, sorry, what I wanted to say is that z2 is connected to w2,
1:19:20
but a1 is not connected to w2. So you wanna choose the path that you're going
1:19:28
through in the proper way so that there's no cancellation in these derivatives.
1:19:33
You- you cannot compute derivative of w2 with
1:19:39
respect to- to a1, right?
1:19:45
You cannot compute that, you don't know it. Okay. So I think we're done for today.
1:19:51
So one thing that I'd like you to do if you have time is just think about the things that can be tweaked in a neural network.
1:19:58
When you build a neural network, you are not done, you have to tweak it, you have to tweak the activations, you have to tweak the loss function.
1:20:05
There's many things you can tweak, and that's what we're going to see next week. Okay. Thanks.