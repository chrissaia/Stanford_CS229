{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# Lesson 15 - PCA via SVD and Variance Explained\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Objectives\n- Compute PCA using SVD.\n- Visualize principal components.\n- Measure variance explained by top components.\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## From the notes\n\n**PCA**\n- Find orthonormal directions that maximize variance.\n- SVD on centered data yields principal components.\n\n_TODO: Validate PCA derivation in the CS229 main notes PDF._\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Intuition\nPCA rotates data into a new coordinate system ordered by variance, enabling dimensionality reduction with minimal reconstruction loss.\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Data\nWe use a 2D Gaussian dataset with correlated features.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "import numpy as np\nimport matplotlib.pyplot as plt\n\nnp.random.seed(42)\n\nmean = np.array([0, 0])\ncov = np.array([[2.0, 1.2], [1.2, 0.8]])\nX = np.random.multivariate_normal(mean, cov, 200)\nX_centered = X - X.mean(axis=0)\n\nU, S, Vt = np.linalg.svd(X_centered, full_matrices=False)\ncomponents = Vt\nexplained = (S**2) / np.sum(S**2)\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Experiments\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "explained\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Visualizations\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "plt.figure(figsize=(6,4))\nplt.scatter(X_centered[:,0], X_centered[:,1], alpha=0.6)\nfor i, comp in enumerate(components[:2]):\n    plt.arrow(0, 0, comp[0]*3, comp[1]*3, color=\"red\", head_width=0.1)\nplt.title(\"PCA directions\")\nplt.xlabel(\"x1\")\nplt.ylabel(\"x2\")\nplt.axis(\"equal\")\nplt.show()\n\nplt.figure(figsize=(6,4))\nplt.bar([1,2], explained[:2])\nplt.title(\"Variance explained\")\nplt.xlabel(\"component\")\nplt.ylabel(\"fraction\")\nplt.show()\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Takeaways\n- PCA finds orthogonal directions of maximum variance.\n- Variance explained helps decide how many components to keep.\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Explain it in an interview\n- Describe how PCA relates to the covariance matrix.\n- Explain why centering the data matters.\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Exercises\n- Reconstruct the data using only the first principal component.\n- Compare PCA via eigen-decomposition vs SVD.\n- Try PCA on non-centered data and observe changes.\n"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "pygments_lexer": "ipython3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}