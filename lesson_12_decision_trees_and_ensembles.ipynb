{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# Lesson 12 - Decision Trees and Ensemble Methods\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Objectives\n- Implement a simple decision stump.\n- Use bagging to combine stumps.\n- Visualize how ensembles improve stability.\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## From the notes\n\n**Decision trees**\n- Split data by maximizing information gain or minimizing impurity.\n\n**Bagging**\n- Train models on bootstrap samples and average predictions.\n\n_TODO: Confirm definitions in the CS229 main notes PDF._\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Intuition\nTrees are flexible but high-variance. Bagging reduces variance by averaging many noisy learners.\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Data\nWe create a 2D classification dataset with a nonlinear boundary.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "import numpy as np\nimport matplotlib.pyplot as plt\n\nnp.random.seed(42)\n\nX = np.random.uniform(-2, 2, size=(200, 2))\ny = (X[:,0]**2 + X[:,1]**2 > 1.5).astype(int)\n\ndef best_stump(X, y):\n    best = None\n    for feature in range(X.shape[1]):\n        thresholds = np.unique(X[:, feature])\n        for t in thresholds:\n            pred = (X[:, feature] > t).astype(int)\n            err = np.mean(pred != y)\n            if best is None or err < best[0]:\n                best = (err, feature, t)\n    return best\n\nerr, feat, thresh = best_stump(X, y)\nerr\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Experiments\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "def stump_predict(X, feat, thresh):\n    return (X[:, feat] > thresh).astype(int)\n\ndef bagging_predict(X, y, n_estimators=25):\n    preds = []\n    for _ in range(n_estimators):\n        idx = np.random.choice(len(X), len(X), replace=True)\n        err, feat, thresh = best_stump(X[idx], y[idx])\n        preds.append(stump_predict(X, feat, thresh))\n    return (np.mean(preds, axis=0) > 0.5).astype(int)\n\nbagged = bagging_predict(X, y)\n(bagged == y).mean()\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Visualizations\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "plt.figure(figsize=(6,4))\nplt.scatter(X[:,0], X[:,1], c=y, cmap=\"coolwarm\", alpha=0.7)\nplt.title(\"Synthetic classification data\")\nplt.xlabel(\"x1\")\nplt.ylabel(\"x2\")\nplt.show()\n\npreds = stump_predict(X, feat, thresh)\nplt.figure(figsize=(6,4))\nplt.scatter(X[:,0], X[:,1], c=preds, cmap=\"coolwarm\", alpha=0.7)\nplt.title(\"Decision stump predictions\")\nplt.xlabel(\"x1\")\nplt.ylabel(\"x2\")\nplt.show()\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Takeaways\n- Decision stumps are weak learners; bagging reduces variance.\n- Ensembles often outperform individual trees.\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Explain it in an interview\n- Explain why bagging helps unstable learners like trees.\n- Describe how a decision tree chooses a split.\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Exercises\n- Implement Gini impurity for splits.\n- Increase the number of bagged stumps and observe accuracy.\n- Try boosting a set of stumps.\n"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "pygments_lexer": "ipython3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}