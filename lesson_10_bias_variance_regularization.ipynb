{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# Lesson 10 - Bias/Variance, Regularization, Model Selection\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Objectives\n- Visualize bias/variance tradeoffs with polynomial regression.\n- Implement L2 and L1 regularization effects.\n- Use a validation set for model selection.\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## From the notes\n\n**Bias/variance**\n- Expected error = bias$^2$ + variance + noise.\n\n**Regularization**\n- L2: $J(\\theta) = \\frac{1}{2m}\\sum (h_\\theta(x^{(i)})-y^{(i)})^2 + \\frac{\\lambda}{2} \\sum_{j=1}^n \\theta_j^2$.\n- L1 adds $\\lambda \\sum |\\theta_j|$.\n\n_TODO: Validate formulas in the CS229 main notes PDF._\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Intuition\nHigh-capacity models reduce bias but increase variance. Regularization shrinks parameters to control variance and improve generalization.\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Data\nWe generate noisy samples from a sine curve and fit polynomials of varying degree.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "import numpy as np\nimport matplotlib.pyplot as plt\n\nnp.random.seed(42)\n\ndef make_data(n=40):\n    x = np.linspace(0, 1, n)\n    y = np.sin(2 * np.pi * x) + 0.2 * np.random.randn(n)\n    return x, y\n\nx, y = make_data()\n\ndef poly_features(x, degree):\n    return np.vstack([x**d for d in range(degree+1)]).T\n\ndef ridge_fit(X, y, lam=0.0):\n    return np.linalg.pinv(X.T @ X + lam * np.eye(X.shape[1])) @ X.T @ y\n\ndegrees = [1, 3, 9]\npreds = {}\nfor d in degrees:\n    X = poly_features(x, d)\n    theta = ridge_fit(X, y, lam=1e-2)\n    preds[d] = X @ theta\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Experiments\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "# Validation split\nidx = np.random.permutation(len(x))\nsplit = int(0.7 * len(x))\ntrain_idx, val_idx = idx[:split], idx[split:]\nx_train, y_train = x[train_idx], y[train_idx]\nx_val, y_val = x[val_idx], y[val_idx]\n\ndef val_mse(deg, lam):\n    Xtr = poly_features(x_train, deg)\n    Xval = poly_features(x_val, deg)\n    theta = ridge_fit(Xtr, y_train, lam)\n    return np.mean((Xval @ theta - y_val) ** 2)\n\nerrors = {(d, lam): val_mse(d, lam) for d in [1,3,5,9] for lam in [0.0, 0.01, 0.1]}\nmin(errors, key=errors.get), min(errors.values())\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Visualizations\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "plt.figure(figsize=(6,4))\nplt.scatter(x, y, alpha=0.6, label=\"data\")\nfor d in degrees:\n    plt.plot(x, preds[d], label=f\"degree {d}\")\nplt.title(\"Bias/variance via polynomial degree\")\nplt.xlabel(\"x\")\nplt.ylabel(\"y\")\nplt.legend()\nplt.show()\n\n# L1 vs L2 effect illustration\nlam_vals = np.linspace(0, 1, 50)\nl2_norms = []\nfor lam in lam_vals:\n    theta = ridge_fit(poly_features(x, 5), y, lam)\n    l2_norms.append(np.linalg.norm(theta[1:]))\nplt.figure(figsize=(6,4))\nplt.plot(lam_vals, l2_norms)\nplt.title(\"L2 shrinkage vs lambda\")\nplt.xlabel(\"lambda\")\nplt.ylabel(\"||theta||\")\nplt.show()\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Takeaways\n- Bias/variance tradeoffs are visible by changing model complexity.\n- Regularization controls variance by shrinking parameter magnitude.\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Explain it in an interview\n- Explain why regularization can improve test performance.\n- Describe how you would select hyperparameters in practice.\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Exercises\n- Implement L1-regularized regression via gradient descent.\n- Plot training vs validation error for multiple degrees.\n- Explain what happens as lambda \u2192 \u221e.\n"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "pygments_lexer": "ipython3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}