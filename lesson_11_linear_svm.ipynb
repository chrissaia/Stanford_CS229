{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Lesson 11: Support Vector Machines (Linear)",
        "",
        "## Objectives",
        "- Implement a linear SVM with hinge loss.",
        "- Visualize the margin and support vectors.",
        "- Compare hinge loss to logistic loss."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## From the notes",
        "Primal objective:",
        "\\[",
        "\\min_{\theta} \frac{1}{2}\\|\theta\\|^2 + C \\sum_{i=1}^m \\max(0, 1 - y^{(i)} \theta^T x^{(i)}).",
        "\\]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Intuition",
        "SVM maximizes the margin while penalizing misclassified points with hinge loss."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data",
        "We generate a linearly separable dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "import numpy as np",
        "import matplotlib.pyplot as plt",
        "",
        "np.random.seed(42)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Synthetic separable data",
        "m = 80",
        "X_pos = np.random.multivariate_normal([2,2], np.eye(2)*0.4, size=m//2)",
        "X_neg = np.random.multivariate_normal([-2,-2], np.eye(2)*0.4, size=m//2)",
        "X_raw = np.vstack([X_pos, X_neg])",
        "y = np.array([1]*(m//2) + [-1]*(m//2))",
        "X = np.c_[np.ones(m), X_raw]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Implementation: subgradient descent for hinge loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "def svm_subgradient(X, y, C=1.0, alpha=0.05, num_iters=300):",
        "    theta = np.zeros(X.shape[1])",
        "    history = []",
        "    for _ in range(num_iters):",
        "        margins = y * (X @ theta)",
        "        indicator = (margins < 1).astype(float)",
        "        grad = theta - C * (X.T @ (y * indicator)) / len(y)",
        "        theta -= alpha * grad",
        "        loss = 0.5 * np.sum(theta**2) + C * np.mean(np.maximum(0, 1 - margins))",
        "        history.append(loss)",
        "    return theta, np.array(history)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Experiments"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "theta, history = svm_subgradient(X, y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Visualizations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "plt.figure(figsize=(6,4))",
        "plt.scatter(X_pos[:,0], X_pos[:,1], label=\"+1\", alpha=0.7)",
        "plt.scatter(X_neg[:,0], X_neg[:,1], label=\"-1\", alpha=0.7)",
        "",
        "x1_vals = np.linspace(X_raw[:,0].min()-1, X_raw[:,0].max()+1, 100)",
        "x2_vals = -(theta[0] + theta[1]*x1_vals) / theta[2]",
        "plt.plot(x1_vals, x2_vals, color=\"black\", label=\"boundary\")",
        "plt.xlabel(\"x1\")",
        "plt.ylabel(\"x2\")",
        "plt.title(\"Linear SVM boundary\")",
        "plt.legend()",
        "plt.show()",
        "",
        "plt.figure(figsize=(6,4))",
        "plt.plot(history)",
        "plt.xlabel(\"iteration\")",
        "plt.ylabel(\"objective\")",
        "plt.title(\"SVM optimization\")",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Takeaways",
        "- SVM focuses on maximizing margin, often improving generalization.",
        "- The hinge loss ignores correctly classified points outside the margin."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Explain it in an interview",
        "- Describe margin maximization and hinge loss.",
        "- Explain the role of the regularization parameter \\(C\\)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exercises",
        "1. Try different values of \\(C\\) and compare margins.",
        "2. Compare hinge loss to logistic loss on the same data.",
        "3. Implement a hard-margin SVM when data is perfectly separable."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}