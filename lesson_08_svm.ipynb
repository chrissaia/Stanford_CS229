{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# Lesson 08 - Support Vector Machines and Hinge Loss\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Objectives\n- Implement a linear SVM with hinge loss.\n- Visualize margins and support vectors.\n- Compare to logistic regression boundaries.\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## From the notes\n\n**Soft-margin SVM**\n- Objective: minimize $\\frac{1}{2}\\|w\\|^2 + C \\sum_i \\max(0, 1 - y^{(i)}(w^T x^{(i)} + b))$.\n\n_TODO: Validate the SVM objective with CS229 main notes PDF._\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Intuition\nSVMs trade off margin maximization and hinge loss penalties for misclassified points. Only points on the margin (support vectors) influence the solution.\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Data\nWe use a separable 2D dataset to show the margin geometry.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "import numpy as np\nimport matplotlib.pyplot as plt\n\nnp.random.seed(42)\n\nX_pos = np.random.multivariate_normal([2, 2], np.eye(2), 60)\nX_neg = np.random.multivariate_normal([-2, -1], np.eye(2), 60)\nX = np.vstack([X_pos, X_neg])\ny = np.hstack([np.ones(len(X_pos)), -np.ones(len(X_neg))])\nXb = np.c_[np.ones(len(X)), X]\n\ndef svm_train(X, y, C=1.0, lr=0.01, iters=2000):\n    theta = np.zeros(X.shape[1])\n    for _ in range(iters):\n        margins = y * (X @ theta)\n        misclassified = margins < 1\n        grad = theta.copy()\n        grad[0] = 0  # no regularization on bias\n        grad -= C * (X[misclassified].T @ (y[misclassified]))\n        theta -= lr * grad / len(y)\n    return theta\n\ntheta = svm_train(Xb, y)\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Experiments\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "preds = np.sign(Xb @ theta)\n(preds == y).mean()\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Visualizations\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "plt.figure(figsize=(6,4))\nplt.scatter(X_pos[:,0], X_pos[:,1], label=\"+1\")\nplt.scatter(X_neg[:,0], X_neg[:,1], label=\"-1\")\nx1 = np.linspace(-4, 4, 100)\nx2 = -(theta[0] + theta[1]*x1) / theta[2]\nplt.plot(x1, x2, color=\"black\", label=\"boundary\")\nplt.title(\"Linear SVM boundary\")\nplt.xlabel(\"x1\")\nplt.ylabel(\"x2\")\nplt.legend()\nplt.show()\n\nmargins = y * (Xb @ theta)\nplt.figure(figsize=(6,4))\nplt.hist(margins, bins=20, alpha=0.7)\nplt.title(\"Hinge margins\")\nplt.xlabel(\"y*(w^T x)\")\nplt.ylabel(\"count\")\nplt.show()\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Takeaways\n- The hinge loss focuses on margin violations rather than all errors.\n- Support vectors define the decision boundary.\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Explain it in an interview\n- Describe the tradeoff controlled by C in a soft-margin SVM.\n- Explain how hinge loss differs from logistic loss.\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Exercises\n- Implement SVM with subgradient descent and compare to this version.\n- Add polynomial features and observe boundary changes.\n- Track how many points become support vectors as C changes.\n"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "pygments_lexer": "ipython3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}