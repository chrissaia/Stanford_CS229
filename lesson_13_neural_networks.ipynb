{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# Lesson 13 - Neural Networks and Backprop\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Objectives\n- Implement a 1-hidden-layer neural network.\n- Carry out forward and backward propagation.\n- Visualize decision boundaries.\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## From the notes\n\n**Neural networks**\n- Layered composition of affine transforms and nonlinearities.\n- Backprop computes gradients efficiently.\n\n_TODO: Align the notation with the CS229 main notes PDF._\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Intuition\nNeural networks stack linear transforms and nonlinearities to form flexible function approximators. Backprop efficiently computes gradients for all parameters.\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Data\nWe create a 2D dataset that requires nonlinear separation.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "import numpy as np\nimport matplotlib.pyplot as plt\n\nnp.random.seed(42)\n\nX = np.random.uniform(-1.5, 1.5, size=(200, 2))\ny = (X[:,0] * X[:,1] > 0).astype(int)\n\ndef sigmoid(z):\n    return 1 / (1 + np.exp(-z))\n\ndef train_nn(X, y, hidden=8, lr=0.5, iters=2000):\n    m, n = X.shape\n    W1 = np.random.randn(n, hidden) * 0.5\n    b1 = np.zeros(hidden)\n    W2 = np.random.randn(hidden, 1) * 0.5\n    b2 = np.zeros(1)\n    y = y.reshape(-1, 1)\n    for _ in range(iters):\n        z1 = X @ W1 + b1\n        a1 = np.tanh(z1)\n        z2 = a1 @ W2 + b2\n        a2 = sigmoid(z2)\n        dz2 = a2 - y\n        dW2 = a1.T @ dz2 / m\n        db2 = dz2.mean(axis=0)\n        dz1 = dz2 @ W2.T * (1 - np.tanh(z1)**2)\n        dW1 = X.T @ dz1 / m\n        db1 = dz1.mean(axis=0)\n        W1 -= lr * dW1\n        b1 -= lr * db1\n        W2 -= lr * dW2\n        b2 -= lr * db2\n    return W1, b1, W2, b2\n\nW1, b1, W2, b2 = train_nn(X, y)\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Experiments\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "def predict(X):\n    a1 = np.tanh(X @ W1 + b1)\n    a2 = sigmoid(a1 @ W2 + b2)\n    return (a2 > 0.5).astype(int).ravel()\n\npreds = predict(X)\n(preds == y).mean()\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Visualizations\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "plt.figure(figsize=(6,4))\nplt.scatter(X[:,0], X[:,1], c=y, cmap=\"coolwarm\", alpha=0.7)\nplt.title(\"Nonlinear dataset\")\nplt.xlabel(\"x1\")\nplt.ylabel(\"x2\")\nplt.show()\n\ngrid_x1 = np.linspace(-1.5, 1.5, 120)\ngrid_x2 = np.linspace(-1.5, 1.5, 120)\nxx1, xx2 = np.meshgrid(grid_x1, grid_x2)\ngrid = np.c_[xx1.ravel(), xx2.ravel()]\ngrid_preds = predict(grid).reshape(xx1.shape)\nplt.figure(figsize=(6,4))\nplt.contourf(xx1, xx2, grid_preds, levels=2, cmap=\"coolwarm\", alpha=0.4)\nplt.scatter(X[:,0], X[:,1], c=y, cmap=\"coolwarm\", alpha=0.7)\nplt.title(\"Neural network decision boundary\")\nplt.xlabel(\"x1\")\nplt.ylabel(\"x2\")\nplt.show()\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Takeaways\n- Backprop efficiently computes gradients for all weights.\n- Hidden layers enable nonlinear decision boundaries.\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Explain it in an interview\n- Explain the forward and backward pass in a simple neural network.\n- Describe why activation functions are necessary.\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Exercises\n- Try ReLU instead of tanh and compare performance.\n- Increase hidden units and observe overfitting.\n- Add L2 regularization to the loss.\n"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "pygments_lexer": "ipython3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}