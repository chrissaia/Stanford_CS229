{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# Lesson 09 - Kernels and Kernelized Regression\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Objectives\n- Compute kernel matrices for RBF and polynomial kernels.\n- Fit a kernel ridge regression model.\n- Visualize nonlinear decision boundaries.\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## From the notes\n\n**Kernel trick**\n- Representer theorem: solution lies in span of training examples.\n- Kernel: $K(x, z) = \\phi(x)^T \\phi(z)$ without explicit $\\phi$.\n\n_TODO: Validate kernel definitions in the CS229 main notes PDF._\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Intuition\nKernels let linear models act nonlinearly by replacing dot products with similarity functions in high-dimensional feature spaces.\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Data\nWe use a nonlinear 2D dataset (two moons style) to see the benefit of kernels.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "import numpy as np\nimport matplotlib.pyplot as plt\n\nnp.random.seed(42)\n\n# Simple nonlinear dataset\nangles = np.linspace(0, np.pi, 80)\nX1 = np.c_[np.cos(angles), np.sin(angles)] + 0.1 * np.random.randn(len(angles), 2)\nX2 = np.c_[1 - np.cos(angles), 1 - np.sin(angles)] + 0.1 * np.random.randn(len(angles), 2)\nX = np.vstack([X1, X2])\ny = np.hstack([np.ones(len(X1)), -np.ones(len(X2))])\n\ndef rbf_kernel(X, Z, gamma=2.0):\n    X_norm = (X**2).sum(axis=1)[:, None]\n    Z_norm = (Z**2).sum(axis=1)[None, :]\n    return np.exp(-gamma * (X_norm + Z_norm - 2 * X @ Z.T))\n\nK = rbf_kernel(X, X)\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Experiments\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "# Kernel ridge regression for classification\nlam = 1e-2\nalpha = np.linalg.pinv(K + lam * np.eye(len(X))) @ y\n\ndef predict(X_new):\n    K_new = rbf_kernel(X_new, X)\n    return K_new @ alpha\n\npreds = np.sign(predict(X))\n(preds == y).mean()\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Visualizations\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "plt.figure(figsize=(6,4))\nplt.scatter(X1[:,0], X1[:,1], label=\"class 1\")\nplt.scatter(X2[:,0], X2[:,1], label=\"class -1\")\nplt.title(\"Nonlinear dataset\")\nplt.xlabel(\"x1\")\nplt.ylabel(\"x2\")\nplt.legend()\nplt.show()\n\n# Decision boundary plot\ngrid_x1 = np.linspace(-1.5, 2.5, 100)\ngrid_x2 = np.linspace(-1.0, 2.0, 100)\nxx1, xx2 = np.meshgrid(grid_x1, grid_x2)\ngrid = np.c_[xx1.ravel(), xx2.ravel()]\nscores = predict(grid).reshape(xx1.shape)\nplt.figure(figsize=(6,4))\nplt.contourf(xx1, xx2, scores, levels=20, cmap=\"coolwarm\", alpha=0.6)\nplt.contour(xx1, xx2, scores, levels=[0], colors=\"black\")\nplt.scatter(X1[:,0], X1[:,1], c=\"white\", edgecolor=\"black\")\nplt.scatter(X2[:,0], X2[:,1], c=\"black\")\nplt.title(\"Kernel ridge regression boundary\")\nplt.xlabel(\"x1\")\nplt.ylabel(\"x2\")\nplt.show()\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Takeaways\n- Kernel methods let linear models learn nonlinear boundaries.\n- The kernel matrix encodes pairwise similarities among training examples.\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Explain it in an interview\n- Describe the representer theorem intuition.\n- Explain how RBF kernel bandwidth affects model flexibility.\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Exercises\n- Try a polynomial kernel and compare decision boundaries.\n- Increase gamma and observe overfitting behavior.\n- Implement kernelized logistic regression.\n"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "pygments_lexer": "ipython3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}