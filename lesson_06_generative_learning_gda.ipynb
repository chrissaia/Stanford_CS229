{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# Lesson 06 - Generative Learning (Gaussian NB, LDA, QDA)\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Objectives\n- Implement Gaussian Naive Bayes, LDA, and QDA.\n- Compare generative vs discriminative decision boundaries.\n- Visualize class-conditional densities.\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## From the notes\n\n**GDA setup**\n- Model $p(x|y)$ as Gaussian, $p(y)$ as Bernoulli.\n- Shared covariance $\\Sigma$ yields LDA; class-specific $\\Sigma_k$ yields QDA.\n\n**Gaussian Naive Bayes**\n- Assume features independent: $p(x|y) = \\prod_j \\mathcal{N}(x_j; \\mu_{y,j}, \\sigma_{y,j}^2)$.\n\n_TODO: Validate equations in the CS229 main notes PDF._\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Intuition\nGenerative models learn class-conditional densities, then apply Bayes' rule. LDA assumes shared covariance, QDA allows separate covariance matrices, and Naive Bayes assumes feature independence.\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Data\nWe generate a 2D two-class dataset with Gaussian clusters.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "import numpy as np\nimport matplotlib.pyplot as plt\n\nnp.random.seed(42)\n\nX0 = np.random.multivariate_normal([-2, -1], [[1, 0.2], [0.2, 1]], 60)\nX1 = np.random.multivariate_normal([2, 1], [[1, -0.3], [-0.3, 1]], 60)\nX = np.vstack([X0, X1])\ny = np.hstack([np.zeros(len(X0)), np.ones(len(X1))])\n\ndef lda_fit(X, y):\n    mu0 = X[y==0].mean(axis=0)\n    mu1 = X[y==1].mean(axis=0)\n    Sigma = np.cov(X.T)\n    return mu0, mu1, Sigma\n\ndef lda_predict(X, mu0, mu1, Sigma):\n    inv = np.linalg.pinv(Sigma)\n    score0 = X @ inv @ mu0 - 0.5 * mu0 @ inv @ mu0\n    score1 = X @ inv @ mu1 - 0.5 * mu1 @ inv @ mu1\n    return (score1 > score0).astype(int)\n\nmu0, mu1, Sigma = lda_fit(X, y)\npreds = lda_predict(X, mu0, mu1, Sigma)\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Experiments\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "acc = (preds == y).mean()\nacc\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Visualizations\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "plt.figure(figsize=(6,4))\nplt.scatter(X0[:,0], X0[:,1], label=\"class 0\")\nplt.scatter(X1[:,0], X1[:,1], label=\"class 1\")\nx1 = np.linspace(-5, 5, 200)\nx2 = np.linspace(-5, 5, 200)\nxx1, xx2 = np.meshgrid(x1, x2)\ngrid = np.c_[xx1.ravel(), xx2.ravel()]\npred_grid = lda_predict(grid, mu0, mu1, Sigma).reshape(xx1.shape)\nplt.contour(xx1, xx2, pred_grid, levels=[0.5], colors=\"black\")\nplt.title(\"LDA decision boundary\")\nplt.xlabel(\"x1\")\nplt.ylabel(\"x2\")\nplt.legend()\nplt.show()\n\nplt.figure(figsize=(6,4))\nplt.hist(X0[:,0], bins=15, alpha=0.7, label=\"x1 | y=0\")\nplt.hist(X1[:,0], bins=15, alpha=0.7, label=\"x1 | y=1\")\nplt.title(\"Class-conditional feature distribution\")\nplt.xlabel(\"x1\")\nplt.ylabel(\"count\")\nplt.legend()\nplt.show()\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Takeaways\n- LDA and QDA differ by covariance assumptions, which change the shape of the decision boundary.\n- Naive Bayes trades modeling accuracy for simplicity by assuming feature independence.\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Explain it in an interview\n- Explain when you would prefer a generative model over a discriminative one.\n- Describe the difference between LDA and QDA.\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Exercises\n- Implement QDA with class-specific covariances.\n- Add Gaussian Naive Bayes for higher-dimensional features.\n- Compare to logistic regression on the same dataset.\n"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "pygments_lexer": "ipython3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}